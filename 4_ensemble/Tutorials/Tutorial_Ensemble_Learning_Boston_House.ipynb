{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§î Ensemble LearningÏùÄ Imbalanced DataÏóêÎèÑ Ìö®Í≥ºÍ∞Ä ÏûàÏùÑÍπå?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "# import fetch_california_housing as dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import copy\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F  \n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Distribution (Target Y) ÌôïÏù∏\n",
    "\n",
    "- DatasetÏùÑ ÌôïÏù∏ÌïòÏó¨ Imbalanced DataÏù∏ÏßÄ ÌôïÏù∏ÌïòÍ∏∞ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_distribution(y):\n",
    "    # View distribution\n",
    "    sns.distplot(y)\n",
    "    plt.xlabel('x values')\n",
    "    plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUhElEQVR4nO3dd3xUVcI+8OdOTZ303kNChwQIhNAVVlzLslZWUDAqroVXdrO+ry/vrrK77horP+woiqKCYG+rWCJFpBNCJxDSSe/JJJnJzNzfH5NEIi1lkjOZeb6fz3x2ubkz82RU8uTcc8+RZFmWQUREROQgFKIDEBEREdkSyw0RERE5FJYbIiIicigsN0RERORQWG6IiIjIobDcEBERkUNhuSEiIiKHohIdYKBZLBaUlJTA09MTkiSJjkNERETdIMsyGhsbERoaCoXi0mMzTlduSkpKEBERIToGERER9UJRURHCw8MveY7TlRtPT08A1g9Hp9MJTkNERETd0dDQgIiIiM6f45fidOWm41KUTqdjuSEiIhpkujOlhBOKiYiIyKGw3BAREZFDYbkhIiIih8JyQ0RERA6F5YaIiIgcCssNERERORSWGyIiInIodlFuXn75ZURHR8PFxQXJycnYu3fvRc99++23IUlSl4eLi8sApiUiIiJ7JrzcbNq0CWlpaVixYgUyMzORkJCAuXPnoqKi4qLP0el0KC0t7XwUFBQMYGIiIiKyZ8LLzcqVK7FkyRKkpqZi5MiRWL16Ndzc3LB27dqLPkeSJAQHB3c+goKCBjAxERER2TOh5cZoNOLAgQOYM2dO5zGFQoE5c+Zg165dF31eU1MToqKiEBERgXnz5uHYsWMXPddgMKChoaHLg4iIiByX0HJTVVUFs9l83shLUFAQysrKLvicYcOGYe3atfj888/x3nvvwWKxYMqUKSguLr7g+enp6fDy8up8cEdwIiIixyb8slRPpaSkYNGiRUhMTMTMmTPxySefICAgAK+99toFz1++fDnq6+s7H0VFRQOcmIiIiAaS0F3B/f39oVQqUV5e3uV4eXk5goODu/UaarUa48aNQ05OzgW/rtVqodVq+5yViIiIBgehIzcajQYTJkxARkZG5zGLxYKMjAykpKR06zXMZjOOHDmCkJCQ/opJREREg4jQkRsASEtLw+LFi5GUlIRJkyZh1apV0Ov1SE1NBQAsWrQIYWFhSE9PBwD885//xOTJkxEXF4e6ujo888wzKCgowD333CPy2yDqtGFPoc1fc0FypM1fk4jIUQkvN/Pnz0dlZSUee+wxlJWVITExEZs3b+6cZFxYWAiF4pcBptraWixZsgRlZWXw8fHBhAkTsHPnTowcOVLUt0BERER2RJJlWRYdYiA1NDTAy8sL9fX10Ol0ouOQA+LIDRGR7fXk5/egu1uKiIiI6FJYboiIiMihsNwQERGRQ2G5ISIiIofCckNEREQOheWGiIiIHArLDRERETkUlhsiIiJyKCw3RERE5FBYboiIiMihsNwQERGRQ2G5ISIiIofCckNEREQOheWGiIiIHArLDRERETkUlhsiIiJyKCw3RERE5FBYboiIiMihsNwQERGRQ2G5ISIiIofCckNEREQOheWGiIiIHArLDRERETkUlhsiIiJyKCw3RERE5FBYboiIiMihsNwQERGRQ2G5ISIiIofCckNEREQOheWGiIiIHArLDRERETkUlhsiIiJyKCw3RERE5FBYboiIiMihsNwQERGRQ1GJDkDkrPQGEyoaDQAAhQQE6VzgolYKTkVENPix3BANoBajGT+fqcKJ0gaU1rd2+ZoEIMzHFWPCvJAc4weNigOrRES9wXJDNABMFgu2n6rCjpxKtLZZOo/7uKmhUihgNFtQ39KG4toWFNe2YPvpKlwxLACTY/2gkCSByYmIBh+WG6J+VtdsxPt7C1FU2wIACNJpMSM+AHGBHvB0UXc5L7u8EdtPVaK2uQ1fHS5FdlkjbkmKEBWdiGhQYrkh6kd5VXqs31OAZqMZLmoFfpcQirHh3hccjfF20yA5xg9JUb7Yl1+Db46W4nRFE1768TSmxvlhVKiXgO+AiGjw4UV9on6SV6XH2zvz0Gw0I8zbFUuviEdihM9lLzMpFRImx/rh/llxCPDQoqHVhAVr9uDo2foBSk5ENLix3BD1g/wqPdbtzEebWUZcoAfunRELX3dNj14jWOeC+2cNQYSPK+pb2rBgzW4cLq7rn8BERA6El6WIbKxGb8S7uwtgNFsQF+CBOyZHQa3s3e8RLmolUqfGYN3OfBTUNGPBmj14YNYQeLv1rChdzILkSJu8DhGRPeHIDZENNRtNeG93AVrazAj3ccXtfSg2HVzUStw5JRrBOhc0GUzW4mSyXP6JREROiuWGyEZkWcb/fHQYZQ2tcNeqsDA5ymZr1WjVStyREgV3jRKl9a346EARZFm2yWsTETkalhsiG/kk8yy+OlwKhQQsnBQJL1f15Z/UAz5uGtw+OQpKScLRkgbsy6+16esTETkKlhsiGyitb8HfvzwGAJgzIgjR/u798j5Rfu64alQQAOA/R0pQ0dh6mWcQETkflhuiPpJlGY98fASNrSYkRHhjenxAv77f1Dh/xAV4oM0s44N9RTBZOP+GiOhcLDdEffTpwbPYfqoSGpUCz90yFkpF/26XoJAk3DwhHG4aJUrqW/HT6ap+fT8iosGG5YaoD5oMJqR/cxIAsGx2POICPQfkfXWualw3NhQAsOVkBaradxcnIiKWG6I+eenHHFQ2GhDl54Z7pscM6HsnhHshPtADJouMT7PO8u4pIqJ2LDdEvZRXpcebO3IBAI9dNxJalXJA31+SJMxLDINaKSGvSo/MwroBfX8iInvFckPUS09vPok2s4yZQwNw5fBAIRl83TWYPdx699S3x8rQ2mYWkoOIyJ6w3BD1wtGz9fjmaBkkCfjrtSMgXWYzzP40Jc4Pfu4aNBlM2JpdISwHEZG9YLkh6oWV358CAMxLCMXQoIGZRHwxKoUC144NAQD8nFONqiZOLiYi58ZyQ9RDBwpq8ePJCigVEpbNGSo6DgBgWJAnhgZ5wCzL2Hy0THQcIiKhWG6IemjVD9ZRm5vHhyOmn1Yi7ilJknDN6BBIAI6XNqCopll0JCIiYVhuiHrgSHE9fjpdBaVCwtIr40TH6SJQ54LxkT4AgO+Oc/SGiJwXyw1RD7y2/QwA4HcJoYjwdROc5nxXjgiEUpJwplKPnIom0XGIiIRguSHqpoJqPb4+UgoAuHdGrOA0F+bjpsGkGF8A1tEbLuxHRM6I5Yaom974KQ8WGZg5NAAjQnSi41zUrGEBUCslFNe24ERpo+g4REQDTiU6AAC8/PLLeOaZZ1BWVoaEhAS8+OKLmDRp0mWft3HjRtx2222YN28ePvvss/4PSg5lw57Cbp+rN5jw/l7r+fGBHj167kDzdFFj6hB/bD1Vie+Ol2F4iCcUAtfhISIaaMJHbjZt2oS0tDSsWLECmZmZSEhIwNy5c1FRcenFyPLz8/Hwww9j+vTpA5SUnNmBglqYLDJCvV3s5g6pS5keHwAXtQIVjQYcKqoTHYeIaEAJLzcrV67EkiVLkJqaipEjR2L16tVwc3PD2rVrL/ocs9mMhQsX4h//+AdiYy8998FgMKChoaHLg6gnLLKMPXnVAIDJMX5CVyPuLleNEjPjAwAAGScrYLJYBCciIho4QsuN0WjEgQMHMGfOnM5jCoUCc+bMwa5duy76vH/+858IDAzE3Xfffdn3SE9Ph5eXV+cjIiLCJtnJeZwqa0Rtcxtc1UqMDfcWHafbUob4w0OrQo3eiIMFdaLjEBENGKHlpqqqCmazGUFBQV2OBwUFoazswut07NixA2+++SbWrFnTrfdYvnw56uvrOx9FRUV9zk3OZXf7qM2EKB9oVMIHO7tNo1JgxlDr6M2205UwW3jnFBE5h8HzNzWAxsZG3HHHHVizZg38/f279RytVgudTtflQdRd1U0GnCpvggQguf0W68FkUrQv3DRK1OiNOFxcJzoOEdGAEHq3lL+/P5RKJcrLy7scLy8vR3Bw8HnnnzlzBvn5+bj++us7j1na5xKoVCpkZ2djyJAh/RuanMq+/FoAQHyQB/w8tILT9JxGpcD0OH98e7wcW7MrkRDhzTuniMjhCR250Wg0mDBhAjIyMjqPWSwWZGRkICUl5bzzhw8fjiNHjiArK6vz8bvf/Q5XXHEFsrKyOJ+GbMpskXGw0FpukqIG36hNh+RYP7iqlahsMuBYCSfUE5HjE77OTVpaGhYvXoykpCRMmjQJq1atgl6vR2pqKgBg0aJFCAsLQ3p6OlxcXDB69Oguz/f29gaA844T9dXpikY0Gkxw0ygxPMRTdJxec1ErMWWIHzJOVmDLyQqMCtVx9IaIHJrwcjN//nxUVlbiscceQ1lZGRITE7F58+bOScaFhYVQKAbV1CByEAcKrKM24yK8oRrk/w5OGeKPHTlVKGtoRXZZo12vsExE1FeS7GSbzzQ0NMDLywv19fWcXOzkLrXKcJPBhKe+OQmzLOOhK+MR7OUygMn6x7fHyrDtVCXCfVxx/8whkCQJC5IjRcciIuqWnvz8Hty/jhL1k0NFdTDLMsK8XR2i2ADA1Dj/zj2nTnPHcCJyYCw3RBdwsMh6SWpClI/gJLbjoVUhOcYPALAl+9LbmxARDWYsN0S/UtloQEldKxQSMCbMS3Qcm5oW5w+lQkJBdTMKqvWi4xAR9QuWG6JfOdS+2F18oCfctcLn3NuUzlWNcRHeAIBtpyrFhiEi6icsN0TnkGW5cxfthAjHGrXpMCM+ABKAk2WNyC5rFB2HiMjmWG6IznG2rgXVeiPUSslhb5f299RiVKj1e3tt+xnBaYiIbI/lhugcHaM2w4N10KqUYsP0o44NNb/IKsHZuhbBaYiIbIvlhqidRZZx5Gw9ACCxfV6Kowr3ccOQAHeYLDLe+ClXdBwiIptiuSFqV1TTjIZWE7QqBeIDPUTH6Xcdozcb9xahRm8UnIaIyHZYbojaHW0ftRkRooNK6fj/acQFeGB0mA4tbWas25kvOg4Rkc04/t/gRN0gyzKOlVp3zO6YbOvoJEnC/TPjAADrduWj2WgSnIiIyDZYbohgvUuqrrkNaqWE+MDBuwN4T109OhjRfm6oa27Dxr1FouMQEdkEyw0RgGMl1lGbYcE6aFTO85+FUiHh3hlDAABv/JSLNrNFcCIior5znr/FiS5CluXO+TajneSS1LluHB8Gfw8tSupb8UVWieg4RER9xnJDTq+80YBqvREqhYRhQc5zSaqDi1qJu6fFAABWbzsDi0UWnIiIqG9YbsjpnWyfSDwkwANateMu3HcpCydHwlOrwumKJvx4kjuGE9HgxnJDTu9Ee7lx1O0WukPnosbCyVEAgFe25kCWOXpDRIMXyw05tcbWNhTXWrcfGB7sfJekznXX1GhoVApkFtZhX36t6DhERL3GckNOLbusETKAMG9X6FzVouMIFahzwU3jwwFY594QEQ1WLDfk1E6UNQIAhoc496hNhz/OiIVCAn48WdF5uY6IaLBhuSGn1Wa2IKfCWm5GBDvvfJtzRfu747ejQwAAr3H0hogGKZYbclpnKpvQZpbh5apGiJeL6Dh2476Z1kX9vjxciqKaZsFpiIh6juWGnFZ2+yWpYcGekCRJcBr7MSbcC9Pj/WG2yHjjp1zRcYiIeozlhpySLMs4Vd5ebpxw4b7L6Ri92bS/CNVNBsFpiIh6huWGnFJelR61zW1QKiTEBriLjmN3pgzxw9hwL7S2WbBuZ77oOEREPcJyQ05pa3YlACDazw1alXOuSnwpkiR1jt6s21UAvcEkOBERUfex3JBT2nbKWm6G8pLURc0dFYwYf3fUt7Rh474i0XGIiLqN5YacTmubGbtzqwGw3FyKUiFhyfRYAMCbP+WizWwRnIiIqHtYbsjp7M6thsFkgZerGoGeWtFx7NqN48Pg76FFSX0rvjxUIjoOEVG3sNyQ0+mYbzM0yIO3gF+Gi1qJ1KnRAIDXtuVyQ00iGhRYbsjp/HTaWm7iA3lJqjtuT46Cu0aJ7PLGzmJIRGTPWG7IqZTWt+BMpR4KCRgS4CE6zqDg5abGguRIANxQk4gGB5Ybcio7TlcBAMaEe8NVw1vAu+uuaTFQKSTsyavBwcJa0XGIiC6J5Yacys851nIzPc5fcJLBJcTLFfMSwwBY594QEdkzlhtyGrIsY0eO9RbwqSw3PfbHmdbbwr89XobcyibBaYiILo7lhpxGdnkjqpoMcFUrMT7KW3ScQWdokCdmDw+ELANrfsoTHYeI6KJYbshpdMy3mRTjyy0Xeum+WdYtGT7OLEZFY6vgNEREF8ZyQ06jY77NNF6S6rWkKB+Mj/SG0WTB2z/ni45DRHRBLDfkFIwmC/bk1QDgfJu+kCQJf2zfUPPd3QVobG0TnIiI6HwsN+QUDhfXodlohq+7BsODuXhfX/xmRBBiA9zR2GrCxr3cUJOI7A/LDTmFjo0yJ8f6QqHglgt9oVBI+OOM9g01d+TBaOKGmkRkX1huyCns6iw3foKTOIbfjwtDoKcWZQ2t+DzrrOg4RERdsNyQwzOYzNifb11VN4Xlxia0KiXumhYDAHh9ey4sFm6oSUT2g+WGHF5WYR0MJgv8PbSIC+R+UrayIDkSHloVTlc0YUt2heg4RESdVKIDEHXHhj2FvX5uxolyAECIlwve5wRYm9G5qLEwORKvbc/F6m1nMHtEkOhIREQAOHJDTiC3Sg8AiA1wF5zE8dw1LQZqpYR9+bU4UFAjOg4REQCWG3JwbWYLimqaAQCx/rwkZWtBOhfcMI4bahKRfWG5IYdWVNMMk0WGp4sK/h4a0XEc0r0zrIv6fX+iHDkV3FCTiMRjuSGHll9tvSQV7ecOSeL6Nv0hLtADvxkZZN1QcztHb4hIPJYbcmgF1dZLUtH+nG/Tn+6baV3U79ODZ1HewA01iUgslhtyWGaLjIL2+TbRfm6C0zi2CVG+mBjtA6PZgre4oSYRCcZyQw6rrL4VRpMFLmoFgnQuouM4vCXTraM37+8tRLPRJDgNETkzlhtyWB3zbaJ83aHgfJt+N3tEEKL83FDf0oaPDxSLjkNEToyL+JHD6iw3vCQ1IJQKCalTovH3L49j1Q+nIUmSTUvlguRIm70WETk2jtyQQ5Jl+ZfJxH6cTDxQbkmKgItagWq9EdlljaLjEJGTYrkhh1StN6LJYIJKISHcx1V0HKfhrlVhYrQvAODnnCrBaYjIWbHckEMqaL8kFebjCpWS/5oPpJRYPygk67YXJXUtouMQkRPinBtySPlVvCTVHX3ZkPRivN00GB3mhcPF9fg5pwq3JEXY/D2IiC6Fv9KSQ/plZWJOJhZh6hB/AMDh4no0tLYJTkNEzoblhhxOY2sbqvVGSAAifTlyI0KErxuifN1glmXszq0WHYeInEyvyk1urm33j3n55ZcRHR0NFxcXJCcnY+/evRc995NPPkFSUhK8vb3h7u6OxMREvPvuuzbNQ4Nbx11SQToXuGqUgtM4r6lx1tGbvXk1MJosgtMQkTPpVbmJi4vDFVdcgffeew+trX3bR2bTpk1IS0vDihUrkJmZiYSEBMydOxcVFRUXPN/X1xd//etfsWvXLhw+fBipqalITU3Ft99+26cc5Dg6L0n585KUSCNDdfBxU6PZaMbBolrRcYjIifSq3GRmZmLs2LFIS0tDcHAw/vjHP15ytOVSVq5ciSVLliA1NRUjR47E6tWr4ebmhrVr117w/FmzZuGGG27AiBEjMGTIECxbtgxjx47Fjh07Lni+wWBAQ0NDlwc5tnN3AidxFJKEKe1zb3bmVMMiy4ITEZGz6FW5SUxMxPPPP4+SkhKsXbsWpaWlmDZtGkaPHo2VK1eisrKyW69jNBpx4MABzJkz55dACgXmzJmDXbt2Xfb5siwjIyMD2dnZmDFjxgXPSU9Ph5eXV+cjIoJ3bjgyQ5sZpXXW0cQolhvhkqJ8oFUpUNlkwOlyLupHRAOjTxOKVSoVbrzxRnz44Yd46qmnkJOTg4cffhgRERFYtGgRSktLL/n8qqoqmM1mBAUFdTkeFBSEsrKyiz6vvr4eHh4e0Gg0uPbaa/Hiiy/iN7/5zQXPXb58Oerr6zsfRUVFPf9GadAorGmGDMDHTQ0vV7XoOE5Pq1aes6gfJxYT0cDoU7nZv38/HnjgAYSEhGDlypV4+OGHcebMGXz//fcoKSnBvHnzbJWzC09PT2RlZWHfvn3497//jbS0NGzduvWC52q1Wuh0ui4Pclz53HLB7qQM8YMEIKeyCWX1fZujR0TUHb1axG/lypV46623kJ2djWuuuQbvvPMOrrnmGigU1q4UExODt99+G9HR0Zd8HX9/fyiVSpSXl3c5Xl5ejuDg4Is+T6FQIC4uDoD1EtmJEyeQnp6OWbNm9ebbIQdSwPk2dsfHTYNRoTocLWnA7txq/H5cmOhIROTgejVy8+qrr2LBggUoKCjAZ599huuuu66z2HQIDAzEm2++ecnX0Wg0mDBhAjIyMjqPWSwWZGRkICUlpdt5LBYLDAZDz74Jcjhmi4yiWuvITSQX77MrKe0Tiw8W1aLFaBachogcXa9Gbk6fPn3ZczQaDRYvXnzZ89LS0rB48WIkJSVh0qRJWLVqFfR6PVJTUwEAixYtQlhYGNLT0wFYJwgnJSVhyJAhMBgM+Prrr/Huu+/i1Vdf7c23Qg6kvKEVbWYZLmoFAjy1ouPQOaL93BCsc0FZQysOFNZiWvsaOERE/aFX5eatt96Ch4cHbrnlli7HP/zwQzQ3N3er1HSYP38+Kisr8dhjj6GsrAyJiYnYvHlz5yTjwsLCLqNCer0eDzzwAIqLi+Hq6orhw4fjvffew/z583vzrZADKayxjtpE+LhBIUmC09C5JElCSqwfPs06i9251ZgyxI//jIio30iy3PPFJ4YOHYrXXnsNV1xxRZfj27Ztw7333ovs7GybBbS1hoYGeHl5ob6+npOLB5HubPD44f4iHCyqw5XDAzFnRNBlz6eBZTRZ8OTmE2hts2BxShSGBffsv78FyZH9lIyIBoOe/Pzu1ZybwsJCxMTEnHc8KioKhYW232WYqDs6Rm4ifTnfxh5pVAokRVlvC9/F/aaIqB/1qtwEBgbi8OHD5x0/dOgQ/Pz8+hyKqKeaDSZU640ArJelyD4lx/hCAnCqvAnVTbwJgIj6R6/KzW233YaHHnoIW7Zsgdlshtlsxo8//ohly5bhD3/4g60zEl1WYftdUgEeWm6Wacf8PLQYGuQJANwtnIj6Ta8mFD/++OPIz8/H7NmzoVJZX8JisWDRokV44oknbBqQqDs6JxPzkpTdSxnih+zyRhworMWckUHQqlhGici2elVuNBoNNm3ahMcffxyHDh2Cq6srxowZg6ioKFvnI+qWIs63GTTiAj3g565Btd6IrKI6JMfwUjYR2Vavyk2HoUOHYujQobbKQtQrFllGUW0LAJabwUAhSZgc64f/HCnF7txqTIr2hcTbwonIhnpVbsxmM95++21kZGSgoqICFouly9d//PFHm4Qj6o6KBgOMJgu0KgUCdVy8bzAYH+mD746XobzBgLxqPWL9PURHIiIH0qtys2zZMrz99tu49tprMXr0aP7WRUJ1zLcJ93HlwnCDhKtGiXGRPtibV4NdZ6pZbojIpnpVbjZu3IgPPvgA11xzja3zEPVYEScTD0qTY/2wN68GJ0obUNdshLebRnQkInIQvboVXKPRdO7KTSQaF+8bnIJ1Lojxd4dFBvbm14iOQ0QOpFfl5i9/+Quef/559GLnBiKbajaaUNm+GBwX7xt8UmKtd0rty6uByWy5zNlERN3Tq8tSO3bswJYtW/DNN99g1KhRUKvVXb7+ySef2CQc0eUUt98l5eeugbu2Tzf/kQAjQnTQuajQ0GrC0ZJ6JEb4iI5ERA6gVz8NvL29ccMNN9g6C1GP8ZLU4KZUSJgU44sfTlRgd24Nyw0R2USvys1bb71l6xxEvcLJxINfUrQvfjxZgcKaZpTUtSDU21V0JCIa5Ho15wYATCYTfvjhB7z22mtobGwEAJSUlKCpqclm4Yguxbp4H0duBjudixqjQr0AAHvyOLGYiPquV+WmoKAAY8aMwbx58/Dggw+isrISAPDUU0/h4YcftmlAooupbDSgtc0CtVJCkM5FdBzqg8ntE4uzimrR2mYWnIaIBrtelZtly5YhKSkJtbW1cHX9ZQj5hhtuQEZGhs3CEV1KUefifW5QKrh432AW7eeGQE8t2swyMgtrRcchokGuV+Xmp59+wt/+9jdoNF0X3YqOjsbZs2dtEozocjrulIrw4RyNwU5q328KAPbk1nCZCSLqk16VG4vFArP5/KHj4uJieHp69jkUUXcU1/4yckODX2KENzQqBSqbDMit0ouOQ0SDWK/KzVVXXYVVq1Z1/lmSJDQ1NWHFihXckoEGRJvZgrKGVgDWPaVo8HNRKzEuwhsAsCe3WmwYIhrUelVunnvuOfz8888YOXIkWltbsWDBgs5LUk899ZStMxKdp6SuBRYZ8NSq4OWqvvwTaFBIjrFemjpe2oCGljbBaYhosOrVOjfh4eE4dOgQNm7ciMOHD6OpqQl33303Fi5c2GWCMVF/6ZhvE+7jyl3pHUiwlwui/dyQX92Mffk1mD0iSHQkIhqEer1evUqlwu23327LLETd1rG+TTjXt3E4ybF+neVm1rBA3glHRD3Wq3LzzjvvXPLrixYt6lUYou46d+SGHMuoUB3ctdb9pk6UNmB0mJfoSEQ0yPSq3CxbtqzLn9va2tDc3AyNRgM3NzeWG+pXzQYTavRGAEC4N0duHI1KocDEaB9sza7E7rxqlhsi6rFeTSiura3t8mhqakJ2djamTZuG999/39YZiboorrOO2vh7aOCqUQpOQ/1hUrQvJAC5lXpUtN8VR0TUXb3eW+rX4uPj8eSTT543qkNka+euTEyOydtNg+EhOgDcb4qIes5m5QawTjIuKSmx5UsSnYfzbZzD5BhfAEBmYS0MJu43RUTd16s5N1988UWXP8uyjNLSUrz00kuYOnWqTYIRXYh8zk7gERy5cWhDAj3g565Btd6IQ0X1ouMQ0SDSq3Lz+9//vsufJUlCQEAArrzySjz33HO2yEV0QbXNbWg2mqGUJIR4cSdwR6aQJCTH+uHrI6XYnVsNWZa5phERdUuvyo3FYrF1DqJu6dhPKsTbBSqlTa+qkh2aEOmD74+XoayhFZmFtZgQ5Ss6EhENAvzpQIMK59s4F1eNEmPDvQEA7+4qEBuGiAaNXo3cpKWldfvclStX9uYtiC6oiDuBO53JMX44UFCLr4+U4W/XGeDvoRUdiYjsXK/KzcGDB3Hw4EG0tbVh2LBhAIBTp05BqVRi/Pjxnefx+jjZktkio6SOIzfOJszHFeE+riiubcEH+4vwwKw40ZGIyM71qtxcf/318PT0xLp16+Dj4wPAurBfamoqpk+fjr/85S82DUkEABWNrWgzy9CqFPzt3clMjvHDR7XFWL+7EH+cMYT7TRHRJfVqzs1zzz2H9PT0zmIDAD4+PvjXv/7Fu6Wo3xTX/DJqo+CooFMZE+4Fbzc1zta1YGt2heg4RGTnelVuGhoaUFlZed7xyspKNDY29jkU0YVwvo3zUisVuDUpAgDw7m5OLCaiS+tVubnhhhuQmpqKTz75BMXFxSguLsbHH3+Mu+++GzfeeKOtMxIB+OVOqQjOt3FKC5MjAQDbTlWioFovOA0R2bNelZvVq1fjt7/9LRYsWICoqChERUVhwYIFuPrqq/HKK6/YOiMRDCYzyts3UOTIjXOK8nPHzKEBkGVgw55C0XGIyI71qty4ubnhlVdeQXV1deedUzU1NXjllVfg7u5u64xEKKlrhQxA56KCzlUtOg4JcsfkKADApv1FaG3jflNEdGF9WsSvtLQUpaWliI+Ph7u7O2RZtlUuoi6KOd+GAFwxPBBh3q6oa27Dfw6Xio5DRHaqV+Wmuroas2fPxtChQ3HNNdegtNT6l8zdd9/N28CpXxRxvg0BUCokLGife/MOJxYT0UX0qtz8+c9/hlqtRmFhIdzcfvlNev78+di8ebPNwhF16By58eXIjbObPzECaqWEQ0V1OFxcJzoOEdmhXpWb7777Dk899RTCw8O7HI+Pj0dBAX+bItuqbDSgrrkNEoAwb47cODt/Dy2uGRMCAHiPozdEdAG9Kjd6vb7LiE2HmpoaaLVcOZZsq+O38wBPLVzUSrFhyC4sSrFOLP48qwT1zW2C0xCRvelVuZk+fTreeeedzj9LkgSLxYKnn34aV1xxhc3CEQHAoaI6AJxMTL8YH+mDESE6GEwWfHigSHQcIrIzvdpb6umnn8bs2bOxf/9+GI1G/M///A+OHTuGmpoa/Pzzz7bOSE4uq7geADfLpF9IkoQ7Jkfh/z49gvV7CnHX1BgouN8UEbXr1cjN6NGjcerUKUybNg3z5s2DXq/HjTfeiIMHD2LIkCG2zkhOTJblzpGbCI7c0DnmJYbCU6tCXpUeP5+pEh2HiOxIj0du2tracPXVV2P16tX461//2h+ZiDoVVDejvqUNKoWEIC/O56JfuGtVuGlCON7emY93dxVgenyA6EhEZCd6PHKjVqtx+PDh/shCdJ6s9lGbEC8XqBR9WnOSHNDtk61r3vxwohwldS2C0xCRvejVT4vbb78db775pq2zEJ2no9xwfRu6kLhAT6TE+sEiA+/v5X5TRGTVqwnFJpMJa9euxQ8//IAJEyact5/UypUrbRKO6FD7beBcmZgu5o6UKOzKrcb7e4vwX1fGQ6PiCB+Rs+tRucnNzUV0dDSOHj2K8ePHAwBOnTrV5RxJ4h0LZBtGkwXHShoA8DZwurjfjAxCoKcWFY0GfHusDNcnhIqORESC9ajcxMfHo7S0FFu2bAFg3W7hhRdeQFBQUL+EI+eWXdYIo8kCL1c1/Nw1ouOQnVIrFbhtUiSezziNd3cXsNwQUc/m3Px61+9vvvkGer3epoGIOmQV1QIAEiK8OSJIl3TbpEgoFRL25tXgRGmD6DhEJFiv5tx0+HXZIbKlrCLr4n2J4V6Ck5A92LDn0hOGR4bocORsPf726VHcNCH8kud26NhhnIgcS49GbiRJOu83aP5GTf2lYzJxQoS30Bw0OEyN8wcAZBXXobGV+00RObMejdzIsow777yzc3PM1tZW3HfffefdLfXJJ5/YLiE5pYbWNpypbAJgLTffHSsXnIjsXaSvGyJ93VBY04zduTX4zUjOBSRyVj0qN4sXL+7y59tvv92mYYg6HC2uhyxb95Py9+DKxNQ9U+P8Ubi3EHvyqjFrWADUSt4WTuSMelRu3nrrrf7KQdTFwfbF+3hJinpiZIgO3m5q1DW3IauwDhNjfEVHIiIB7OLXmpdffhnR0dFwcXFBcnIy9u7de9Fz16xZg+nTp8PHxwc+Pj6YM2fOJc+nwaljs8zEcG+hOWhwUSokTBlinXuz40wVb3ogclLCy82mTZuQlpaGFStWIDMzEwkJCZg7dy4qKioueP7WrVtx2223YcuWLdi1axciIiJw1VVX4ezZswOcnPoTJxNTbyVF+UCrUqCy0YDTFU2i4xCRAMLLzcqVK7FkyRKkpqZi5MiRWL16Ndzc3LB27doLnr9+/Xo88MADSExMxPDhw/HGG2/AYrEgIyPjgucbDAY0NDR0eZB9K6tvRXmDAQoJGB2mEx2HBhkXtRJJUT4AgB05VYLTEJEIQsuN0WjEgQMHMGfOnM5jCoUCc+bMwa5du7r1Gs3NzWhra4Ov74Wvraenp8PLy6vzERERYZPs1H86NsscGuQJN02flmIiJzVliD8kADkVTShraBUdh4gGmNByU1VVBbPZfN72DUFBQSgrK+vWazzyyCMIDQ3tUpDOtXz5ctTX13c+ioqK+pyb+ldHuRkX6S00Bw1ePu4ajAqzLv7482mO3hA5G+GXpfriySefxMaNG/Hpp5/CxcXlgudotVrodLouD7JvHZOJEziZmPpgWseifkV1qG/hon5EzkRoufH394dSqUR5edcF2srLyxEcHHzJ5z777LN48skn8d1332Hs2LH9GZMGkNki48hZ67YLnExMfRHp64YYf3eYZRk/c+4NkVMRWm40Gg0mTJjQZTJwx+TglJSUiz7v6aefxuOPP47NmzcjKSlpIKLSAMmtbEKTwQRXtRLxgR6i49AgN3NoAABgb34Nmo0mwWmIaKAIvyyVlpaGNWvWYN26dThx4gTuv/9+6PV6pKamAgAWLVqE5cuXd57/1FNP4dFHH8XatWsRHR2NsrIylJWVoamJt3w6go75NmPCvKDi6rLUR/GBHgjxcoHRZMHu3BrRcYhogAj/6TF//nw8++yzeOyxx5CYmIisrCxs3ry5c5JxYWEhSktLO89/9dVXYTQacfPNNyMkJKTz8eyzz4r6FsiGsjpXJuZO4NR3kiRhRrx19GbnmSoYTRbBiYhoINjFfbZLly7F0qVLL/i1rVu3dvlzfn5+/wciYToW70uM8BEbhBzG6DAvfHe8DLXNbThQUIOU9hWMichxCR+5IerQ2mbGydJGABy5IdtRKiRMbx+9+SmnCmYLt2QgcnQsN2Q3jpU0wGSR4e+hQZi3q+g45EAmRPnAXatCXXMbDrePDhKR42K5Ibtx7vo2kiSJDUMORa1UYOoQPwDA9tOV3FCTyMGx3JDd+GUysbfQHOSYkmP8oFUpUN5gwIn2y59E5JhYbshu/DKZ2FtoDnJMrholJsdaR29+PFnO0RsiB8ZyQ3ahVm9EQXUzAGBsOCcTU/+YFucPjVKBkvpWnCzj6A2Ro2K5IbvQMWoT4+8ObzeN2DDksNy1KqS0z73J4OgNkcNiuSG7cKiofT8pjtpQP+scvalrRcaJCtFxiKgfsNyQXcgqqgXAycTU/9y1qs65N89nnOboDZEDYrkh4WRZxqFi7gROA2davHX05sjZevx4kqM3RI6G5YaEK6huRo3eCI1SgVGhOtFxyAl4aFWYHOsLgKM3RI6I5YaEyyy0XpIaFaaDVqUUnIacxbT4ALiqlThcXI8t2Ry9IXIkLDckXEe5GR/JzTJp4HhoVVg0JQoAsPL7U7Bwzykih8FyQ8JlFtQBYLmhgXfv9Fi4a5Q4erYBXx8tFR2HiGyE5YaE0htMOFnWAAAYH+UtNgw5HT8PLZbMiAUAPPfdKbSZLYITEZEtsNyQUIeK62CRgRAvF4R4cSdwGnj3TI+Fn7sGeVV6fLC/SHQcIrIBlhsS6mBhHQBekiJxPLQqLL0yDgDw/A+n0WI0C05ERH3FckNCZRZYJxOPi/QWG4Sc2oLkSIT7uKKi0YC3duaJjkNEfcRyQ8LIsoyDRXUAgPFRHLkhcbQqJdJ+MxQAsHrrGdQ3twlORER9wXJDwuRz8T6yI/MSwzA82BMNrSa8si1HdBwi6gOWGxKm45LUaC7eR3ZAqZDw33OHAQDe/jkfpfUtghMRUW+x3JAwXLyP7M2VwwMxMdoHBpMFz2zOFh2HiHqJ5YaE6bxTivNtyE5IkoS/XTsSAPDJwbPIap8TRkSDC8sNCdFl8T6O3JAdSYjwxk3jwwEA//zyGDfVJBqEWG5IiI7F+0K9XBDs5SI6DlEX/3P1MLhplMgsrMMXh0pExyGiHmK5ISE6LkmN46gN2aEgnQsemDUEAPDkNye5sB/RIMNyQ0Jw8T6yd/dMj0WYtytK61vx+vZc0XGIqAdYbmjAcfE+Ggxc1Eosv2Y4AGD1tjO8NZxoEGG5oQHHxftosLh2TAgmRvugpc2MJ785KToOEXUTyw0NOC7eR4OFJEl47LpRkCTg86wS7DxTJToSEXUDyw0NuP0FNQCACbwkRYPAmHAv3J4cBQD422dHYTBxcjGRvWO5oQG3N89abibF+AlOQtQ9D88dBn8PLXIr9Xh9GycXE9k7lhsaUFVNBpyp1AMAkjhyQ4OEl6saj143AgDw0pYcFFTrBSciokthuaEBtT/fOmozNMgDPu4awWmIuu93CaGYGucHg8mCxz7nysVE9ozlhgbU3jzrZOJJMb6CkxD1jCRJeHzeaGiUCmw7VYmvj5SJjkREF8FyQwNqX/vIzcRolhsafGIDPHB/+8rF//jyGBpa2wQnIqILYbmhAdPY2oZjJfUAOHJDg9f9s4Yg2s8NFY0GpH/NtW+I7BHLDQ2YzELrZpnhPq4I8XIVHYeoV1zUSqTfOBYA8P7eQvx0ulJwIiL6NZYbGjD7Om8B56gNDW4pQ/ywKMW69s0jHx1GIy9PEdkVlhsaMHvb59tM4nwbcgCPXD0cEb6uKKlvxRNfnxAdh4jOwXJDA8JgMiOrfbPMiRy5IQfgrlXh6ZsSAADv7y3C9lO8PEVkL1huaEAcLq6H0WSBv4cGsf7uouMQ2UTKED8sbr889b8f8/IUkb1guaEB0bHlwsRoX0iSJDgNke088tvhiPR1Q0l9K/79H16eIrIHKtEByLFs2FN4weOfZ50FACgk6aLnEA00W/27eNWoILzxUx427ivCrGEBuHp0iE1el4h6hyM31O8ssoyC6mYAQDQvSZEDivX3wIx4fwDA/3x0GGfrWgQnInJuLDfU78rqW2EwWaBVKRDi5SI6DlG/+M3IYIT7uKKh1YQ/b8yCyWwRHYnIabHcUL/Lb99BOdLXDQrOtyEHpVRImJ8UAQ+tCnvza/DSlhzRkYicFssN9bv8Kmu5ieElKXJwfh5a/PuG0QCAFzJOd06kJ6KBxQnF1K9kWUZ++3ybKD+WG3J88xLDsP1UFT7OLMafNh7E18umw9tNIzoWXUR/3OCwIDnS5q9JPcORG+pXlY0GNBlMUCkkhPtwPylyDv+YNwox/u4oqW/FnzZlwWyRRUciciosN9SvzrRfkoryc4NayX/dyDl4aFV4acE4uKgV2JpdiVU/nBIdicip8KcN9avcyiYAwJAAD8FJiAbWqFAvPNm+e/iLP+bg22NlghMROQ+WG+o3FllGbqV15CaW5Yac0O/HheGuqTEAgL98cAg5FY2CExE5B5Yb6jdl9a1oaTNDq1IgzJvzbcg5Lb9mOCbH+qLJYMK97x5AA/efIup3LDfUb860X5KK9nOHUsH1bcg5qZUKvLRgPEK8XJBbqUcaJxgT9TuWG+o3HZekhgTwFnBybv4eWrx2xwRoVAr8cKICj391HLLMgkPUX1huqF+YLTLyqjnfhqjD2HBv/L9bEwEAb+/Mx5s78sQGInJgLDfUL87WNsNossBVrUQw95MiAgBcOzYEf71mBADgX/85gf8cLhWciMgxsdxQvzjdcQt4oAf3kyI6xz3TY3DnlGgAwJ8/yMK+fG7RQGRrLDfUL3LKreUmPpCXpIjOJUkSHr1uJK4aGQSjyYIl7+znLeJENia83Lz88suIjo6Gi4sLkpOTsXfv3ouee+zYMdx0002Ijo6GJElYtWrVwAWlbmttM6Oo1rqfVBzLDdF5lAoJz/9hHBIjvFHX3IYFa/Ygr301byLqO6HlZtOmTUhLS8OKFSuQmZmJhIQEzJ07FxUVFRc8v7m5GbGxsXjyyScRHBw8wGmpu3Irm2CRAX8PDXy4YSDRBblqlFh750QMC/JERaMBC9bsRlFNs+hYRA5BaLlZuXIllixZgtTUVIwcORKrV6+Gm5sb1q5de8HzJ06ciGeeeQZ/+MMfoNVqBzgtddfpCuslqbhAT8FJiOybr7sG65ckY0iAO0rrW3Hbmt04W9ciOhbRoCes3BiNRhw4cABz5sz5JYxCgTlz5mDXrl02ex+DwYCGhoYuD+pfORWcb0PUXf4eWry/ZDJi/N1RXNuCBWt2o6y+VXQsokFNWLmpqqqC2WxGUFBQl+NBQUEoK7PdBnPp6enw8vLqfERERNjstel8NXojqvVGKCQg1p+L9xF1R6DOBRuWJCPC1xUF1c24bc1uFNfyEhVRbwmfUNzfli9fjvr6+s5HUVGR6EgO7XT7XR+Rvu7QqpWC0xANHiFerthwz2SEebsir0qPm1/dhVPlvIuKqDeElRt/f38olUqUl5d3OV5eXm7TycJarRY6na7Lg/rPqTLrX8bxQbwkRdRTEb5u+Oj+FMQHeqCsoRW3rN6FAwW1omMRDTrCyo1Go8GECROQkZHRecxisSAjIwMpKSmiYlEfGExm5LQv3jcsiJOJiXojxMsVH96XgvGR3qhvacPCN3ZjS/aF7yAlogsTelkqLS0Na9aswbp163DixAncf//90Ov1SE1NBQAsWrQIy5cv7zzfaDQiKysLWVlZMBqNOHv2LLKyspCTkyPqW6Bz7MmtQZtZhs5FhRBuuUDUa95uGrx3TzJmDQtAa5sFS9btx6Z9haJjEQ0aKpFvPn/+fFRWVuKxxx5DWVkZEhMTsXnz5s5JxoWFhVAofulfJSUlGDduXOefn332WTz77LOYOXMmtm7dOtDx6Vd+PGn97XJokCckbrlA1CduGhXWLErC/3x0GJ8ePItHPj6CYyUNePS6kVArHX66JFGfCC03ALB06VIsXbr0gl/7dWGJjo6GLMsDkIp6Y2v70PmwYF6SIrIFtVKB525JQIy/O1Z+fwrv7CpAdlkjXlk4Hn4eXOuL6GKElxtyDLmVTcivboZSkhAXwMnE5Lw27LH95SN/Dy3WLErCnzdlYU9eDX730s947Y4JGB3mZfP3InIEHNskm9iSXQkAiPZ34y3gRP3gNyOD8NmDUxDj746zdS248dWdWLsjDxYLR7OJfo3lhmzix5PWW/qHBfNWe6L+Ehfoic8enIrZwwNhNFnwz6+OY/Fbe1HewBWNic7FckN9Vt/chj25NQCA4ZxvQ9SvvFzVeGNxEh6fNwpalQI/na7C3FXb8fWRUtHRiOwG59xQn23JroDJImNokAf8OcmRqF/8ei6PUqHA/bOG4IP9RSipa8UD6zMxJswL14wJgZeruluvuSA5sj+iEgnHkRvqs++OW/cCu2qk7VaWJqLLC/R0wX0zh2DW0ABIAI6crcf/+/4Utp+qhMliER2PSBiWG+qT1jYztrZPJr5qVNBlziYiW1MpFLhqVDAevCIOkb5uMJot2HysDC/+mIOciibR8YiEYLmhPtl5pgrNRjOCdS4Yw9tSiYQJ9XbFvTNicdP4cLhrlKhsNGDtz3l4c0cuCqr1ouMRDSjOuaE++e6Y9S6pq0YFcVViIsEUkoQJUT4YGaLD9yfKsS+vBmcq9ThTmYv4QA/MHhGESF830TGJ+h3LDfWa2SLjhxPWcjN3FOfbENkLV40Sv0sIxfR4f2zNrsCBglqcrmjC6YomxPq7Y3KsH0aEcNkGclwsN9Rre/KqUdVkhJerGpNifEXHIaJf8XHT4IZx4Zg5NBBbTlbgYFEtcqv0yK3Sw8tVjYbWNvxhYgS3ciCHw3JDvfbVYeu6GlePCuZGfkR2zNddg5smhGP2iEDsyavBvvwa1Le04Zlvs7Hqh1OYOTQA8xLDMGdEEFw1XGGcBj+WG+oVk9mCzUett4BflxAiOA0RdYe3mwZzRwXjyuGBOHK2HqfLG3GouB4/nKjADycq4K5RYu6oYFwzJgRT4/xZdGjQYrmhXtl5pho1eiN83TVIifUTHYeIekCtVGB8pA+evSUBp8sb8XlWCT7LOovi2hZ8cvAsPjl4FlqVAtPi/DF7RBBmjwhEkM5FdGyibmO5oV75T8clqdHBUPGSFNGgFR/kiYfnDsNfrhqKzMJafJFVgh9OVOBsXQsyTlYg42QF8CkwJswLs0cEYvbwIIwK1UGh4N2RZL9YbqjHjCbrImEAcN1YXpIicgSSJGFClC8mRPni77+TcbKsERknyvHDiQocKq7DkbP1OHK2Hqt+OA1/Dy1mDQvAlcMDMS3eHzqX7m33QDRQWG6ox3bkVKK+pQ0Bnlokx/CSFJGjkSQJI0J0GBGiw9Ir41HZaMCWkxX44UQ5duRUoarJgI8OFOOjA8VQKaxr61w5PBBXDA9EfKAH17wi4VhuqMc+PnAWgHXURsmhaSKHF+Cpxa0TI3DrxAgYTGbsz6/FlpMV+DG7ArmVeuzJq8GevBqkf3MSYd6u+M3IIPx2dDCSon35dwQJwXJDPVLf3Ibvj1sX7rtpfLjgNETUF7/eabwnYgM8EBvggeomA06VNyK7vBG5lXqcrWvB2zvz8fbOfLhrVRgVosOoMB1i/T36VHS4gzn1BMsN9chXR0pgNFswPNgTo0K5wimRs/Pz0CLFQ4uUIf4wmiw4U9mEYyX1OF7aAL3BhL35NdibXwNXtRIjQnQYG+6FIQF9KzpEl8NyQz3y8YFiANZRG15XJ6JzaVSKzrk6ZouM3MomHC1pwPGSeuiNZmQW1iKzsBZuGiVGh3phbLgXov3doeDfJWRjLDfUbbmVTcgsrINCAuYlhoqOQ0R2TKmQEB/kifggT8xLDEV+lR5HztbjaEnXER1PFxXGhHlhbLg3Inxc+UsT2QTLDXXbx5nWUZsZQwMQyAW9iKibFJLUOUfnurGhyKvS43BxHY6W1KOx1YSdZ6qx80w1vN3UGBvmjbHhXgjxcmHRoV5juaFuaTNbsGmftdzcMiFCcBoiGqyUCglxgR6IC/TA7xJDkVPehMNnrXN06prbsP10JbafroS/hwYjQ7wwNNgDUb7uA5ZPlmXUt7ShvKEVZQ0GVDS0otFggt5ggtFkgQxAIQFuGhXctSoEemoRrHNBlJ8bvN00A5aTLo3lhrrlu2PlqGoyIMBTi6tGBYmOQ0QOQKVQYHiIDsNDdDCaLMgub8Th4jpklzWiqsnYWXS0KgV+zqnC9KH+SIzwxtAgT5ts1lurNyK3sqmzyJQ3tKK8oRUGk6UbzzYCAE6U/nIk0FOLkaE6TI/3R4SvW5/zUe+x3FC3rN9TAAD4w8QI7gBORDanUSkwJswLY8K8YGgz42SZ9fbyU+WNaDaasflYWefK6FqVAiNDdRgb5oUhgR4I93FFkM4FXq5qeLqooVRIkAC0tJmhN5hQ0WjA2doWnK2zPgqrm5Fd3ojKRsMFsygk69o+QTqXztd116igUSmgkACzLKPZYEajwYTyhlaU1LXgbG0LKhoNqMiuxIxntmDW0AAsvTIeE6J8BvBTpA4sN3RZZyqbsPNMNRQS8IdJXGuCiPqXVq1EQoQ3EiK8YZFllNS1QKtSYm9+NQ4XW+fpHCysw8HCuj6/l4+bGsHtJSbIy/q//h4aqBQ9+yWu2WjCqfImZBbUIqeyCVuyK7EluxKzhwdi+TXDERfo2ees1H0sN3RZ77cv9HXFsECEebsKTkNEzkQhSQj3cWtfxC8eFouMgppm64Tks/XIr25GUU0zqpqMaGhpg9Hc9ZKSq1oJf08NwrxdEertinBvV4T7uCE+yANDgzzxeVaJTXK6aVRIjPBGYoQ3pgzxw6tbz+CjzGJknKzA9tOVuH9WHB6YNQQuaqVN3o8ujeWGLqnJYMIH+4sAALdPjhKchoicnUIhIcbfHTH+7piXGNbla7Iso80swyJbH1qVUshigdH+7njq5rH448xYPP7VcWzJrsQLGaex+WgpXlowHkODOIrT3zh5gi7pg31FaGg1IdbfHTOHBoiOQ0R0UZIkQaNSwEWthJtGJXwV5NgAD6y9cyJeWjAO/h4anCpvwu9e2oH39xZClmWh2Rwdyw1dlMlswZs78gAAd0+PgYLLpRMR9YgkSbhubCi+WTYD0+P90dpmwfJPjuCvnx1Fm7k7d2VRb7Dc0EVtPlaGs3Ut8HXXcJNMIqI+CPDUYl3qJPz33GGQJOumpbe/sQd1zUbR0RwSyw1dkCzLWLM9FwBwx+QoToIjIuojhULCg1fEYc0dSXDXKLEnrwbzX9uN8oZW0dEcDssNXdCOnCocKq6HVqXAHSmcSExEZCtzRgbhkwemIkinRXZ5I256dSfyq/SiYzkUlhs6jyzL+H/fnwIALEiOhL+HVnAiIiLHMizYEx/dNwXRfm4orm3Bzat34XhJg+hYDoPlhs6z/XQVMgvroFUpcP/MIaLjEBE5pAhfN3x43xSMCNGhqsmA+a/vwr78GtGxHALXuaEuZFnGyvZRm9snR3H3byKyCxvaFxN1NAGeWmy8dzLuWbcP+/JrsejNvVh750SkDPETHW1Q48gNdZFxogKHiurgolbgPo7aEBH1Oy9XNd65Kxkzhgagpc2M1Lf3YmdOlehYgxrLDXVqM1vwxDcnAACLp0QjwJNzbYiIBoKrRonX75iAK4YFoLXNgtS392HHaRac3mK5oU7v7S5AbqUefu4aPHhFnOg4REROxUWtxOo7JmD28EAYTBbcvW4ftp+qFB1rUGK5IQBAXbMRq344DQD4y1XDoHNRC05EROR8tColXrl9POaMCILBZME97+zH1uwK0bEGHZYbAgA8990p1Le0YXiwJ+ZPjBAdh4jIaWlVSryycDzmjgqC0WTBve8cwI8ny0XHGlRYbgj78mvw3p4CAMBj148UvtkcEZGz06gUeGnBePx2dDCMZgv++O4B/HCcBae7WG6cXGubGY98fBiyDNyaFI4pQ/xFRyIiIgBqpQIv3DYO144JQZtZxv3rD+C7Y2WiYw0KLDdO7qUfc5BbqUeApxZ/vWak6DhERHQOtVKB5/+QiOvGWgvOA+szsfloqehYdo/lxontya3GK1tzAACPzxsFLzdOIiYisjcqpQKr5idiXmIoTBYZD244iK+PsOBcCsuNk6puMuChjQdhkYGbxofj6tEhoiMREdFFqJQKrLw1ETeMC4PZIuO/3j+Irw6XiI5lt1hunJDFIuPhDw+hvMGA2AB3/HPeKNGRiIjoMpQKCc/ekoCbxofDbJHx0PsHHXZbir5iuXFCT3+bjS3ZldCoFHh5wXi4a7nFGBHRYKBUSHj65rH4w8QIWGTg/z49gqc2n4TFIouOZldYbpzMxr2FWL3tDAAg/YYxGBGiE5yIiIh6QqmQkH7jGPx5zlAAwKtbz2DZpiwYTGbByewHy40T2ZJdgb99dhQA8NCVcbhpQrjgRERE1BuSJGHZnHg8e0sCVAoJXx4qwR1v7EVds1F0NLvAcuMktmZX4I/vHoDJIuN3CaH482+Gio5ERER9dPOEcKy7axI8tSrsza/B7176GcdK6kXHEo7lxglsOVmBe989AKPJgrmjgvDsLQmQJK5CTETkCKbG+eOj+6cgzNsVhTXNuPGVndi0rxCy7LzzcFhuHNy7u/Jx97p9ncXmxdvGQ6PiP3YiIkcyLNgT/3loGmYNC4DBZMEjHx/BgxsyUat3zstU/CnnoAwmM1Z8fhSPfn4MFtk6dMliQ0TkuLzdNFi7eCL+e+4wqBQSvj5ShrmrtmPzUefbsoE/6RxQTkUjfv/yTqzbZd0M83+uHoZnbh7LYkNE5OAUCgkPXhGHTx6YgtgAd1Q0GnDfewdwz7r9KKppFh1vwPCnnQNpbTPjhYzTuPaFHThR2gBfdw3eXJyEB2bFcY4NEZETGRvuja8fmo4HrxgClULCDyfKMfu5bfjXV8ed4o4qrt7mAMwWGV8dLsHK70+hoNrazKfH++O5WxIQqHMRnI6IiERwUSvx33OHY15iGP7x5TH8nFONN3bk4f29hbh9chTunh6DQE/H/BnBcjOINRtN+PJQCV7fnoszlXoAQJBOi79eOxLXjw3haA0REWFokCfeuzsZ205V4qnN2ThR2oDXtudi7c95uHp0CBYmR2JStC8UCsf5mcFyM8iYzBbszavBV0dK8WVWCRoNJgCAl6sa90yLQeq0GHhwOwUiIjqHJEmYNSwQM4cG4MeTFXh5Sw4yC+vw5aESfHmoBGHerrhubAiuHB6IcZE+g36Opl38FHz55ZfxzDPPoKysDAkJCXjxxRcxadKki57/4Ycf4tFHH0V+fj7i4+Px1FNP4ZprrhnAxAPHaLLgdEUjDhXV4+czVdiZU4Xa5rbOr0f5uWHBpEgsSI6Ep4taYFIiIrJ3kiRh9oggzB4RhKNn67F+TwG+PFSKs3UteG17Ll7bngtXtRITY3wxdYgfkmP9MCzIE64apejoPSLJglf52bRpExYtWoTVq1cjOTkZq1atwocffojs7GwEBgaed/7OnTsxY8YMpKen47rrrsOGDRvw1FNPITMzE6NHj77s+zU0NMDLywv19fXQ6exjXyWT2YKKRgNK6lpQUt+K0roWnCpvwvHSBuRUNKLN3PUfkY+bGnNHBeP6hFCkxPrZ1VAid6glIme3IDlSdIQeaW0zY2t2Bb4+UoadZ6pQ1dR1wrEkAZG+bhgW5IlhwZ6I8HVDiJcLgnUuCPZyGbBfrHvy81t4uUlOTsbEiRPx0ksvAQAsFgsiIiLwX//1X/jf//3f886fP38+9Ho9vvrqq85jkydPRmJiIlavXn3Z9+uvclPR2Irvj5fD0GaB0WyBoc0Cg8kMo8kCg8nS/r9mGM0WNLaa0NDShrqWNtS3tKGhpQ2X2tBV56LCyFAdJsf6YVqcPxIivKFW2ueQIcsNETm7wVZuziXLMrLLG/FzTjV+zqnCoaI6VF9mIUBXtRJerurOh85VjWHBHvjvucNtmq0nP7+FXpYyGo04cOAAli9f3nlMoVBgzpw52LVr1wWfs2vXLqSlpXU5NnfuXHz22WcXPN9gMMBgMHT+ub7euudGQ0NDH9N3daKwFss37u3181UKCYE6LUJ0rgjy0iLK1x3DQzwxPNgTod6uXSYHt+ib0GKL0P2gWd8oOgIRkVC2/vky0ELdgFvG+uGWsX4AgOomA3LKm3C6shE5FXqU1regosGAsoYWNLaaoTcA+iag5JzXqIzwxh9TQm2aq+Nz7c6YjNByU1VVBbPZjKCgoC7Hg4KCcPLkyQs+p6ys7ILnl5VdeAXG9PR0/OMf/zjveERERC9T95880QGIiKjPlogOYAeKAHj9pX9eu7GxEV5eXpc8xy4mFPen5cuXdxnpsVgsqKmpgZ+fn9PeKt3Q0ICIiAgUFRXZzbwjkfh5nI+fyfn4mZyPn8n5+Jmcz1afiSzLaGxsRGjo5UeEhJYbf39/KJVKlJeXdzleXl6O4ODgCz4nODi4R+drtVpotdoux7y9vXsf2oHodDr+x3cOfh7n42dyPn4m5+Nncj5+JuezxWdyuRGbDkJnpWo0GkyYMAEZGRmdxywWCzIyMpCSknLB56SkpHQ5HwC+//77i55PREREzkX4Zam0tDQsXrwYSUlJmDRpElatWgW9Xo/U1FQAwKJFixAWFob09HQAwLJlyzBz5kw899xzuPbaa7Fx40bs378fr7/+ushvg4iIiOyE8HIzf/58VFZW4rHHHkNZWRkSExOxefPmzknDhYWFUCh+GWCaMmUKNmzYgL/97W/4v//7P8THx+Ozzz7r1ho3ZKXVarFixYrzLtc5K34e5+Nncj5+JufjZ3I+fibnE/GZCF/nhoiIiMiW7HMlOCIiIqJeYrkhIiIih8JyQ0RERA6F5YaIiIgcCsuNk3n55ZcRHR0NFxcXJCcnY+/e3u+H5Qi2b9+O66+/HqGhoZAk6aJ7lDmL9PR0TJw4EZ6enggMDMTvf/97ZGdni44l1KuvvoqxY8d2LkCWkpKCb775RnQsu/Lkk09CkiT86U9/Eh1FmL///e+QJKnLY/hw224cORidPXsWt99+O/z8/ODq6ooxY8Zg//79/f6+LDdOZNOmTUhLS8OKFSuQmZmJhIQEzJ07FxUVFaKjCaPX65GQkICXX35ZdBS7sG3bNjz44IPYvXs3vv/+e7S1teGqq66CXq8XHU2Y8PBwPPnkkzhw4AD279+PK6+8EvPmzcOxY8dER7ML+/btw2uvvYaxY8eKjiLcqFGjUFpa2vnYsWOH6EhC1dbWYurUqVCr1fjmm29w/PhxPPfcc/Dx8en/N5fJaUyaNEl+8MEHO/9sNpvl0NBQOT09XWAq+wFA/vTTT0XHsCsVFRUyAHnbtm2io9gVHx8f+Y033hAdQ7jGxkY5Pj5e/v777+WZM2fKy5YtEx1JmBUrVsgJCQmiY9iVRx55RJ42bZqQ9+bIjZMwGo04cOAA5syZ03lMoVBgzpw52LVrl8BkZM/q6+sBAL6+voKT2Aez2YyNGzdCr9dzyxcADz74IK699touf684s9OnTyM0NBSxsbFYuHAhCgsLRUcS6osvvkBSUhJuueUWBAYGYty4cVizZs2AvDfLjZOoqqqC2WzuXPm5Q1BQEMrKygSlIntmsVjwpz/9CVOnTnX6FcCPHDkCDw8PaLVa3Hffffj0008xcuRI0bGE2rhxIzIzMzu3xnF2ycnJePvtt7F582a8+uqryMvLw/Tp09HY2Cg6mjC5ubl49dVXER8fj2+//Rb3338/HnroIaxbt67f31v49gtEZJ8efPBBHD161OnnDQDAsGHDkJWVhfr6enz00UdYvHgxtm3b5rQFp6ioCMuWLcP3338PFxcX0XHswm9/+9vO/z927FgkJycjKioKH3zwAe6++26BycSxWCxISkrCE088AQAYN24cjh49itWrV2Px4sX9+t4cuXES/v7+UCqVKC8v73K8vLwcwcHBglKRvVq6dCm++uorbNmyBeHh4aLjCKfRaBAXF4cJEyYgPT0dCQkJeP7550XHEubAgQOoqKjA+PHjoVKpoFKpsG3bNrzwwgtQqVQwm82iIwrn7e2NoUOHIicnR3QUYUJCQs77BWDEiBEDcrmO5cZJaDQaTJgwARkZGZ3HLBYLMjIyOHeAOsmyjKVLl+LTTz/Fjz/+iJiYGNGR7JLFYoHBYBAdQ5jZs2fjyJEjyMrK6nwkJSVh4cKFyMrKglKpFB1RuKamJpw5cwYhISGiowgzderU85aSOHXqFKKiovr9vXlZyomkpaVh8eLFSEpKwqRJk7Bq1Sro9XqkpqaKjiZMU1NTl9+s8vLykJWVBV9fX0RGRgpMJsaDDz6IDRs24PPPP4enp2fnfCwvLy+4uroKTifG8uXL8dvf/haRkZFobGzEhg0bsHXrVnz77beiownj6el53jwsd3d3+Pn5Oe38rIcffhjXX389oqKiUFJSghUrVkCpVOK2224THU2YP//5z5gyZQqeeOIJ3Hrrrdi7dy9ef/11vP766/3/5kLu0SJhXnzxRTkyMlLWaDTypEmT5N27d4uOJNSWLVtkAOc9Fi9eLDqaEBf6LADIb731luhowtx1111yVFSUrNFo5ICAAHn27Nnyd999JzqW3XH2W8Hnz58vh4SEyBqNRg4LC5Pnz58v5+TkiI4l3JdffimPHj1a1mq18vDhw+XXX399QN5XkmVZ7v8KRURERDQwOOeGiIiIHArLDRERETkUlhsiIiJyKCw3RERE5FBYboiIiMihsNwQERGRQ2G5ISIiIofCckNEREQOheWGiBzO1q1bIUkS6urqREchIgFYboiIiMihsNwQERGRQ2G5IaJ+U1lZieDgYDzxxBOdx3bu3AmNRoOMjIwLPmfKlCl45JFHznsdtVqN7du3AwDeffddJCUlwdPTE8HBwViwYAEqKioumuPvf/87EhMTuxxbtWoVoqOjuxx74403MGLECLi4uGD48OF45ZVXOr9mNBqxdOlShISEwMXFBVFRUUhPT+/Ox0BEA4zlhoj6TUBAANauXYu///3v2L9/PxobG3HHHXdg6dKlmD179gWfs3DhQmzcuBHn7um7adMmhIaGYvr06QCAtrY2PP744zh06BA+++wz5Ofn48477+xT1vXr1+Oxxx7Dv//9b5w4cQJPPPEEHn30Uaxbtw4A8MILL+CLL77ABx98gOzsbKxfv/68ckRE9kElOgARObZrrrkGS5YswcKFC5GUlAR3d/dLjnjceuut+NOf/oQdO3Z0lpkNGzbgtttugyRJAIC77rqr8/zY2Fi88MILmDhxIpqamuDh4dGrnCtWrMBzzz2HG2+8EQAQExOD48eP47XXXsPixYtRWFiI+Ph4TJs2DZIkISoqqlfvQ0T9jyM3RNTvnn32WZhMJnz44YdYv349tFrtRc8NCAjAVVddhfXr1wMA8vLysGvXLixcuLDznAMHDuD6669HZGQkPD09MXPmTABAYWFhr/Lp9XqcOXMGd999Nzw8PDof//rXv3DmzBkAwJ133omsrCwMGzYMDz30EL777rtevRcR9T+WGyLqd2fOnEFJSQksFgvy8/Mve/7ChQvx0Ucfoa2tDRs2bMCYMWMwZswYANYiMnfuXOh0Oqxfvx779u3Dp59+CsA6L+ZCFApFl8tcgPXSVoempiYAwJo1a5CVldX5OHr0KHbv3g0AGD9+PPLy8vD444+jpaUFt956K26++eYefxZE1P94WYqI+pXRaMTtt9+O+fPnY9iwYbjnnntw5MgRBAYGXvQ58+bNw7333ovNmzdjw4YNWLRoUefXTp48ierqajz55JOIiIgAAOzfv/+SGQICAlBWVgZZljsvbWVlZXV+PSgoCKGhocjNze0yQvRrOp0O8+fPx/z583HzzTfj6quvRk1NDXx9fbvzURDRAGG5IaJ+9de//hX19fV44YUX4OHhga+//hp33XUXvvrqq4s+x93dHb///e/x6KOP4sSJE7jttts6vxYZGQmNRoMXX3wR9913H44ePYrHH3/8khlmzZqFyspKPP3007j55puxefNmfPPNN9DpdJ3n/OMf/8BDDz0ELy8vXH311TAYDNi/fz9qa2uRlpaGlStXIiQkBOPGjYNCocCHH36I4OBgeHt79/kzIiIbk4mI+smWLVtklUol//TTT53H8vLyZJ1OJ7/yyiuXfO7XX38tA5BnzJhx3tc2bNggR0dHy1qtVk5JSZG/+OILGYB88ODBzvcFINfW1nY+59VXX5UjIiJkd3d3edGiRfK///1vOSoqqsvrrl+/Xk5MTJQ1Go3s4+Mjz5gxQ/7kk09kWZbl119/XU5MTJTd3d1lnU4nz549W87MzOzdB0NE/UqS5V9diCYiIiIaxDihmIiIiBwKyw0RERE5FJYbIiIicigsN0RERORQWG6IiIjIobDcEBERkUNhuSEiIiKHwnJDREREDoXlhoiIiBwKyw0RERE5FJYbIiIicij/HyQ0bvfivdu5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset_name = 'diabetes'\n",
    "dataset_name = 'california_house'\n",
    "\n",
    "if dataset_name == 'diabetes':\n",
    "    x, y= datasets.load_diabetes(return_X_y=True)\n",
    "    threshold_rare = 270\n",
    "    EPOCHS = 3500\n",
    "    TRAIN_BATCH = 2048\n",
    "elif dataset_name == 'california_house':\n",
    "    data = datasets.fetch_california_housing()\n",
    "    x = data.data\n",
    "    y = data.target\n",
    "    threshold_rare = 3.5\n",
    "    EPOCHS = 800\n",
    "    TRAIN_BATCH = 4096 \n",
    "\n",
    "np.random.seed(seed=rand_seed)\n",
    "sample = np.random.choice(range(len(y)), 500)\n",
    "x_sample, y_sample = x[sample,:], y[sample]\n",
    "\n",
    "\n",
    "view_distribution(y_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape\n",
      "(20640, 8)\n",
      "Y shape\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape\")\n",
    "print(x.shape)\n",
    "print(\"Y shape\")\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=rand_seed, train_size=0.8)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, random_state=rand_seed, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling by minmax scaler\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "scaler = scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_valid = scaler.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_valid = y_valid.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 1)"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train :  13209\n",
      "num valid :  3303\n",
      "num test  :  4128\n"
     ]
    }
   ],
   "source": [
    "print(\"num train : \", len(y_train))\n",
    "print(\"num valid : \", len(y_valid))\n",
    "print(\"num test  : \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048 \n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "NUM_INPUT = x_train.shape[1]\n",
    "NUM_OUTPUT = 1 \n",
    "NUM_1ST_HIDDEN = 32 \n",
    "NUM_2ND_HIDDEN = 16 \n",
    "NUM_1ST_DROPOUT = 0.6\n",
    "NUM_2ND_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "\n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(torch.FloatTensor(x_train), torch.FloatTensor(y_train))\n",
    "valid_data = TrainData(torch.FloatTensor(x_valid), torch.FloatTensor(y_valid))\n",
    "test_data = TestData(torch.FloatTensor(x_test))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=TRAIN_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=256)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class BasicRegressor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(BasicRegressor, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(NUM_INPUT, NUM_1ST_HIDDEN)\n",
    "        self.layer_2 = nn.Linear(NUM_1ST_HIDDEN, NUM_2ND_HIDDEN)\n",
    "        self.layer_out = nn.Linear(NUM_2ND_HIDDEN, NUM_OUTPUT)\n",
    "\n",
    "        # self.actvation = nn.ReLU()\n",
    "        self.actvation_1 = nn.ReLU()\n",
    "        self.actvation_2 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(p=NUM_1ST_DROPOUT)\n",
    "        self.dropout_2 = nn.Dropout(p=NUM_2ND_DROPOUT)\n",
    "        self.batchnorm_1 = nn.BatchNorm1d(NUM_1ST_HIDDEN)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(NUM_2ND_HIDDEN)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.actvation_1(self.layer_1(inputs))\n",
    "        x = self.batchnorm_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.actvation_2(self.layer_2(x))\n",
    "        x = self.batchnorm_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicRegressor(\n",
      "  (layer_1): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (layer_2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (actvation_1): ReLU()\n",
      "  (actvation_2): ReLU()\n",
      "  (dropout_1): Dropout(p=0.6, inplace=False)\n",
      "  (dropout_2): Dropout(p=0.5, inplace=False)\n",
      "  (batchnorm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BasicRegressor()\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_train_data, num_eval_data):\n",
    "\n",
    "    best_loss_on_valid = 999999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        eval_epoch_loss = 0\n",
    "        eval_epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "        # acc = calc_accuracy(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        # epoch_acc += acc.item()\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for x, y in valid_loader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    output = model(x)\n",
    "\n",
    "                    eval_loss = criterion(output, y)\n",
    "                # eval_acc = calc_accuracy(output, y)\n",
    "\n",
    "                    eval_epoch_loss += eval_loss.item()\n",
    "                # eval_epoch_acc += eval_acc.item()\n",
    "        \n",
    "            if best_loss_on_valid >= (eval_epoch_loss/num_eval_data):\n",
    "                best_loss_on_valid = (eval_epoch_loss/num_eval_data)\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print(\"Best Model is copied - Best Loss : \", best_loss_on_valid)\n",
    "        \n",
    "\n",
    "\n",
    "            print(f\"Epoch {epoch+0:03}: : Loss: T_{epoch_loss/num_train_data:.3f} V_{eval_epoch_loss/num_eval_data:.3f} | Acc: T_{epoch_acc/num_train_data:.3f}) V_{eval_epoch_acc/num_eval_data:.3f}\")\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y_pred, y_test):\n",
    "    mse_criterion = nn.L1Loss() \n",
    "    mse = mse_criterion(y_pred, y_test)\n",
    "\n",
    "    return mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_l1(y_pred, y_test):\n",
    "    return np.abs(y_pred - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model is copied - Best Loss :  4.899481654167175\n",
      "Epoch 010: : Loss: T_5.456 V_4.899 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  4.22360759973526\n",
      "Epoch 020: : Loss: T_4.780 V_4.224 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.7270203828811646\n",
      "Epoch 030: : Loss: T_4.312 V_3.727 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.0021201968193054\n",
      "Epoch 040: : Loss: T_3.591 V_3.002 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.2070263028144836\n",
      "Epoch 050: : Loss: T_3.045 V_2.207 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.5191646218299866\n",
      "Epoch 060: : Loss: T_2.396 V_1.519 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.0574408769607544\n",
      "Epoch 070: : Loss: T_1.909 V_1.057 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.7551149129867554\n",
      "Epoch 080: : Loss: T_1.587 V_0.755 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6595936864614487\n",
      "Epoch 090: : Loss: T_1.377 V_0.660 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.604136660695076\n",
      "Epoch 100: : Loss: T_1.267 V_0.604 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5903613865375519\n",
      "Epoch 110: : Loss: T_1.191 V_0.590 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_1.093 V_0.597 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5677912160754204\n",
      "Epoch 130: : Loss: T_1.037 V_0.568 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5509510561823845\n",
      "Epoch 140: : Loss: T_0.997 V_0.551 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5447462573647499\n",
      "Epoch 150: : Loss: T_0.990 V_0.545 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.539119727909565\n",
      "Epoch 160: : Loss: T_0.920 V_0.539 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5308054611086845\n",
      "Epoch 170: : Loss: T_0.929 V_0.531 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5277068167924881\n",
      "Epoch 180: : Loss: T_0.896 V_0.528 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.857 V_0.538 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.867 V_0.528 | Acc: T_0.000) V_0.000\n",
      "Epoch 210: : Loss: T_0.865 V_0.534 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.526991493999958\n",
      "Epoch 220: : Loss: T_0.835 V_0.527 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5267211645841599\n",
      "Epoch 230: : Loss: T_0.822 V_0.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.823 V_0.529 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5244469493627548\n",
      "Epoch 250: : Loss: T_0.851 V_0.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_0.783 V_0.528 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_0.791 V_0.532 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5114863365888596\n",
      "Epoch 280: : Loss: T_0.808 V_0.511 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5076943188905716\n",
      "Epoch 290: : Loss: T_0.794 V_0.508 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5067595019936562\n",
      "Epoch 300: : Loss: T_0.767 V_0.507 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_0.782 V_0.511 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4967290312051773\n",
      "Epoch 320: : Loss: T_0.768 V_0.497 | Acc: T_0.000) V_0.000\n",
      "Epoch 330: : Loss: T_0.770 V_0.511 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4897560328245163\n",
      "Epoch 340: : Loss: T_0.768 V_0.490 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.751 V_0.491 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_0.747 V_0.492 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4895095154643059\n",
      "Epoch 370: : Loss: T_0.735 V_0.490 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4837814122438431\n",
      "Epoch 380: : Loss: T_0.753 V_0.484 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4810543805360794\n",
      "Epoch 390: : Loss: T_0.729 V_0.481 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.726 V_0.487 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.47495032101869583\n",
      "Epoch 410: : Loss: T_0.751 V_0.475 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.720 V_0.492 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.761 V_0.477 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.713 V_0.480 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4731757491827011\n",
      "Epoch 450: : Loss: T_0.689 V_0.473 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4706759750843048\n",
      "Epoch 460: : Loss: T_0.713 V_0.471 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4619789943099022\n",
      "Epoch 470: : Loss: T_0.685 V_0.462 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4538547769188881\n",
      "Epoch 480: : Loss: T_0.709 V_0.454 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.45047007501125336\n",
      "Epoch 490: : Loss: T_0.675 V_0.450 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.681 V_0.491 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.686 V_0.467 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_0.673 V_0.466 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.688 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43862272799015045\n",
      "Epoch 540: : Loss: T_0.684 V_0.439 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.652 V_0.454 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.660 V_0.447 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42828739434480667\n",
      "Epoch 570: : Loss: T_0.651 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.655 V_0.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.622 V_0.446 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42370619624853134\n",
      "Epoch 600: : Loss: T_0.646 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.633 V_0.456 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.653 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.631 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.622 V_0.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 650: : Loss: T_0.616 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.627 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4213667958974838\n",
      "Epoch 670: : Loss: T_0.623 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.627 V_0.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.615 V_0.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.612 V_0.427 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4210050702095032\n",
      "Epoch 710: : Loss: T_0.603 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4104457050561905\n",
      "Epoch 720: : Loss: T_0.593 V_0.410 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.608 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.606 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.606 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.594 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.583 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4040478840470314\n",
      "Epoch 780: : Loss: T_0.583 V_0.404 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.584 V_0.409 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.582 V_0.425 | Acc: T_0.000) V_0.000\n"
     ]
    }
   ],
   "source": [
    "num_train_data = len(train_loader)\n",
    "num_eval_data = len(valid_loader)\n",
    "\n",
    "\n",
    "elapsed_time_basic_ann = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "best_model = train_model(num_train_data, num_eval_data)\n",
    "\n",
    "\n",
    "elapsed_time_basic_ann.append((datetime.now()-start_time).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time  [129.203242, 0.000999]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "data = torch.from_numpy(x_test).float().to(device)\n",
    "answer = torch.from_numpy(y_test).float().to(device)\n",
    "\n",
    "# data = torch.from_numpy(x_train).float().to(device)\n",
    "# answer = torch.from_numpy(y_train_onehot).float().to(device)\n",
    "\n",
    "# data = torch.from_numpy(x_valid).float().to(device)\n",
    "# answer = torch.from_numpy(y_valid_onehot).float().to(device)\n",
    "\n",
    "start_time = datetime.now()\n",
    "output = best_model(data)\n",
    "loss_basic_ann = calc_loss(output, answer)\n",
    "elapsed_time_basic_ann.append((datetime.now()-start_time).total_seconds())\n",
    "\n",
    "# print('Accuracy ', acc_basic_ann)\n",
    "print('elapsed time ', elapsed_time_basic_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_distribution(answer.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_l1_loss_by_shots(output, answer):\n",
    "    l1_loss = calc_l1(output, answer)\n",
    "    rare_indicies = np.where(y_test>threshold_rare)[0]\n",
    "    # print(rare_indicies)\n",
    "\n",
    "\n",
    "    normal_indicies = np.where(y_test<=threshold_rare)[0]\n",
    "    # print(normal_indicies)\n",
    "\n",
    "\n",
    "    avg_rare_l1 = np.average(l1_loss[rare_indicies])\n",
    "    avg_normal_l1 = np.average(l1_loss[normal_indicies])\n",
    "    avg_total_l1 = np.average(l1_loss)\n",
    "\n",
    "    return avg_rare_l1, avg_normal_l1, avg_total_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Loss  1.0098977\n",
      "Normal Loss  0.3627187\n",
      "Total Loss  0.44549745\n"
     ]
    }
   ],
   "source": [
    "rare_loss, normal_loss, total_loss = calc_l1_loss_by_shots(output.cpu().detach().numpy(), answer.cpu().detach().numpy())\n",
    "print(\"Rare Loss \", rare_loss)\n",
    "print(\"Normal Loss \", normal_loss)\n",
    "print(\"Total Loss \", total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENSEMBLE_MODELS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model is copied - Best Loss :  4.371871471405029\n",
      "Epoch 010: : Loss: T_4.452 V_4.372 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.175911247730255\n",
      "Epoch 020: : Loss: T_3.477 V_3.176 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.8959530889987946\n",
      "Epoch 030: : Loss: T_2.461 V_1.896 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.1207960397005081\n",
      "Epoch 040: : Loss: T_1.652 V_1.121 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.7238341420888901\n",
      "Epoch 050: : Loss: T_1.277 V_0.724 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6407651007175446\n",
      "Epoch 060: : Loss: T_1.114 V_0.641 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.615755170583725\n",
      "Epoch 070: : Loss: T_1.016 V_0.616 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5642416104674339\n",
      "Epoch 080: : Loss: T_0.944 V_0.564 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5421244874596596\n",
      "Epoch 090: : Loss: T_0.892 V_0.542 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5329940542578697\n",
      "Epoch 100: : Loss: T_0.847 V_0.533 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.521917849779129\n",
      "Epoch 110: : Loss: T_0.823 V_0.522 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.822 V_0.542 | Acc: T_0.000) V_0.000\n",
      "Epoch 130: : Loss: T_0.802 V_0.533 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.757 V_0.523 | Acc: T_0.000) V_0.000\n",
      "Epoch 150: : Loss: T_0.782 V_0.523 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.776 V_0.525 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5181431844830513\n",
      "Epoch 170: : Loss: T_0.748 V_0.518 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5111682638525963\n",
      "Epoch 180: : Loss: T_0.744 V_0.511 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.715 V_0.512 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.727 V_0.522 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4993011951446533\n",
      "Epoch 210: : Loss: T_0.714 V_0.499 | Acc: T_0.000) V_0.000\n",
      "Epoch 220: : Loss: T_0.703 V_0.506 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.48919540643692017\n",
      "Epoch 230: : Loss: T_0.692 V_0.489 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.47632884979248047\n",
      "Epoch 240: : Loss: T_0.703 V_0.476 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_0.681 V_0.493 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4729566425085068\n",
      "Epoch 260: : Loss: T_0.680 V_0.473 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4712521582841873\n",
      "Epoch 270: : Loss: T_0.664 V_0.471 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_0.661 V_0.474 | Acc: T_0.000) V_0.000\n",
      "Epoch 290: : Loss: T_0.634 V_0.479 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4480537921190262\n",
      "Epoch 300: : Loss: T_0.641 V_0.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_0.654 V_0.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.637 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4425896629691124\n",
      "Epoch 330: : Loss: T_0.638 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.618 V_0.482 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.626 V_0.459 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43608666211366653\n",
      "Epoch 360: : Loss: T_0.624 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.609 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 380: : Loss: T_0.618 V_0.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.611 V_0.465 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43284186720848083\n",
      "Epoch 400: : Loss: T_0.596 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.609 V_0.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.592 V_0.434 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.583 V_0.466 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42182572185993195\n",
      "Epoch 440: : Loss: T_0.581 V_0.422 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42120490968227386\n",
      "Epoch 450: : Loss: T_0.583 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.573 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.574 V_0.422 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4206054359674454\n",
      "Epoch 480: : Loss: T_0.568 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4205317422747612\n",
      "Epoch 490: : Loss: T_0.567 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4194207563996315\n",
      "Epoch 500: : Loss: T_0.551 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.557 V_0.429 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41909588128328323\n",
      "Epoch 520: : Loss: T_0.563 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.546 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.542 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.549 V_0.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.554 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.526 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.413253054022789\n",
      "Epoch 580: : Loss: T_0.533 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.522 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.532 V_0.449 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.518 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.517 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.518 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.515 V_0.449 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4109230116009712\n",
      "Epoch 650: : Loss: T_0.515 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.523 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4080452471971512\n",
      "Epoch 670: : Loss: T_0.529 V_0.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.503 V_0.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.512 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.509 V_0.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.503 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.503 V_0.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.504 V_0.409 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.504 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.492 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4040802866220474\n",
      "Epoch 760: : Loss: T_0.495 V_0.404 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.501 V_0.410 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.494 V_0.410 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.491 V_0.410 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.489 V_0.408 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  4.450201869010925\n",
      "Epoch 010: : Loss: T_4.770 V_4.450 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.6368626952171326\n",
      "Epoch 020: : Loss: T_3.800 V_3.637 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.2064387798309326\n",
      "Epoch 030: : Loss: T_2.538 V_2.206 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.183255285024643\n",
      "Epoch 040: : Loss: T_1.638 V_1.183 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.7222329080104828\n",
      "Epoch 050: : Loss: T_1.271 V_0.722 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6134247928857803\n",
      "Epoch 060: : Loss: T_1.063 V_0.613 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5667559802532196\n",
      "Epoch 070: : Loss: T_0.980 V_0.567 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5292542576789856\n",
      "Epoch 080: : Loss: T_0.912 V_0.529 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5114848092198372\n",
      "Epoch 090: : Loss: T_0.872 V_0.511 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5101088061928749\n",
      "Epoch 100: : Loss: T_0.848 V_0.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 110: : Loss: T_0.812 V_0.531 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.793 V_0.525 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5090063065290451\n",
      "Epoch 130: : Loss: T_0.775 V_0.509 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.766 V_0.515 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4997337982058525\n",
      "Epoch 150: : Loss: T_0.761 V_0.500 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4806055426597595\n",
      "Epoch 160: : Loss: T_0.733 V_0.481 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.46860989183187485\n",
      "Epoch 170: : Loss: T_0.732 V_0.469 | Acc: T_0.000) V_0.000\n",
      "Epoch 180: : Loss: T_0.733 V_0.473 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.46654029935598373\n",
      "Epoch 190: : Loss: T_0.719 V_0.467 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4620470702648163\n",
      "Epoch 200: : Loss: T_0.727 V_0.462 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.44118261337280273\n",
      "Epoch 210: : Loss: T_0.714 V_0.441 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4401208236813545\n",
      "Epoch 220: : Loss: T_0.690 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43607763946056366\n",
      "Epoch 230: : Loss: T_0.688 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.690 V_0.459 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_0.678 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_0.671 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_0.666 V_0.494 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_0.651 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42710381001234055\n",
      "Epoch 290: : Loss: T_0.675 V_0.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_0.636 V_0.457 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_0.630 V_0.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.651 V_0.438 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.418583944439888\n",
      "Epoch 330: : Loss: T_0.633 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.639 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.641 V_0.463 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_0.616 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.608 V_0.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 380: : Loss: T_0.610 V_0.450 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.617 V_0.470 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.602 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.592 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.570 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.580 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.583 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.564 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41333092749118805\n",
      "Epoch 460: : Loss: T_0.569 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.572 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.556 V_0.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 490: : Loss: T_0.563 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.533 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.547 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_0.539 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41048819571733475\n",
      "Epoch 530: : Loss: T_0.533 V_0.410 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.537 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.530 V_0.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.542 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.535 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.526 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.511 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.510 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.509 V_0.451 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4066409692168236\n",
      "Epoch 620: : Loss: T_0.502 V_0.407 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.516 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.508 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 650: : Loss: T_0.501 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.504 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.508 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.503 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.498 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.488 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.488 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.485 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.40622173249721527\n",
      "Epoch 730: : Loss: T_0.478 V_0.406 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.493 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.474 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.474 V_0.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.482 V_0.434 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.470 V_0.435 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.479 V_0.462 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.467 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  4.710755944252014\n",
      "Epoch 010: : Loss: T_5.407 V_4.711 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.6699657440185547\n",
      "Epoch 020: : Loss: T_4.331 V_3.670 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.2680190205574036\n",
      "Epoch 030: : Loss: T_2.970 V_2.268 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.1155631691217422\n",
      "Epoch 040: : Loss: T_1.901 V_1.116 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.7437922954559326\n",
      "Epoch 050: : Loss: T_1.404 V_0.744 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6422131210565567\n",
      "Epoch 060: : Loss: T_1.183 V_0.642 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.61977569013834\n",
      "Epoch 070: : Loss: T_1.071 V_0.620 | Acc: T_0.000) V_0.000\n",
      "Epoch 080: : Loss: T_1.007 V_0.625 | Acc: T_0.000) V_0.000\n",
      "Epoch 090: : Loss: T_0.932 V_0.629 | Acc: T_0.000) V_0.000\n",
      "Epoch 100: : Loss: T_0.907 V_0.644 | Acc: T_0.000) V_0.000\n",
      "Epoch 110: : Loss: T_0.878 V_0.625 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.858 V_0.633 | Acc: T_0.000) V_0.000\n",
      "Epoch 130: : Loss: T_0.826 V_0.631 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.825 V_0.641 | Acc: T_0.000) V_0.000\n",
      "Epoch 150: : Loss: T_0.820 V_0.647 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.772 V_0.644 | Acc: T_0.000) V_0.000\n",
      "Epoch 170: : Loss: T_0.778 V_0.651 | Acc: T_0.000) V_0.000\n",
      "Epoch 180: : Loss: T_0.766 V_0.642 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.748 V_0.659 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.744 V_0.657 | Acc: T_0.000) V_0.000\n",
      "Epoch 210: : Loss: T_0.741 V_0.663 | Acc: T_0.000) V_0.000\n",
      "Epoch 220: : Loss: T_0.721 V_0.677 | Acc: T_0.000) V_0.000\n",
      "Epoch 230: : Loss: T_0.733 V_0.692 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.715 V_0.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_0.719 V_0.717 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_0.701 V_0.690 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_0.702 V_0.736 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_0.680 V_0.763 | Acc: T_0.000) V_0.000\n",
      "Epoch 290: : Loss: T_0.675 V_0.791 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_0.669 V_0.745 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_0.670 V_0.715 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.673 V_0.746 | Acc: T_0.000) V_0.000\n",
      "Epoch 330: : Loss: T_0.649 V_0.748 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.651 V_0.687 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.654 V_0.667 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_0.645 V_0.737 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.627 V_0.629 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5496604070067406\n",
      "Epoch 380: : Loss: T_0.626 V_0.550 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.623 V_0.570 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.620 V_0.563 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.614 V_0.652 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.602 V_0.588 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.596 V_0.624 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.588 V_0.631 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.588 V_0.564 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.592 V_0.601 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5410182625055313\n",
      "Epoch 470: : Loss: T_0.583 V_0.541 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.602 V_0.588 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5066519230604172\n",
      "Epoch 490: : Loss: T_0.560 V_0.507 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.566 V_0.656 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.562 V_0.597 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_0.572 V_0.546 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.559 V_0.551 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.557 V_0.608 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.542 V_0.533 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.548 V_0.556 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5036661997437477\n",
      "Epoch 570: : Loss: T_0.540 V_0.504 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.538 V_0.561 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.526 V_0.556 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.521 V_0.589 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.514 V_0.625 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.512 V_0.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.513 V_0.546 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.511 V_0.547 | Acc: T_0.000) V_0.000\n",
      "Epoch 650: : Loss: T_0.501 V_0.621 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.507 V_0.608 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.493 V_0.594 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.487 V_0.656 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.500 V_0.578 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.492 V_0.577 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.48729783296585083\n",
      "Epoch 710: : Loss: T_0.505 V_0.487 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.497 V_0.538 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.479 V_0.605 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.488 V_0.521 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.488 V_0.560 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.484 V_0.520 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.46459637582302094\n",
      "Epoch 770: : Loss: T_0.482 V_0.465 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.486 V_0.609 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.480 V_0.634 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.477 V_0.553 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  4.235525965690613\n",
      "Epoch 010: : Loss: T_4.911 V_4.236 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.136642038822174\n",
      "Epoch 020: : Loss: T_3.824 V_3.137 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.748985767364502\n",
      "Epoch 030: : Loss: T_2.598 V_1.749 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.9813312441110611\n",
      "Epoch 040: : Loss: T_1.717 V_0.981 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6721916049718857\n",
      "Epoch 050: : Loss: T_1.327 V_0.672 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.579481452703476\n",
      "Epoch 060: : Loss: T_1.165 V_0.579 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5523019507527351\n",
      "Epoch 070: : Loss: T_1.048 V_0.552 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5368992611765862\n",
      "Epoch 080: : Loss: T_0.971 V_0.537 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5197209566831589\n",
      "Epoch 090: : Loss: T_0.928 V_0.520 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.513252004981041\n",
      "Epoch 100: : Loss: T_0.866 V_0.513 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5094563141465187\n",
      "Epoch 110: : Loss: T_0.855 V_0.509 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.836 V_0.521 | Acc: T_0.000) V_0.000\n",
      "Epoch 130: : Loss: T_0.818 V_0.516 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.803 V_0.520 | Acc: T_0.000) V_0.000\n",
      "Epoch 150: : Loss: T_0.788 V_0.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.774 V_0.512 | Acc: T_0.000) V_0.000\n",
      "Epoch 170: : Loss: T_0.794 V_0.518 | Acc: T_0.000) V_0.000\n",
      "Epoch 180: : Loss: T_0.776 V_0.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.767 V_0.516 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.769 V_0.520 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5028278231620789\n",
      "Epoch 210: : Loss: T_0.736 V_0.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 220: : Loss: T_0.733 V_0.513 | Acc: T_0.000) V_0.000\n",
      "Epoch 230: : Loss: T_0.740 V_0.504 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.49924639612436295\n",
      "Epoch 240: : Loss: T_0.735 V_0.499 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_0.727 V_0.507 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4965005964040756\n",
      "Epoch 260: : Loss: T_0.715 V_0.497 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4944458678364754\n",
      "Epoch 270: : Loss: T_0.700 V_0.494 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4885670691728592\n",
      "Epoch 280: : Loss: T_0.719 V_0.489 | Acc: T_0.000) V_0.000\n",
      "Epoch 290: : Loss: T_0.690 V_0.492 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_0.680 V_0.498 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4803750589489937\n",
      "Epoch 310: : Loss: T_0.652 V_0.480 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.658 V_0.483 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4614695757627487\n",
      "Epoch 330: : Loss: T_0.653 V_0.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.648 V_0.474 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.645 V_0.467 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_0.636 V_0.490 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.613 V_0.486 | Acc: T_0.000) V_0.000\n",
      "Epoch 380: : Loss: T_0.636 V_0.470 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.623 V_0.470 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.45422863215208054\n",
      "Epoch 400: : Loss: T_0.612 V_0.454 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.623 V_0.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.606 V_0.467 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.606 V_0.463 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.604 V_0.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.577 V_0.479 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.598 V_0.457 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.608 V_0.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.580 V_0.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 490: : Loss: T_0.585 V_0.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.567 V_0.488 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.566 V_0.465 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.45086970180273056\n",
      "Epoch 520: : Loss: T_0.555 V_0.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.554 V_0.469 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.556 V_0.455 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.551 V_0.460 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4492161124944687\n",
      "Epoch 560: : Loss: T_0.552 V_0.449 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.546 V_0.458 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4436778053641319\n",
      "Epoch 580: : Loss: T_0.521 V_0.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.539 V_0.462 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.522 V_0.455 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4365103766322136\n",
      "Epoch 610: : Loss: T_0.528 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.528 V_0.441 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.533 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.520 V_0.450 | Acc: T_0.000) V_0.000\n",
      "Epoch 650: : Loss: T_0.519 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.508 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.507 V_0.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.495 V_0.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.503 V_0.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.502 V_0.442 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.499 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.490 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4357975199818611\n",
      "Epoch 730: : Loss: T_0.500 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4324231371283531\n",
      "Epoch 740: : Loss: T_0.506 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.491 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.497 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.493 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.498 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.479 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.490 V_0.454 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.872288703918457\n",
      "Epoch 010: : Loss: T_3.981 V_3.872 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.7477596402168274\n",
      "Epoch 020: : Loss: T_3.124 V_2.748 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.9104591012001038\n",
      "Epoch 030: : Loss: T_2.241 V_1.910 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.092410609126091\n",
      "Epoch 040: : Loss: T_1.564 V_1.092 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.7597020715475082\n",
      "Epoch 050: : Loss: T_1.260 V_0.760 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.645433709025383\n",
      "Epoch 060: : Loss: T_1.085 V_0.645 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5999770313501358\n",
      "Epoch 070: : Loss: T_0.963 V_0.600 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5755468606948853\n",
      "Epoch 080: : Loss: T_0.918 V_0.576 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5652310699224472\n",
      "Epoch 090: : Loss: T_0.884 V_0.565 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5460311397910118\n",
      "Epoch 100: : Loss: T_0.845 V_0.546 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5426945313811302\n",
      "Epoch 110: : Loss: T_0.807 V_0.543 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5315055698156357\n",
      "Epoch 120: : Loss: T_0.781 V_0.532 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5104833468794823\n",
      "Epoch 130: : Loss: T_0.782 V_0.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.771 V_0.515 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.49752502888441086\n",
      "Epoch 150: : Loss: T_0.763 V_0.498 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.714 V_0.502 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4924938604235649\n",
      "Epoch 170: : Loss: T_0.730 V_0.492 | Acc: T_0.000) V_0.000\n",
      "Epoch 180: : Loss: T_0.711 V_0.501 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.48450425267219543\n",
      "Epoch 190: : Loss: T_0.701 V_0.485 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.704 V_0.493 | Acc: T_0.000) V_0.000\n",
      "Epoch 210: : Loss: T_0.690 V_0.490 | Acc: T_0.000) V_0.000\n",
      "Epoch 220: : Loss: T_0.681 V_0.491 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.47557757794857025\n",
      "Epoch 230: : Loss: T_0.684 V_0.476 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.673 V_0.487 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.46612805128097534\n",
      "Epoch 250: : Loss: T_0.672 V_0.466 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.46369481086730957\n",
      "Epoch 260: : Loss: T_0.660 V_0.464 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4546585828065872\n",
      "Epoch 270: : Loss: T_0.649 V_0.455 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_0.648 V_0.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 290: : Loss: T_0.650 V_0.457 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.45278189331293106\n",
      "Epoch 300: : Loss: T_0.637 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.44820453971624374\n",
      "Epoch 310: : Loss: T_0.627 V_0.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.624 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.44431664794683456\n",
      "Epoch 330: : Loss: T_0.609 V_0.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.623 V_0.447 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4374242573976517\n",
      "Epoch 350: : Loss: T_0.608 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_0.596 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.596 V_0.472 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4305959939956665\n",
      "Epoch 380: : Loss: T_0.597 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41955988109111786\n",
      "Epoch 390: : Loss: T_0.584 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41392751038074493\n",
      "Epoch 400: : Loss: T_0.579 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.575 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.571 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.558 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.567 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.556 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.554 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.552 V_0.435 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.543 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 490: : Loss: T_0.550 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.544 V_0.422 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.545 V_0.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_0.549 V_0.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.522 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.530 V_0.441 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.528 V_0.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.521 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.517 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.524 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.516 V_0.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.512 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.505 V_0.423 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.410726860165596\n",
      "Epoch 620: : Loss: T_0.505 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.504 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.504 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 650: : Loss: T_0.498 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.504 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.493 V_0.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.498 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.490 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.488 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.505 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.490 V_0.444 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.40645574033260345\n",
      "Epoch 730: : Loss: T_0.494 V_0.406 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.491 V_0.418 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.481 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.496 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.494 V_0.418 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.483 V_0.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.488 V_0.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.480 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  4.383524179458618\n",
      "Epoch 010: : Loss: T_3.981 V_4.384 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.1208488941192627\n",
      "Epoch 020: : Loss: T_3.127 V_3.121 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.307014286518097\n",
      "Epoch 030: : Loss: T_2.327 V_2.307 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.5126827955245972\n",
      "Epoch 040: : Loss: T_1.687 V_1.513 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.9783041030168533\n",
      "Epoch 050: : Loss: T_1.323 V_0.978 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.7417100071907043\n",
      "Epoch 060: : Loss: T_1.145 V_0.742 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6568402051925659\n",
      "Epoch 070: : Loss: T_1.023 V_0.657 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5918750017881393\n",
      "Epoch 080: : Loss: T_0.954 V_0.592 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.583382785320282\n",
      "Epoch 090: : Loss: T_0.889 V_0.583 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5473105236887932\n",
      "Epoch 100: : Loss: T_0.869 V_0.547 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5398420169949532\n",
      "Epoch 110: : Loss: T_0.818 V_0.540 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.789 V_0.542 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5291688665747643\n",
      "Epoch 130: : Loss: T_0.782 V_0.529 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.761 V_0.536 | Acc: T_0.000) V_0.000\n",
      "Epoch 150: : Loss: T_0.726 V_0.545 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5240682885050774\n",
      "Epoch 160: : Loss: T_0.738 V_0.524 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.521347314119339\n",
      "Epoch 170: : Loss: T_0.726 V_0.521 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.48989387601614\n",
      "Epoch 180: : Loss: T_0.730 V_0.490 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.685 V_0.493 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.48336589336395264\n",
      "Epoch 200: : Loss: T_0.702 V_0.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 210: : Loss: T_0.668 V_0.486 | Acc: T_0.000) V_0.000\n",
      "Epoch 220: : Loss: T_0.697 V_0.489 | Acc: T_0.000) V_0.000\n",
      "Epoch 230: : Loss: T_0.679 V_0.484 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.658 V_0.500 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_0.659 V_0.485 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_0.658 V_0.498 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_0.671 V_0.503 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4734055772423744\n",
      "Epoch 280: : Loss: T_0.636 V_0.473 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.45564716309309006\n",
      "Epoch 290: : Loss: T_0.629 V_0.456 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_0.617 V_0.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_0.630 V_0.509 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.617 V_0.472 | Acc: T_0.000) V_0.000\n",
      "Epoch 330: : Loss: T_0.630 V_0.457 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4514365866780281\n",
      "Epoch 340: : Loss: T_0.593 V_0.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.610 V_0.470 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.44882792979478836\n",
      "Epoch 360: : Loss: T_0.602 V_0.449 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.590 V_0.474 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4360622391104698\n",
      "Epoch 380: : Loss: T_0.595 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4344212934374809\n",
      "Epoch 390: : Loss: T_0.577 V_0.434 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.583 V_0.441 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.595 V_0.457 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.580 V_0.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.557 V_0.435 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.575 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.574 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42970556765794754\n",
      "Epoch 460: : Loss: T_0.562 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.547 V_0.448 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42704053223133087\n",
      "Epoch 480: : Loss: T_0.555 V_0.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 490: : Loss: T_0.547 V_0.469 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.537 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.548 V_0.446 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_0.548 V_0.450 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42703671753406525\n",
      "Epoch 530: : Loss: T_0.548 V_0.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.523 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.535 V_0.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.536 V_0.468 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.530 V_0.449 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42264698445796967\n",
      "Epoch 580: : Loss: T_0.532 V_0.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.532 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4195272773504257\n",
      "Epoch 600: : Loss: T_0.518 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.521 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.525 V_0.469 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.511 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.503 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 650: : Loss: T_0.514 V_0.422 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4139033779501915\n",
      "Epoch 660: : Loss: T_0.509 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.509 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.500 V_0.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.494 V_0.435 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.499 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.500 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.40810926258563995\n",
      "Epoch 720: : Loss: T_0.496 V_0.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.494 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.490 V_0.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.484 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.497 V_0.410 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.490 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.496 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.494 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.486 V_0.412 | Acc: T_0.000) V_0.000\n"
     ]
    }
   ],
   "source": [
    "best_models = []\n",
    "for i in range(NUM_ENSEMBLE_MODELS):\n",
    "    model = BasicRegressor()\n",
    "    model.to(device)\n",
    "\n",
    "    # criterion = nn.L1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    bagg_indices = np.random.choice(range(len(x_train)), len(x_train), replace=True)\n",
    "\n",
    "    x_train_bagg = x_train[bagg_indices, :]\n",
    "    y_train_bagg = y_train[bagg_indices, :]\n",
    "    # train_data = TrainData(torch.FloatTensor(x_train), torch.FloatTensor(y_train))\n",
    "    train_data = TrainData(torch.FloatTensor(x_train_bagg), torch.FloatTensor(y_train_bagg))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=2048, shuffle=True)\n",
    "\n",
    "\n",
    "    num_train_data = len(train_loader)\n",
    "    num_eval_data = len(valid_loader)\n",
    "\n",
    "\n",
    "    elapsed_time_basic_ann = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    best_model = train_model(num_train_data, num_eval_data)\n",
    "\n",
    "    best_models.append(best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "sum_output = np.zeros(y_test.shape)\n",
    "\n",
    "for best_model in best_models:\n",
    "    best_model.eval()\n",
    "    output = best_model(data)\n",
    "    sum_output += output.cpu().detach().numpy()\n",
    "\n",
    "avg_output = sum_output / len(best_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Loss  0.9416354726435561\n",
      "Normal Loss  0.37368827726140064\n",
      "Total Loss  0.4463326859730717\n"
     ]
    }
   ],
   "source": [
    "rare_loss, normal_loss, total_loss = calc_l1_loss_by_shots(avg_output, answer.cpu().detach().numpy())\n",
    "print(\"Rare Loss \", rare_loss)\n",
    "print(\"Normal Loss \", normal_loss)\n",
    "print(\"Total Loss \", total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble ANN with REBAGG (REsampling Bagging Method, 2018, P Branco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENSEMBLE_MODELS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_value = threshold_rare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_indicies = np.where(y_train>threshold_rare)[0]\n",
    "normal_indicies = np.where(y_train<=threshold_rare)[0]\n",
    "\n",
    "ov_rare_indicies = np.random.choice(range(len(rare_indicies)), len(normal_indicies), replace=True)\n",
    "\n",
    "x_train_normal_bagg = x_train[normal_indicies, :]\n",
    "y_train_normal_bagg = y_train[normal_indicies, :]\n",
    "\n",
    "\n",
    "x_train_rare_bagg = x_train[ov_rare_indicies, :]\n",
    "y_train_rare_bagg = y_train[ov_rare_indicies, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model is copied - Best Loss :  4.879338502883911\n",
      "Epoch 010: : Loss: T_5.493 V_4.879 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.7002702355384827\n",
      "Epoch 020: : Loss: T_4.346 V_3.700 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.23188853263855\n",
      "Epoch 030: : Loss: T_3.032 V_2.232 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.2030926942825317\n",
      "Epoch 040: : Loss: T_1.978 V_1.203 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.713167205452919\n",
      "Epoch 050: : Loss: T_1.471 V_0.713 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6170088946819305\n",
      "Epoch 060: : Loss: T_1.286 V_0.617 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5838344544172287\n",
      "Epoch 070: : Loss: T_1.155 V_0.584 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5701484680175781\n",
      "Epoch 080: : Loss: T_1.073 V_0.570 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5454920381307602\n",
      "Epoch 090: : Loss: T_1.016 V_0.545 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5395391210913658\n",
      "Epoch 100: : Loss: T_0.929 V_0.540 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5157362967729568\n",
      "Epoch 110: : Loss: T_0.930 V_0.516 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.509844109416008\n",
      "Epoch 120: : Loss: T_0.890 V_0.510 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5055290311574936\n",
      "Epoch 130: : Loss: T_0.863 V_0.506 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.861 V_0.521 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4790750667452812\n",
      "Epoch 150: : Loss: T_0.840 V_0.479 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.804 V_0.485 | Acc: T_0.000) V_0.000\n",
      "Epoch 170: : Loss: T_0.813 V_0.508 | Acc: T_0.000) V_0.000\n",
      "Epoch 180: : Loss: T_0.805 V_0.490 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4782124236226082\n",
      "Epoch 190: : Loss: T_0.795 V_0.478 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.785 V_0.485 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4628538563847542\n",
      "Epoch 210: : Loss: T_0.761 V_0.463 | Acc: T_0.000) V_0.000\n",
      "Epoch 220: : Loss: T_0.763 V_0.474 | Acc: T_0.000) V_0.000\n",
      "Epoch 230: : Loss: T_0.749 V_0.472 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.756 V_0.485 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4579729363322258\n",
      "Epoch 250: : Loss: T_0.763 V_0.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_0.727 V_0.465 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_0.737 V_0.475 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_0.731 V_0.492 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4496409744024277\n",
      "Epoch 290: : Loss: T_0.708 V_0.450 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_0.704 V_0.456 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_0.718 V_0.465 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.712 V_0.471 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43861035257577896\n",
      "Epoch 330: : Loss: T_0.696 V_0.439 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.690 V_0.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.688 V_0.451 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4374013617634773\n",
      "Epoch 360: : Loss: T_0.685 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43624351918697357\n",
      "Epoch 370: : Loss: T_0.682 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43004487454891205\n",
      "Epoch 380: : Loss: T_0.673 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.667 V_0.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.659 V_0.441 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.645 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.635 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.636 V_0.494 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4252792224287987\n",
      "Epoch 440: : Loss: T_0.629 V_0.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.628 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.633 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4164716675877571\n",
      "Epoch 470: : Loss: T_0.614 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.617 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 490: : Loss: T_0.608 V_0.418 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.620 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.598 V_0.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_0.607 V_0.435 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.575 V_0.473 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.585 V_0.456 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.572 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.581 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.570 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.568 V_0.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.579 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.563 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.556 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.561 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.553 V_0.418 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.551 V_0.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 650: : Loss: T_0.543 V_0.423 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41523370146751404\n",
      "Epoch 660: : Loss: T_0.542 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4106048345565796\n",
      "Epoch 670: : Loss: T_0.544 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.544 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.529 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.535 V_0.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.518 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.40837570279836655\n",
      "Epoch 720: : Loss: T_0.521 V_0.408 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4076463282108307\n",
      "Epoch 730: : Loss: T_0.521 V_0.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.512 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.512 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.515 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.519 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.518 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.512 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.503 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  4.261445164680481\n",
      "Epoch 010: : Loss: T_4.557 V_4.261 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.274298310279846\n",
      "Epoch 020: : Loss: T_3.558 V_3.274 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.8045876026153564\n",
      "Epoch 030: : Loss: T_2.364 V_1.805 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.8538179695606232\n",
      "Epoch 040: : Loss: T_1.590 V_0.854 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6110923737287521\n",
      "Epoch 050: : Loss: T_1.221 V_0.611 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5744126588106155\n",
      "Epoch 060: : Loss: T_1.055 V_0.574 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.526029959321022\n",
      "Epoch 070: : Loss: T_0.951 V_0.526 | Acc: T_0.000) V_0.000\n",
      "Epoch 080: : Loss: T_0.893 V_0.527 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5152623057365417\n",
      "Epoch 090: : Loss: T_0.856 V_0.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 100: : Loss: T_0.833 V_0.520 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5100708454847336\n",
      "Epoch 110: : Loss: T_0.810 V_0.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.804 V_0.525 | Acc: T_0.000) V_0.000\n",
      "Epoch 130: : Loss: T_0.768 V_0.518 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5060308203101158\n",
      "Epoch 140: : Loss: T_0.764 V_0.506 | Acc: T_0.000) V_0.000\n",
      "Epoch 150: : Loss: T_0.742 V_0.513 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5027172192931175\n",
      "Epoch 160: : Loss: T_0.733 V_0.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 170: : Loss: T_0.745 V_0.508 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5012215971946716\n",
      "Epoch 180: : Loss: T_0.740 V_0.501 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.715 V_0.511 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.713 V_0.507 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4874762073159218\n",
      "Epoch 210: : Loss: T_0.703 V_0.487 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.47704149037599564\n",
      "Epoch 220: : Loss: T_0.687 V_0.477 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.464565172791481\n",
      "Epoch 230: : Loss: T_0.687 V_0.465 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.690 V_0.476 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_0.676 V_0.485 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_0.672 V_0.467 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_0.654 V_0.470 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.45831794291734695\n",
      "Epoch 280: : Loss: T_0.662 V_0.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 290: : Loss: T_0.668 V_0.462 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_0.655 V_0.478 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4509653300046921\n",
      "Epoch 310: : Loss: T_0.631 V_0.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.621 V_0.452 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4380136579275131\n",
      "Epoch 330: : Loss: T_0.629 V_0.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.637 V_0.447 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4306214228272438\n",
      "Epoch 350: : Loss: T_0.635 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_0.615 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.611 V_0.468 | Acc: T_0.000) V_0.000\n",
      "Epoch 380: : Loss: T_0.607 V_0.434 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.604 V_0.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.601 V_0.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.583 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4294447749853134\n",
      "Epoch 420: : Loss: T_0.588 V_0.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.586 V_0.438 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4211307242512703\n",
      "Epoch 440: : Loss: T_0.572 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4187692776322365\n",
      "Epoch 450: : Loss: T_0.565 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.558 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.572 V_0.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.554 V_0.439 | Acc: T_0.000) V_0.000\n",
      "Epoch 490: : Loss: T_0.559 V_0.454 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.555 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41812293231487274\n",
      "Epoch 510: : Loss: T_0.541 V_0.418 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4173874109983444\n",
      "Epoch 520: : Loss: T_0.549 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.548 V_0.441 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.547 V_0.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.540 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.524 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.539 V_0.418 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.534 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.531 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41649653762578964\n",
      "Epoch 600: : Loss: T_0.538 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.527 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.513 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.521 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.514 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41336579620838165\n",
      "Epoch 650: : Loss: T_0.502 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.499 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.502 V_0.446 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.496 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.499 V_0.457 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.496 V_0.442 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.503 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.506 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.501 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.487 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.502 V_0.418 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.494 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.483 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.492 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.488 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.40927834808826447\n",
      "Epoch 800: : Loss: T_0.483 V_0.409 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  5.417015075683594\n",
      "Epoch 010: : Loss: T_5.605 V_5.417 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  4.180543661117554\n",
      "Epoch 020: : Loss: T_4.482 V_4.181 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.5016369819641113\n",
      "Epoch 030: : Loss: T_3.056 V_2.502 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.39679816365242\n",
      "Epoch 040: : Loss: T_2.024 V_1.397 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.8794944882392883\n",
      "Epoch 050: : Loss: T_1.498 V_0.879 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6766437590122223\n",
      "Epoch 060: : Loss: T_1.270 V_0.677 | Acc: T_0.000) V_0.000\n",
      "Epoch 070: : Loss: T_1.138 V_0.696 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5783029049634933\n",
      "Epoch 080: : Loss: T_1.043 V_0.578 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5115982741117477\n",
      "Epoch 090: : Loss: T_0.995 V_0.512 | Acc: T_0.000) V_0.000\n",
      "Epoch 100: : Loss: T_0.933 V_0.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 110: : Loss: T_0.901 V_0.540 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.861 V_0.524 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.504063569009304\n",
      "Epoch 130: : Loss: T_0.861 V_0.504 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.846 V_0.505 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.49706101417541504\n",
      "Epoch 150: : Loss: T_0.836 V_0.497 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.830 V_0.512 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.48704974353313446\n",
      "Epoch 170: : Loss: T_0.821 V_0.487 | Acc: T_0.000) V_0.000\n",
      "Epoch 180: : Loss: T_0.795 V_0.489 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.790 V_0.496 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.784 V_0.488 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4794260337948799\n",
      "Epoch 210: : Loss: T_0.779 V_0.479 | Acc: T_0.000) V_0.000\n",
      "Epoch 220: : Loss: T_0.759 V_0.505 | Acc: T_0.000) V_0.000\n",
      "Epoch 230: : Loss: T_0.761 V_0.492 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.744 V_0.486 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_0.756 V_0.480 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_0.746 V_0.488 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4634777829051018\n",
      "Epoch 270: : Loss: T_0.736 V_0.463 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4532346576452255\n",
      "Epoch 280: : Loss: T_0.733 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4494912475347519\n",
      "Epoch 290: : Loss: T_0.714 V_0.449 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_0.714 V_0.480 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4444641098380089\n",
      "Epoch 310: : Loss: T_0.691 V_0.444 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43784408271312714\n",
      "Epoch 320: : Loss: T_0.696 V_0.438 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43371812999248505\n",
      "Epoch 330: : Loss: T_0.676 V_0.434 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.687 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42456628382205963\n",
      "Epoch 350: : Loss: T_0.652 V_0.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_0.649 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.661 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4195539206266403\n",
      "Epoch 380: : Loss: T_0.650 V_0.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.653 V_0.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.652 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41532423347234726\n",
      "Epoch 410: : Loss: T_0.640 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41377759724855423\n",
      "Epoch 420: : Loss: T_0.639 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.629 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.623 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.615 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.603 V_0.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.593 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.608 V_0.469 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4118873402476311\n",
      "Epoch 490: : Loss: T_0.594 V_0.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.597 V_0.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.580 V_0.417 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41115885972976685\n",
      "Epoch 520: : Loss: T_0.591 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.572 V_0.441 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.578 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.573 V_0.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.569 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.554 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.544 V_0.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.545 V_0.422 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4021570384502411\n",
      "Epoch 600: : Loss: T_0.538 V_0.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.546 V_0.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.528 V_0.405 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.537 V_0.407 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.40202367305755615\n",
      "Epoch 640: : Loss: T_0.539 V_0.402 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.39486438781023026\n",
      "Epoch 650: : Loss: T_0.534 V_0.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.531 V_0.398 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.39233221858739853\n",
      "Epoch 670: : Loss: T_0.515 V_0.392 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.518 V_0.399 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.512 V_0.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.504 V_0.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.511 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.503 V_0.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.501 V_0.392 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.504 V_0.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.500 V_0.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.492 V_0.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.496 V_0.401 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.493 V_0.409 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.492 V_0.394 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.489 V_0.407 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.558172345161438\n",
      "Epoch 010: : Loss: T_4.098 V_3.558 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.7441659569740295\n",
      "Epoch 020: : Loss: T_3.272 V_2.744 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.732482224702835\n",
      "Epoch 030: : Loss: T_2.401 V_1.732 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.9562279284000397\n",
      "Epoch 040: : Loss: T_1.631 V_0.956 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6823787242174149\n",
      "Epoch 050: : Loss: T_1.300 V_0.682 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5857732146978378\n",
      "Epoch 060: : Loss: T_1.133 V_0.586 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5596208572387695\n",
      "Epoch 070: : Loss: T_0.996 V_0.560 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5444096177816391\n",
      "Epoch 080: : Loss: T_0.932 V_0.544 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5257351845502853\n",
      "Epoch 090: : Loss: T_0.904 V_0.526 | Acc: T_0.000) V_0.000\n",
      "Epoch 100: : Loss: T_0.863 V_0.533 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5181419104337692\n",
      "Epoch 110: : Loss: T_0.824 V_0.518 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.787 V_0.521 | Acc: T_0.000) V_0.000\n",
      "Epoch 130: : Loss: T_0.792 V_0.519 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5046209320425987\n",
      "Epoch 140: : Loss: T_0.768 V_0.505 | Acc: T_0.000) V_0.000\n",
      "Epoch 150: : Loss: T_0.771 V_0.514 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.758 V_0.526 | Acc: T_0.000) V_0.000\n",
      "Epoch 170: : Loss: T_0.759 V_0.507 | Acc: T_0.000) V_0.000\n",
      "Epoch 180: : Loss: T_0.744 V_0.509 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4989834353327751\n",
      "Epoch 190: : Loss: T_0.745 V_0.499 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.743 V_0.505 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4936332032084465\n",
      "Epoch 210: : Loss: T_0.702 V_0.494 | Acc: T_0.000) V_0.000\n",
      "Epoch 220: : Loss: T_0.707 V_0.495 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4815217927098274\n",
      "Epoch 230: : Loss: T_0.696 V_0.482 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4723850041627884\n",
      "Epoch 240: : Loss: T_0.685 V_0.472 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4691518321633339\n",
      "Epoch 250: : Loss: T_0.679 V_0.469 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.454390212893486\n",
      "Epoch 260: : Loss: T_0.665 V_0.454 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.44650185108184814\n",
      "Epoch 270: : Loss: T_0.656 V_0.447 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_0.655 V_0.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 290: : Loss: T_0.647 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4371524378657341\n",
      "Epoch 300: : Loss: T_0.653 V_0.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_0.644 V_0.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.625 V_0.448 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.43452001363039017\n",
      "Epoch 330: : Loss: T_0.632 V_0.435 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.621 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.617 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42637406289577484\n",
      "Epoch 360: : Loss: T_0.613 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.602 V_0.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 380: : Loss: T_0.593 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.594 V_0.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.593 V_0.433 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4184378981590271\n",
      "Epoch 410: : Loss: T_0.588 V_0.418 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.570 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.41161295771598816\n",
      "Epoch 430: : Loss: T_0.569 V_0.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.573 V_0.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.563 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.559 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.560 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.554 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 490: : Loss: T_0.546 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.40581927448511124\n",
      "Epoch 500: : Loss: T_0.550 V_0.406 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.538 V_0.407 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4047742262482643\n",
      "Epoch 520: : Loss: T_0.543 V_0.405 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.540 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4042535573244095\n",
      "Epoch 540: : Loss: T_0.535 V_0.404 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.3993929848074913\n",
      "Epoch 550: : Loss: T_0.527 V_0.399 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.525 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.531 V_0.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.523 V_0.402 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.3979944959282875\n",
      "Epoch 590: : Loss: T_0.520 V_0.398 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.520 V_0.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.39509814232587814\n",
      "Epoch 610: : Loss: T_0.521 V_0.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.512 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.502 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.505 V_0.398 | Acc: T_0.000) V_0.000\n",
      "Epoch 650: : Loss: T_0.500 V_0.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.506 V_0.399 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.523 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.39254339039325714\n",
      "Epoch 680: : Loss: T_0.487 V_0.393 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.491 V_0.393 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.3923248127102852\n",
      "Epoch 700: : Loss: T_0.486 V_0.392 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.490 V_0.404 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.496 V_0.394 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.486 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.484 V_0.394 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.485 V_0.393 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.486 V_0.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.472 V_0.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.482 V_0.398 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.3904418274760246\n",
      "Epoch 790: : Loss: T_0.487 V_0.390 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.483 V_0.395 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.676303744316101\n",
      "Epoch 010: : Loss: T_4.339 V_3.676 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.7262768745422363\n",
      "Epoch 020: : Loss: T_3.383 V_2.726 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.7769614160060883\n",
      "Epoch 030: : Loss: T_2.415 V_1.777 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.9560810774564743\n",
      "Epoch 040: : Loss: T_1.696 V_0.956 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6913339048624039\n",
      "Epoch 050: : Loss: T_1.308 V_0.691 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6192745193839073\n",
      "Epoch 060: : Loss: T_1.136 V_0.619 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5907423347234726\n",
      "Epoch 070: : Loss: T_1.009 V_0.591 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5729083940386772\n",
      "Epoch 080: : Loss: T_0.953 V_0.573 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.549711674451828\n",
      "Epoch 090: : Loss: T_0.892 V_0.550 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5378704890608788\n",
      "Epoch 100: : Loss: T_0.832 V_0.538 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5343365222215652\n",
      "Epoch 110: : Loss: T_0.808 V_0.534 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5317036285996437\n",
      "Epoch 120: : Loss: T_0.798 V_0.532 | Acc: T_0.000) V_0.000\n",
      "Epoch 130: : Loss: T_0.771 V_0.532 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.788 V_0.550 | Acc: T_0.000) V_0.000\n",
      "Epoch 150: : Loss: T_0.750 V_0.537 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.752 V_0.536 | Acc: T_0.000) V_0.000\n",
      "Epoch 170: : Loss: T_0.751 V_0.532 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5274292528629303\n",
      "Epoch 180: : Loss: T_0.740 V_0.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.741 V_0.533 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.738 V_0.536 | Acc: T_0.000) V_0.000\n",
      "Epoch 210: : Loss: T_0.722 V_0.529 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5166910290718079\n",
      "Epoch 220: : Loss: T_0.717 V_0.517 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5014053657650948\n",
      "Epoch 230: : Loss: T_0.703 V_0.501 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.49302513897418976\n",
      "Epoch 240: : Loss: T_0.688 V_0.493 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.48165932297706604\n",
      "Epoch 250: : Loss: T_0.684 V_0.482 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4641439616680145\n",
      "Epoch 260: : Loss: T_0.669 V_0.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_0.670 V_0.491 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_0.667 V_0.469 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.45295368880033493\n",
      "Epoch 290: : Loss: T_0.657 V_0.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4520484358072281\n",
      "Epoch 300: : Loss: T_0.655 V_0.452 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4482148587703705\n",
      "Epoch 310: : Loss: T_0.644 V_0.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.643 V_0.449 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.44245239347219467\n",
      "Epoch 330: : Loss: T_0.639 V_0.442 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.638 V_0.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.622 V_0.460 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.42136555165052414\n",
      "Epoch 360: : Loss: T_0.626 V_0.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.590 V_0.473 | Acc: T_0.000) V_0.000\n",
      "Epoch 380: : Loss: T_0.601 V_0.440 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.582 V_0.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.589 V_0.422 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.577 V_0.454 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.584 V_0.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.584 V_0.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.577 V_0.431 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4078972190618515\n",
      "Epoch 450: : Loss: T_0.560 V_0.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.570 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.568 V_0.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.566 V_0.424 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4005740061402321\n",
      "Epoch 490: : Loss: T_0.557 V_0.401 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.3956844061613083\n",
      "Epoch 500: : Loss: T_0.552 V_0.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.556 V_0.398 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_0.537 V_0.404 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.550 V_0.411 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.546 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.539 V_0.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.531 V_0.404 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.389557421207428\n",
      "Epoch 570: : Loss: T_0.545 V_0.390 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.523 V_0.391 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.522 V_0.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.528 V_0.391 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.509 V_0.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.532 V_0.399 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.516 V_0.390 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.512 V_0.393 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.38895589113235474\n",
      "Epoch 650: : Loss: T_0.505 V_0.389 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.498 V_0.403 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.495 V_0.401 | Acc: T_0.000) V_0.000\n",
      "Epoch 680: : Loss: T_0.494 V_0.394 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.500 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 700: : Loss: T_0.510 V_0.407 | Acc: T_0.000) V_0.000\n",
      "Epoch 710: : Loss: T_0.494 V_0.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.497 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.498 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.490 V_0.407 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.497 V_0.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.489 V_0.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.486 V_0.397 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.486 V_0.397 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.485 V_0.391 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.485 V_0.392 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  3.8107348680496216\n",
      "Epoch 010: : Loss: T_4.107 V_3.811 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  2.765688478946686\n",
      "Epoch 020: : Loss: T_3.221 V_2.766 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  1.496616691350937\n",
      "Epoch 030: : Loss: T_2.223 V_1.497 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.8863517642021179\n",
      "Epoch 040: : Loss: T_1.471 V_0.886 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.6130692809820175\n",
      "Epoch 050: : Loss: T_1.192 V_0.613 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5642860531806946\n",
      "Epoch 060: : Loss: T_1.012 V_0.564 | Acc: T_0.000) V_0.000\n",
      "Epoch 070: : Loss: T_0.932 V_0.566 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5514758378267288\n",
      "Epoch 080: : Loss: T_0.879 V_0.551 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5491738766431808\n",
      "Epoch 090: : Loss: T_0.850 V_0.549 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.517061248421669\n",
      "Epoch 100: : Loss: T_0.818 V_0.517 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5062397494912148\n",
      "Epoch 110: : Loss: T_0.790 V_0.506 | Acc: T_0.000) V_0.000\n",
      "Epoch 120: : Loss: T_0.758 V_0.506 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5046483278274536\n",
      "Epoch 130: : Loss: T_0.762 V_0.505 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_0.741 V_0.528 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.5017890706658363\n",
      "Epoch 150: : Loss: T_0.738 V_0.502 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_0.737 V_0.522 | Acc: T_0.000) V_0.000\n",
      "Epoch 170: : Loss: T_0.726 V_0.502 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.49865853041410446\n",
      "Epoch 180: : Loss: T_0.694 V_0.499 | Acc: T_0.000) V_0.000\n",
      "Epoch 190: : Loss: T_0.698 V_0.506 | Acc: T_0.000) V_0.000\n",
      "Epoch 200: : Loss: T_0.681 V_0.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 210: : Loss: T_0.677 V_0.523 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.49744653701782227\n",
      "Epoch 220: : Loss: T_0.682 V_0.497 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.488250732421875\n",
      "Epoch 230: : Loss: T_0.663 V_0.488 | Acc: T_0.000) V_0.000\n",
      "Epoch 240: : Loss: T_0.669 V_0.537 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_0.655 V_0.528 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_0.648 V_0.528 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_0.651 V_0.514 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_0.643 V_0.528 | Acc: T_0.000) V_0.000\n",
      "Epoch 290: : Loss: T_0.627 V_0.521 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_0.638 V_0.539 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_0.615 V_0.537 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_0.633 V_0.560 | Acc: T_0.000) V_0.000\n",
      "Epoch 330: : Loss: T_0.626 V_0.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 340: : Loss: T_0.604 V_0.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_0.607 V_0.537 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_0.608 V_0.540 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_0.598 V_0.557 | Acc: T_0.000) V_0.000\n",
      "Epoch 380: : Loss: T_0.605 V_0.541 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_0.597 V_0.543 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_0.590 V_0.541 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_0.598 V_0.554 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_0.604 V_0.532 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_0.584 V_0.569 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_0.573 V_0.540 | Acc: T_0.000) V_0.000\n",
      "Epoch 450: : Loss: T_0.568 V_0.514 | Acc: T_0.000) V_0.000\n",
      "Epoch 460: : Loss: T_0.556 V_0.564 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_0.560 V_0.557 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_0.545 V_0.535 | Acc: T_0.000) V_0.000\n",
      "Epoch 490: : Loss: T_0.555 V_0.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 500: : Loss: T_0.547 V_0.509 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_0.541 V_0.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_0.528 V_0.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_0.539 V_0.496 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_0.531 V_0.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 550: : Loss: T_0.531 V_0.498 | Acc: T_0.000) V_0.000\n",
      "Epoch 560: : Loss: T_0.532 V_0.504 | Acc: T_0.000) V_0.000\n",
      "Epoch 570: : Loss: T_0.525 V_0.506 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_0.517 V_0.501 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_0.525 V_0.518 | Acc: T_0.000) V_0.000\n",
      "Epoch 600: : Loss: T_0.517 V_0.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 610: : Loss: T_0.508 V_0.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_0.508 V_0.499 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_0.500 V_0.516 | Acc: T_0.000) V_0.000\n",
      "Epoch 640: : Loss: T_0.499 V_0.494 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4813605472445488\n",
      "Epoch 650: : Loss: T_0.505 V_0.481 | Acc: T_0.000) V_0.000\n",
      "Epoch 660: : Loss: T_0.496 V_0.488 | Acc: T_0.000) V_0.000\n",
      "Epoch 670: : Loss: T_0.502 V_0.500 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4607696905732155\n",
      "Epoch 680: : Loss: T_0.501 V_0.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 690: : Loss: T_0.480 V_0.511 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.45906588435173035\n",
      "Epoch 700: : Loss: T_0.496 V_0.459 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  0.4377969950437546\n",
      "Epoch 710: : Loss: T_0.494 V_0.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 720: : Loss: T_0.486 V_0.462 | Acc: T_0.000) V_0.000\n",
      "Epoch 730: : Loss: T_0.483 V_0.475 | Acc: T_0.000) V_0.000\n",
      "Epoch 740: : Loss: T_0.486 V_0.470 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_0.481 V_0.441 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_0.477 V_0.476 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_0.492 V_0.491 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_0.482 V_0.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_0.476 V_0.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 800: : Loss: T_0.476 V_0.466 | Acc: T_0.000) V_0.000\n"
     ]
    }
   ],
   "source": [
    "best_models = []\n",
    "for i in range(NUM_ENSEMBLE_MODELS):\n",
    "    model = BasicRegressor()\n",
    "    model.to(device)\n",
    "\n",
    "    # criterion = nn.L1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    bagg_indices = np.random.choice(range(len(x_train)), len(x_train), replace=True)\n",
    "    # x_train_bagg = x_train[bagg_indices, :]\n",
    "    # y_train_bagg = y_train[bagg_indices, :]\n",
    "\n",
    "    rare_indicies = np.where(y_train>threshold_rare)[0]\n",
    "    normal_indicies = np.where(y_train<=threshold_rare)[0]\n",
    "\n",
    "    ov_rare_indicies = np.random.choice(range(len(rare_indicies)), len(normal_indicies), replace=True)\n",
    "\n",
    "    x_train_normal_bagg = x_train[normal_indicies, :]\n",
    "    y_train_normal_bagg = y_train[normal_indicies, :]\n",
    "\n",
    "\n",
    "    x_train_rare_bagg = x_train[rare_indicies, :]\n",
    "    y_train_rare_bagg = y_train[rare_indicies, :]\n",
    "\n",
    "\n",
    "    x_train_total_bagg = np.append(x_train_normal_bagg, x_train_rare_bagg, axis=0)\n",
    "    y_train_total_bagg = np.append(y_train_normal_bagg, y_train_rare_bagg, axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # train_data = TrainData(torch.FloatTensor(x_train), torch.FloatTensor(y_train))\n",
    "    train_data = TrainData(torch.FloatTensor(x_train_total_bagg), torch.FloatTensor(y_train_total_bagg))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=2048, shuffle=True)\n",
    "\n",
    "\n",
    "    num_train_data = len(train_loader)\n",
    "    num_eval_data = len(valid_loader)\n",
    "\n",
    "\n",
    "    elapsed_time_basic_ann = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    best_model = train_model(num_train_data, num_eval_data)\n",
    "\n",
    "    best_models.append(best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "sum_output = np.zeros(y_test.shape)\n",
    "\n",
    "for best_model in best_models:\n",
    "    best_model.eval()\n",
    "    output = best_model(data)\n",
    "    sum_output += output.cpu().detach().numpy()\n",
    "\n",
    "avg_output = sum_output / len(best_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(best_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Loss  0.9541582705246077\n",
      "Normal Loss  0.3736453834327835\n",
      "Total Loss  0.44789703178173773\n"
     ]
    }
   ],
   "source": [
    "rare_loss, normal_loss, total_loss = calc_l1_loss_by_shots(avg_output, answer.cpu().detach().numpy())\n",
    "print(\"Rare Loss \", rare_loss)\n",
    "print(\"Normal Loss \", normal_loss)\n",
    "print(\"Total Loss \", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv_python_3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78d04299e464758119aa473303693f33db2a1bc5c94011f00bbd9c1618e77f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
