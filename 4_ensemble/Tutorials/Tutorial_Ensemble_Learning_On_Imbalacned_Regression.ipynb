{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§î Ensemble LearningÏùÄ Imbalanced DataÏóêÎèÑ Ìö®Í≥ºÍ∞Ä ÏûàÏùÑÍπå?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "# import fetch_california_housing as dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import copy\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F  \n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 512\n",
    "\n",
    "NUM_ENSEMBLE_MODELS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Distribution (Target Y) ÌôïÏù∏\n",
    "\n",
    "- DatasetÏùÑ ÌôïÏù∏ÌïòÏó¨ Imbalanced DataÏù∏ÏßÄ ÌôïÏù∏ÌïòÍ∏∞ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_distribution(y):\n",
    "    # View distribution\n",
    "    sns.distplot(y)\n",
    "    plt.xlabel('x values')\n",
    "    plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset_name = 'diabetes'\n",
    "# dataset_name = 'california_house'\n",
    "dataset_name = 'boston_house'\n",
    "\n",
    "if dataset_name == 'diabetes':\n",
    "    x, y= datasets.load_diabetes(return_X_y=True)\n",
    "    threshold_rare = 270\n",
    "    EPOCHS = 3500\n",
    "    TRAIN_BATCH = 2048\n",
    "elif dataset_name == 'california_house':\n",
    "    data = datasets.fetch_california_housing()\n",
    "    x = data.data\n",
    "    y = data.target\n",
    "    threshold_rare = 3.5\n",
    "    EPOCHS = 800\n",
    "    TRAIN_BATCH = 4096 \n",
    "elif dataset_name == 'boston_house':\n",
    "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "    x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "    y = raw_df.values[1::2, 2]\n",
    "\n",
    "    threshold_rare = 35\n",
    "    EPOCHS = 3500\n",
    "    TRAIN_BATCH = 2048\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(seed=rand_seed)\n",
    "sample = np.random.choice(range(len(y)), 500)\n",
    "x_sample, y_sample = x[sample,:], y[sample]\n",
    "\n",
    "\n",
    "view_distribution(y_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape\n",
      "(506, 13)\n",
      "Y shape\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape\")\n",
    "print(x.shape)\n",
    "print(\"Y shape\")\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=rand_seed, train_size=0.8)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, random_state=rand_seed, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling by minmax scaler\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "scaler = scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_valid = scaler.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_valid = y_valid.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train :  323\n",
      "num valid :  81\n",
      "num test  :  102\n"
     ]
    }
   ],
   "source": [
    "print(\"num train : \", len(y_train))\n",
    "print(\"num valid : \", len(y_valid))\n",
    "print(\"num test  : \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048 \n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "NUM_INPUT = x_train.shape[1]\n",
    "NUM_OUTPUT = 1 \n",
    "NUM_1ST_HIDDEN = 32 \n",
    "NUM_2ND_HIDDEN = 16 \n",
    "NUM_1ST_DROPOUT = 0.6\n",
    "NUM_2ND_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "\n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainData(torch.FloatTensor(x_train), torch.FloatTensor(y_train))\n",
    "valid_data = TrainData(torch.FloatTensor(x_valid), torch.FloatTensor(y_valid))\n",
    "test_data = TestData(torch.FloatTensor(x_test))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=TRAIN_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=256)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class BasicRegressor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(BasicRegressor, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(NUM_INPUT, NUM_1ST_HIDDEN)\n",
    "        self.layer_2 = nn.Linear(NUM_1ST_HIDDEN, NUM_2ND_HIDDEN)\n",
    "        self.layer_out = nn.Linear(NUM_2ND_HIDDEN, NUM_OUTPUT)\n",
    "\n",
    "        # self.actvation = nn.ReLU()\n",
    "        self.actvation_1 = nn.ReLU()\n",
    "        self.actvation_2 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(p=NUM_1ST_DROPOUT)\n",
    "        self.dropout_2 = nn.Dropout(p=NUM_2ND_DROPOUT)\n",
    "        self.batchnorm_1 = nn.BatchNorm1d(NUM_1ST_HIDDEN)\n",
    "        self.batchnorm_2 = nn.BatchNorm1d(NUM_2ND_HIDDEN)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.actvation_1(self.layer_1(inputs))\n",
    "        x = self.batchnorm_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.actvation_2(self.layer_2(x))\n",
    "        x = self.batchnorm_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicRegressor(\n",
      "  (layer_1): Linear(in_features=13, out_features=32, bias=True)\n",
      "  (layer_2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (actvation_1): ReLU()\n",
      "  (actvation_2): ReLU()\n",
      "  (dropout_1): Dropout(p=0.6, inplace=False)\n",
      "  (dropout_2): Dropout(p=0.5, inplace=False)\n",
      "  (batchnorm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BasicRegressor()\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_train_data, num_eval_data):\n",
    "\n",
    "    best_loss_on_valid = 999999999\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        eval_epoch_loss = 0\n",
    "        eval_epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "        # acc = calc_accuracy(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        # epoch_acc += acc.item()\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for x, y in valid_loader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    output = model(x)\n",
    "\n",
    "                    eval_loss = criterion(output, y)\n",
    "                # eval_acc = calc_accuracy(output, y)\n",
    "\n",
    "                    eval_epoch_loss += eval_loss.item()\n",
    "                # eval_epoch_acc += eval_acc.item()\n",
    "        \n",
    "            if best_loss_on_valid >= (eval_epoch_loss/num_eval_data):\n",
    "                best_loss_on_valid = (eval_epoch_loss/num_eval_data)\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print(\"Best Model is copied - Best Loss : \", best_loss_on_valid)\n",
    "        \n",
    "\n",
    "\n",
    "            print(f\"Epoch {epoch+0:03}: : Loss: T_{epoch_loss/num_train_data:.3f} V_{eval_epoch_loss/num_eval_data:.3f} | Acc: T_{epoch_acc/num_train_data:.3f}) V_{eval_epoch_acc/num_eval_data:.3f}\")\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y_pred, y_test):\n",
    "    mse_criterion = nn.L1Loss() \n",
    "    mse = mse_criterion(y_pred, y_test)\n",
    "\n",
    "    return mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_l1(y_pred, y_test):\n",
    "    return np.abs(y_pred - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model is copied - Best Loss :  595.6517944335938\n",
      "Epoch 010: : Loss: T_595.416 V_595.652 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  591.7857055664062\n",
      "Epoch 020: : Loss: T_591.108 V_591.786 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  589.2391357421875\n",
      "Epoch 030: : Loss: T_585.663 V_589.239 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  587.8569946289062\n",
      "Epoch 040: : Loss: T_581.048 V_587.857 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  586.8801879882812\n",
      "Epoch 050: : Loss: T_579.342 V_586.880 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  585.6203002929688\n",
      "Epoch 060: : Loss: T_577.749 V_585.620 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  583.6187133789062\n",
      "Epoch 070: : Loss: T_574.509 V_583.619 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  580.8402099609375\n",
      "Epoch 080: : Loss: T_569.942 V_580.840 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  577.9957885742188\n",
      "Epoch 090: : Loss: T_561.696 V_577.996 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  574.1636962890625\n",
      "Epoch 100: : Loss: T_554.475 V_574.164 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  568.7395629882812\n",
      "Epoch 110: : Loss: T_551.971 V_568.740 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  562.9727783203125\n",
      "Epoch 120: : Loss: T_548.917 V_562.973 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  556.3961791992188\n",
      "Epoch 130: : Loss: T_538.232 V_556.396 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  551.119140625\n",
      "Epoch 140: : Loss: T_534.240 V_551.119 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  541.6253662109375\n",
      "Epoch 150: : Loss: T_525.922 V_541.625 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  534.6809692382812\n",
      "Epoch 160: : Loss: T_518.923 V_534.681 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  532.5340576171875\n",
      "Epoch 170: : Loss: T_510.979 V_532.534 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  530.6412353515625\n",
      "Epoch 180: : Loss: T_499.587 V_530.641 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  520.4994506835938\n",
      "Epoch 190: : Loss: T_493.321 V_520.499 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  506.81390380859375\n",
      "Epoch 200: : Loss: T_496.462 V_506.814 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  504.6611633300781\n",
      "Epoch 210: : Loss: T_485.031 V_504.661 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  492.70111083984375\n",
      "Epoch 220: : Loss: T_480.355 V_492.701 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  473.8507080078125\n",
      "Epoch 230: : Loss: T_459.378 V_473.851 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  455.7638854980469\n",
      "Epoch 240: : Loss: T_453.496 V_455.764 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  454.9938659667969\n",
      "Epoch 250: : Loss: T_448.079 V_454.994 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  452.3231201171875\n",
      "Epoch 260: : Loss: T_440.657 V_452.323 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_435.892 V_453.439 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  431.6033935546875\n",
      "Epoch 280: : Loss: T_429.094 V_431.603 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  419.6430969238281\n",
      "Epoch 290: : Loss: T_419.307 V_419.643 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_412.540 V_423.986 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  413.98193359375\n",
      "Epoch 310: : Loss: T_390.505 V_413.982 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  392.7384338378906\n",
      "Epoch 320: : Loss: T_391.642 V_392.738 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  380.4169616699219\n",
      "Epoch 330: : Loss: T_396.129 V_380.417 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  375.7074279785156\n",
      "Epoch 340: : Loss: T_366.199 V_375.707 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  365.1521301269531\n",
      "Epoch 350: : Loss: T_368.783 V_365.152 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  352.3767395019531\n",
      "Epoch 360: : Loss: T_361.511 V_352.377 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  340.9254455566406\n",
      "Epoch 370: : Loss: T_363.116 V_340.925 | Acc: T_0.000) V_0.000\n",
      "Epoch 380: : Loss: T_354.376 V_344.523 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  329.97894287109375\n",
      "Epoch 390: : Loss: T_329.864 V_329.979 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  319.0848388671875\n",
      "Epoch 400: : Loss: T_339.911 V_319.085 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  317.6158447265625\n",
      "Epoch 410: : Loss: T_334.121 V_317.616 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  301.20697021484375\n",
      "Epoch 420: : Loss: T_307.608 V_301.207 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  299.3146667480469\n",
      "Epoch 430: : Loss: T_305.177 V_299.315 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  295.2820129394531\n",
      "Epoch 440: : Loss: T_300.391 V_295.282 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  282.46112060546875\n",
      "Epoch 450: : Loss: T_293.119 V_282.461 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  270.09197998046875\n",
      "Epoch 460: : Loss: T_283.977 V_270.092 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  261.76165771484375\n",
      "Epoch 470: : Loss: T_282.857 V_261.762 | Acc: T_0.000) V_0.000\n",
      "Epoch 480: : Loss: T_266.918 V_263.345 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  257.34124755859375\n",
      "Epoch 490: : Loss: T_258.145 V_257.341 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  244.3081817626953\n",
      "Epoch 500: : Loss: T_264.576 V_244.308 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  226.58091735839844\n",
      "Epoch 510: : Loss: T_267.123 V_226.581 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  217.7538604736328\n",
      "Epoch 520: : Loss: T_237.215 V_217.754 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_234.325 V_218.845 | Acc: T_0.000) V_0.000\n",
      "Epoch 540: : Loss: T_225.027 V_218.033 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  201.29275512695312\n",
      "Epoch 550: : Loss: T_214.108 V_201.293 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  192.40232849121094\n",
      "Epoch 560: : Loss: T_211.196 V_192.402 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  187.30160522460938\n",
      "Epoch 570: : Loss: T_205.745 V_187.302 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  177.20236206054688\n",
      "Epoch 580: : Loss: T_202.685 V_177.202 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  174.49594116210938\n",
      "Epoch 590: : Loss: T_185.272 V_174.496 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  164.8167266845703\n",
      "Epoch 600: : Loss: T_184.823 V_164.817 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  155.88633728027344\n",
      "Epoch 610: : Loss: T_172.767 V_155.886 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  148.17279052734375\n",
      "Epoch 620: : Loss: T_160.595 V_148.173 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  144.6157989501953\n",
      "Epoch 630: : Loss: T_168.580 V_144.616 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  135.0601348876953\n",
      "Epoch 640: : Loss: T_149.228 V_135.060 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  133.27792358398438\n",
      "Epoch 650: : Loss: T_150.994 V_133.278 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  130.51962280273438\n",
      "Epoch 660: : Loss: T_152.579 V_130.520 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  126.41358184814453\n",
      "Epoch 670: : Loss: T_129.158 V_126.414 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  117.26902770996094\n",
      "Epoch 680: : Loss: T_132.889 V_117.269 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  104.65821838378906\n",
      "Epoch 690: : Loss: T_135.192 V_104.658 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  104.27300262451172\n",
      "Epoch 700: : Loss: T_120.556 V_104.273 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  103.7191390991211\n",
      "Epoch 710: : Loss: T_120.735 V_103.719 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  99.81742095947266\n",
      "Epoch 720: : Loss: T_111.551 V_99.817 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  95.42442321777344\n",
      "Epoch 730: : Loss: T_113.770 V_95.424 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  91.68964385986328\n",
      "Epoch 740: : Loss: T_96.964 V_91.690 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  85.95282745361328\n",
      "Epoch 750: : Loss: T_107.301 V_85.953 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  80.88813781738281\n",
      "Epoch 760: : Loss: T_95.468 V_80.888 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  78.26095581054688\n",
      "Epoch 770: : Loss: T_99.038 V_78.261 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  73.8508529663086\n",
      "Epoch 780: : Loss: T_83.271 V_73.851 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  69.53081512451172\n",
      "Epoch 790: : Loss: T_92.911 V_69.531 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  67.71214294433594\n",
      "Epoch 800: : Loss: T_90.326 V_67.712 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  64.97083282470703\n",
      "Epoch 810: : Loss: T_79.982 V_64.971 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  60.644229888916016\n",
      "Epoch 820: : Loss: T_72.640 V_60.644 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  58.17576217651367\n",
      "Epoch 830: : Loss: T_77.068 V_58.176 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  55.69191360473633\n",
      "Epoch 840: : Loss: T_90.647 V_55.692 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  55.04271697998047\n",
      "Epoch 850: : Loss: T_70.330 V_55.043 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  51.07057189941406\n",
      "Epoch 860: : Loss: T_69.886 V_51.071 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.18338394165039\n",
      "Epoch 870: : Loss: T_68.963 V_48.183 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.145748138427734\n",
      "Epoch 880: : Loss: T_71.892 V_48.146 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  47.57320785522461\n",
      "Epoch 890: : Loss: T_65.219 V_47.573 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  45.11328125\n",
      "Epoch 900: : Loss: T_59.823 V_45.113 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  44.073944091796875\n",
      "Epoch 910: : Loss: T_68.549 V_44.074 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  43.06815719604492\n",
      "Epoch 920: : Loss: T_71.557 V_43.068 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  41.37753677368164\n",
      "Epoch 930: : Loss: T_62.979 V_41.378 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  41.29863357543945\n",
      "Epoch 940: : Loss: T_49.909 V_41.299 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.95110321044922\n",
      "Epoch 950: : Loss: T_59.210 V_39.951 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.926692962646484\n",
      "Epoch 960: : Loss: T_62.491 V_39.927 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.99557113647461\n",
      "Epoch 970: : Loss: T_56.780 V_37.996 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.721378326416016\n",
      "Epoch 980: : Loss: T_52.501 V_35.721 | Acc: T_0.000) V_0.000\n",
      "Epoch 990: : Loss: T_62.834 V_36.904 | Acc: T_0.000) V_0.000\n",
      "Epoch 1000: : Loss: T_58.422 V_37.193 | Acc: T_0.000) V_0.000\n",
      "Epoch 1010: : Loss: T_49.898 V_36.830 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.295692443847656\n",
      "Epoch 1020: : Loss: T_60.205 V_34.296 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.10552215576172\n",
      "Epoch 1030: : Loss: T_56.738 V_34.106 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.89072036743164\n",
      "Epoch 1040: : Loss: T_57.607 V_33.891 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.373348236083984\n",
      "Epoch 1050: : Loss: T_53.702 V_33.373 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.465755462646484\n",
      "Epoch 1060: : Loss: T_49.660 V_32.466 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.059078216552734\n",
      "Epoch 1070: : Loss: T_57.814 V_32.059 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.5185546875\n",
      "Epoch 1080: : Loss: T_46.272 V_31.519 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.00576400756836\n",
      "Epoch 1090: : Loss: T_62.882 V_30.006 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.131208419799805\n",
      "Epoch 1100: : Loss: T_60.256 V_29.131 | Acc: T_0.000) V_0.000\n",
      "Epoch 1110: : Loss: T_44.261 V_30.195 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_51.327 V_29.790 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_39.211 V_29.254 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_54.095 V_29.170 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.5119571685791\n",
      "Epoch 1150: : Loss: T_48.663 V_28.512 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_49.304 V_28.581 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.010953903198242\n",
      "Epoch 1170: : Loss: T_39.545 V_28.011 | Acc: T_0.000) V_0.000\n",
      "Epoch 1180: : Loss: T_51.569 V_28.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_49.387 V_29.185 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_58.780 V_28.307 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.96745491027832\n",
      "Epoch 1210: : Loss: T_52.088 V_27.967 | Acc: T_0.000) V_0.000\n",
      "Epoch 1220: : Loss: T_46.198 V_28.101 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_51.545 V_28.568 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.529664993286133\n",
      "Epoch 1240: : Loss: T_47.367 V_27.530 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_53.014 V_27.629 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_51.559 V_27.618 | Acc: T_0.000) V_0.000\n",
      "Epoch 1270: : Loss: T_57.376 V_27.661 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_44.270 V_27.655 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_54.617 V_28.039 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_54.211 V_27.633 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.411977767944336\n",
      "Epoch 1310: : Loss: T_57.564 V_27.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_55.083 V_28.079 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_53.381 V_27.709 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.880817413330078\n",
      "Epoch 1340: : Loss: T_52.552 V_26.881 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_44.330 V_26.910 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_55.328 V_27.767 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_38.962 V_27.237 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_47.431 V_27.709 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_45.898 V_27.389 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.680850982666016\n",
      "Epoch 1400: : Loss: T_42.916 V_26.681 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.299270629882812\n",
      "Epoch 1410: : Loss: T_44.333 V_26.299 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_45.284 V_26.347 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_56.727 V_27.085 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_44.321 V_26.500 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_59.593 V_26.554 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_52.314 V_26.359 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_47.802 V_26.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_44.946 V_26.555 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_60.663 V_27.622 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_44.522 V_26.639 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.13201141357422\n",
      "Epoch 1510: : Loss: T_49.469 V_26.132 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_47.160 V_26.812 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_50.358 V_27.337 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_41.741 V_27.285 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_47.847 V_27.232 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_41.875 V_26.550 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_39.834 V_26.816 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_46.377 V_27.246 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_45.164 V_27.387 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_40.023 V_26.857 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_39.667 V_27.291 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_45.989 V_27.401 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_47.395 V_26.787 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_53.387 V_26.629 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_46.539 V_27.834 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_45.290 V_27.323 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_38.947 V_27.946 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_55.974 V_27.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_50.982 V_27.538 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_42.480 V_28.125 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_50.820 V_27.737 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_40.945 V_27.014 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_46.206 V_26.768 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_58.886 V_26.624 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_43.147 V_28.054 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_45.336 V_27.751 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_44.938 V_27.101 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_43.532 V_26.758 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_56.837 V_26.213 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_55.696 V_26.477 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_54.331 V_27.130 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_44.992 V_27.682 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_47.202 V_26.736 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_49.781 V_27.095 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_48.370 V_27.486 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_49.869 V_27.042 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_46.378 V_26.769 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_48.880 V_27.745 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_54.703 V_27.505 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_38.206 V_27.617 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_46.255 V_27.833 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_56.027 V_27.888 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_51.606 V_27.315 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_51.879 V_27.328 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_42.789 V_28.382 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_50.040 V_28.031 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_44.490 V_27.591 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_40.070 V_28.107 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_43.470 V_28.166 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_49.878 V_27.782 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_42.050 V_29.237 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_50.255 V_28.463 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_36.215 V_27.999 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_46.572 V_27.932 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_52.160 V_28.112 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_49.610 V_27.841 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_59.795 V_27.906 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_42.502 V_28.097 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_49.141 V_27.925 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_42.688 V_27.329 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_51.641 V_27.814 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_42.448 V_27.648 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_43.825 V_27.076 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_49.480 V_28.079 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_53.067 V_28.105 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_42.107 V_28.136 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_39.599 V_28.740 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_43.059 V_28.453 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_38.752 V_27.896 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_46.080 V_28.298 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_40.750 V_28.815 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_40.660 V_28.340 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_42.054 V_27.942 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_53.981 V_28.749 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_48.557 V_29.159 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_47.150 V_29.280 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_48.067 V_28.573 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_49.339 V_28.789 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_47.512 V_28.323 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_45.725 V_28.018 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_48.537 V_27.939 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_55.061 V_28.110 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_42.409 V_28.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_43.199 V_28.242 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_48.295 V_27.825 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_36.948 V_27.871 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_40.835 V_29.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_49.393 V_28.518 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_52.034 V_28.180 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_47.099 V_28.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_39.195 V_28.929 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_43.316 V_28.816 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_42.047 V_29.094 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_49.679 V_28.973 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_37.750 V_28.942 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_47.327 V_28.928 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_44.869 V_28.548 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_44.661 V_29.626 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_38.038 V_30.008 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_49.225 V_29.050 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_43.675 V_28.973 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_46.165 V_29.083 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_42.144 V_29.133 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_37.281 V_29.801 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_47.781 V_29.085 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_44.571 V_29.159 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_51.731 V_29.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_50.069 V_29.554 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_46.669 V_28.769 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_45.504 V_28.721 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_48.671 V_29.062 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_42.370 V_29.509 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_39.669 V_29.203 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_38.425 V_28.735 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_43.898 V_28.898 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_44.467 V_28.814 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_45.091 V_28.683 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_47.137 V_27.932 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_42.778 V_27.283 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_41.849 V_28.666 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_37.095 V_28.667 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_41.214 V_28.545 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_40.579 V_29.520 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_48.951 V_29.122 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_47.430 V_28.822 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_47.306 V_28.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_49.907 V_29.864 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_38.509 V_28.770 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_44.501 V_28.570 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_51.808 V_29.307 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_46.417 V_29.073 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_46.235 V_28.553 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_49.893 V_28.497 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_44.945 V_28.392 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_40.363 V_28.827 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_47.059 V_29.292 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_45.647 V_28.967 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_42.552 V_28.297 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_54.857 V_28.216 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_47.024 V_28.871 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_46.288 V_28.819 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_46.184 V_28.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_39.162 V_27.994 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_43.247 V_27.825 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_47.718 V_28.277 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_44.231 V_27.833 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_53.148 V_28.276 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_48.445 V_28.893 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_44.664 V_29.542 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_49.524 V_28.830 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_43.847 V_28.475 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_44.829 V_28.330 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_48.186 V_28.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_46.318 V_29.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_50.618 V_28.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_38.501 V_28.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_40.116 V_28.835 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_51.464 V_28.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_39.963 V_28.909 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_44.843 V_29.624 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_48.648 V_29.238 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_40.385 V_29.136 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_50.887 V_29.593 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_47.201 V_29.696 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_45.735 V_29.959 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_36.672 V_28.856 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_41.447 V_29.404 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_47.281 V_28.923 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_44.263 V_29.026 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_42.941 V_29.204 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_44.311 V_29.320 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_43.095 V_29.230 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_38.327 V_28.941 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_41.058 V_28.242 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_44.940 V_29.170 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_44.025 V_28.559 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_51.072 V_28.542 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_41.159 V_28.966 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_48.405 V_28.971 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_39.042 V_29.908 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_39.607 V_29.723 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_45.002 V_29.174 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_45.292 V_29.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_43.489 V_28.962 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_46.423 V_28.858 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_44.748 V_28.879 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_42.834 V_29.867 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_49.570 V_30.194 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_43.086 V_30.226 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_47.221 V_30.682 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_37.992 V_29.006 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_42.655 V_29.127 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_38.787 V_29.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_42.551 V_28.992 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_41.331 V_29.264 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_50.406 V_29.493 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_36.493 V_29.053 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_45.496 V_29.146 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_47.138 V_29.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_42.794 V_29.492 | Acc: T_0.000) V_0.000\n"
     ]
    }
   ],
   "source": [
    "num_train_data = len(train_loader)\n",
    "num_eval_data = len(valid_loader)\n",
    "\n",
    "\n",
    "elapsed_time_basic_ann = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "best_model = train_model(num_train_data, num_eval_data)\n",
    "\n",
    "\n",
    "elapsed_time_basic_ann.append((datetime.now()-start_time).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time  [27.547154, 0.004993]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "data = torch.from_numpy(x_test).float().to(device)\n",
    "answer = torch.from_numpy(y_test).float().to(device)\n",
    "\n",
    "# data = torch.from_numpy(x_train).float().to(device)\n",
    "# answer = torch.from_numpy(y_train_onehot).float().to(device)\n",
    "\n",
    "# data = torch.from_numpy(x_valid).float().to(device)\n",
    "# answer = torch.from_numpy(y_valid_onehot).float().to(device)\n",
    "\n",
    "start_time = datetime.now()\n",
    "output = best_model(data)\n",
    "loss_basic_ann = calc_loss(output, answer)\n",
    "elapsed_time_basic_ann.append((datetime.now()-start_time).total_seconds())\n",
    "\n",
    "# print('Accuracy ', acc_basic_ann)\n",
    "print('elapsed time ', elapsed_time_basic_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_distribution(answer.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_l1_loss_by_shots(output, answer):\n",
    "    l1_loss = calc_l1(output, answer)\n",
    "    rare_indicies = np.where(y_test>threshold_rare)[0]\n",
    "    # print(rare_indicies)\n",
    "\n",
    "\n",
    "    normal_indicies = np.where(y_test<=threshold_rare)[0]\n",
    "    # print(normal_indicies)\n",
    "\n",
    "\n",
    "    avg_rare_l1 = np.average(l1_loss[rare_indicies])\n",
    "    avg_normal_l1 = np.average(l1_loss[normal_indicies])\n",
    "    avg_total_l1 = np.average(l1_loss)\n",
    "\n",
    "    return avg_rare_l1, avg_normal_l1, avg_total_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Loss  8.094698\n",
      "Normal Loss  2.0786624\n",
      "Total Loss  2.6094892\n"
     ]
    }
   ],
   "source": [
    "rare_loss, normal_loss, total_loss = calc_l1_loss_by_shots(output.cpu().detach().numpy(), answer.cpu().detach().numpy())\n",
    "print(\"Rare Loss \", rare_loss)\n",
    "print(\"Normal Loss \", normal_loss)\n",
    "print(\"Total Loss \", total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model is copied - Best Loss :  590.6981201171875\n",
      "Epoch 010: : Loss: T_574.288 V_590.698 | Acc: T_0.000) V_0.000\n",
      "Epoch 020: : Loss: T_573.020 V_591.486 | Acc: T_0.000) V_0.000\n",
      "Epoch 030: : Loss: T_569.653 V_590.789 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  588.5795288085938\n",
      "Epoch 040: : Loss: T_565.443 V_588.580 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  584.2538452148438\n",
      "Epoch 050: : Loss: T_560.212 V_584.254 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  580.2279052734375\n",
      "Epoch 060: : Loss: T_556.078 V_580.228 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  573.5448608398438\n",
      "Epoch 070: : Loss: T_554.547 V_573.545 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  565.6581420898438\n",
      "Epoch 080: : Loss: T_545.094 V_565.658 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  561.2141723632812\n",
      "Epoch 090: : Loss: T_540.365 V_561.214 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  559.1353759765625\n",
      "Epoch 100: : Loss: T_539.017 V_559.135 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  554.4747314453125\n",
      "Epoch 110: : Loss: T_533.606 V_554.475 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  545.9810791015625\n",
      "Epoch 120: : Loss: T_523.984 V_545.981 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  538.4580078125\n",
      "Epoch 130: : Loss: T_516.964 V_538.458 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  530.8450927734375\n",
      "Epoch 140: : Loss: T_510.371 V_530.845 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  523.918701171875\n",
      "Epoch 150: : Loss: T_506.182 V_523.919 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  516.209716796875\n",
      "Epoch 160: : Loss: T_501.765 V_516.210 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  509.4402160644531\n",
      "Epoch 170: : Loss: T_497.709 V_509.440 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  505.7669677734375\n",
      "Epoch 180: : Loss: T_477.684 V_505.767 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  503.5321350097656\n",
      "Epoch 190: : Loss: T_473.838 V_503.532 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  495.6782531738281\n",
      "Epoch 200: : Loss: T_474.974 V_495.678 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  484.69732666015625\n",
      "Epoch 210: : Loss: T_459.172 V_484.697 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  475.4844665527344\n",
      "Epoch 220: : Loss: T_470.889 V_475.484 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  469.2674560546875\n",
      "Epoch 230: : Loss: T_456.316 V_469.267 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  454.85357666015625\n",
      "Epoch 240: : Loss: T_440.987 V_454.854 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_433.873 V_455.067 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  449.2298583984375\n",
      "Epoch 260: : Loss: T_427.571 V_449.230 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  443.614501953125\n",
      "Epoch 270: : Loss: T_425.002 V_443.615 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  426.7630310058594\n",
      "Epoch 280: : Loss: T_402.836 V_426.763 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  419.82958984375\n",
      "Epoch 290: : Loss: T_411.227 V_419.830 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_402.206 V_423.003 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  411.28887939453125\n",
      "Epoch 310: : Loss: T_402.729 V_411.289 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  396.9720153808594\n",
      "Epoch 320: : Loss: T_391.125 V_396.972 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  386.5086975097656\n",
      "Epoch 330: : Loss: T_372.473 V_386.509 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  376.4023132324219\n",
      "Epoch 340: : Loss: T_375.521 V_376.402 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  366.201904296875\n",
      "Epoch 350: : Loss: T_357.950 V_366.202 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  358.6783142089844\n",
      "Epoch 360: : Loss: T_336.537 V_358.678 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  356.78302001953125\n",
      "Epoch 370: : Loss: T_344.169 V_356.783 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  349.00238037109375\n",
      "Epoch 380: : Loss: T_339.902 V_349.002 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  334.23089599609375\n",
      "Epoch 390: : Loss: T_324.716 V_334.231 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  322.32720947265625\n",
      "Epoch 400: : Loss: T_316.923 V_322.327 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  321.8510437011719\n",
      "Epoch 410: : Loss: T_308.481 V_321.851 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  308.62103271484375\n",
      "Epoch 420: : Loss: T_318.775 V_308.621 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  298.01226806640625\n",
      "Epoch 430: : Loss: T_294.858 V_298.012 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  291.98870849609375\n",
      "Epoch 440: : Loss: T_290.371 V_291.989 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  284.346435546875\n",
      "Epoch 450: : Loss: T_284.928 V_284.346 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  273.4801940917969\n",
      "Epoch 460: : Loss: T_271.484 V_273.480 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  260.01141357421875\n",
      "Epoch 470: : Loss: T_265.460 V_260.011 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  256.62469482421875\n",
      "Epoch 480: : Loss: T_253.872 V_256.625 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  247.8966064453125\n",
      "Epoch 490: : Loss: T_254.333 V_247.897 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  239.69580078125\n",
      "Epoch 500: : Loss: T_250.671 V_239.696 | Acc: T_0.000) V_0.000\n",
      "Epoch 510: : Loss: T_242.151 V_243.710 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  228.768310546875\n",
      "Epoch 520: : Loss: T_231.129 V_228.768 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  218.54977416992188\n",
      "Epoch 530: : Loss: T_226.912 V_218.550 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  209.84519958496094\n",
      "Epoch 540: : Loss: T_210.380 V_209.845 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  202.66134643554688\n",
      "Epoch 550: : Loss: T_209.858 V_202.661 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  186.97015380859375\n",
      "Epoch 560: : Loss: T_207.956 V_186.970 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  177.81886291503906\n",
      "Epoch 570: : Loss: T_194.718 V_177.819 | Acc: T_0.000) V_0.000\n",
      "Epoch 580: : Loss: T_183.171 V_183.118 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  177.56884765625\n",
      "Epoch 590: : Loss: T_179.519 V_177.569 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  158.49085998535156\n",
      "Epoch 600: : Loss: T_176.834 V_158.491 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  154.5838623046875\n",
      "Epoch 610: : Loss: T_176.243 V_154.584 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_166.342 V_155.379 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  147.4662628173828\n",
      "Epoch 630: : Loss: T_156.563 V_147.466 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  135.9591522216797\n",
      "Epoch 640: : Loss: T_148.692 V_135.959 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  133.58660888671875\n",
      "Epoch 650: : Loss: T_132.911 V_133.587 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  125.67253875732422\n",
      "Epoch 660: : Loss: T_133.375 V_125.673 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  118.22128295898438\n",
      "Epoch 670: : Loss: T_136.647 V_118.221 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  114.7966079711914\n",
      "Epoch 680: : Loss: T_124.684 V_114.797 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  111.39308166503906\n",
      "Epoch 690: : Loss: T_117.358 V_111.393 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  107.45172882080078\n",
      "Epoch 700: : Loss: T_117.013 V_107.452 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  102.52042388916016\n",
      "Epoch 710: : Loss: T_110.689 V_102.520 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  98.29100799560547\n",
      "Epoch 720: : Loss: T_110.440 V_98.291 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  90.17476654052734\n",
      "Epoch 730: : Loss: T_99.418 V_90.175 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  86.50865173339844\n",
      "Epoch 740: : Loss: T_98.319 V_86.509 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  81.06197357177734\n",
      "Epoch 750: : Loss: T_94.478 V_81.062 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  79.04912567138672\n",
      "Epoch 760: : Loss: T_100.287 V_79.049 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  76.48011016845703\n",
      "Epoch 770: : Loss: T_91.506 V_76.480 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  71.59581756591797\n",
      "Epoch 780: : Loss: T_93.921 V_71.596 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  68.0877456665039\n",
      "Epoch 790: : Loss: T_81.730 V_68.088 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  66.88406372070312\n",
      "Epoch 800: : Loss: T_83.815 V_66.884 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  65.19319152832031\n",
      "Epoch 810: : Loss: T_91.507 V_65.193 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  61.68064498901367\n",
      "Epoch 820: : Loss: T_78.128 V_61.681 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  60.110008239746094\n",
      "Epoch 830: : Loss: T_75.517 V_60.110 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  58.356868743896484\n",
      "Epoch 840: : Loss: T_84.214 V_58.357 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  57.525142669677734\n",
      "Epoch 850: : Loss: T_73.594 V_57.525 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  54.9610481262207\n",
      "Epoch 860: : Loss: T_72.991 V_54.961 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  51.81875228881836\n",
      "Epoch 870: : Loss: T_70.284 V_51.819 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  49.00087356567383\n",
      "Epoch 880: : Loss: T_70.821 V_49.001 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  47.209049224853516\n",
      "Epoch 890: : Loss: T_62.361 V_47.209 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  44.79335403442383\n",
      "Epoch 900: : Loss: T_68.394 V_44.793 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  41.036041259765625\n",
      "Epoch 910: : Loss: T_67.898 V_41.036 | Acc: T_0.000) V_0.000\n",
      "Epoch 920: : Loss: T_48.299 V_41.434 | Acc: T_0.000) V_0.000\n",
      "Epoch 930: : Loss: T_62.916 V_42.301 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.75642776489258\n",
      "Epoch 940: : Loss: T_58.767 V_40.756 | Acc: T_0.000) V_0.000\n",
      "Epoch 950: : Loss: T_60.116 V_41.794 | Acc: T_0.000) V_0.000\n",
      "Epoch 960: : Loss: T_61.435 V_40.953 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.55086135864258\n",
      "Epoch 970: : Loss: T_56.594 V_38.551 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.346107482910156\n",
      "Epoch 980: : Loss: T_52.444 V_37.346 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.483829498291016\n",
      "Epoch 990: : Loss: T_62.259 V_36.484 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.92268753051758\n",
      "Epoch 1000: : Loss: T_56.514 V_35.923 | Acc: T_0.000) V_0.000\n",
      "Epoch 1010: : Loss: T_53.881 V_36.352 | Acc: T_0.000) V_0.000\n",
      "Epoch 1020: : Loss: T_50.611 V_36.339 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.127262115478516\n",
      "Epoch 1030: : Loss: T_47.406 V_35.127 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.00880813598633\n",
      "Epoch 1040: : Loss: T_55.079 V_33.009 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.974876403808594\n",
      "Epoch 1050: : Loss: T_52.724 V_32.975 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.32467269897461\n",
      "Epoch 1060: : Loss: T_57.890 V_32.325 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.10386657714844\n",
      "Epoch 1070: : Loss: T_47.807 V_32.104 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.74240493774414\n",
      "Epoch 1080: : Loss: T_46.690 V_31.742 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.38840103149414\n",
      "Epoch 1090: : Loss: T_46.153 V_31.388 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.733617782592773\n",
      "Epoch 1100: : Loss: T_41.371 V_30.734 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.386079788208008\n",
      "Epoch 1110: : Loss: T_51.720 V_30.386 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.103286743164062\n",
      "Epoch 1120: : Loss: T_47.307 V_30.103 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.859821319580078\n",
      "Epoch 1130: : Loss: T_49.937 V_29.860 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_52.500 V_30.008 | Acc: T_0.000) V_0.000\n",
      "Epoch 1150: : Loss: T_44.118 V_30.009 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_50.859 V_30.288 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.60833168029785\n",
      "Epoch 1170: : Loss: T_46.066 V_29.608 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.383113861083984\n",
      "Epoch 1180: : Loss: T_51.912 V_29.383 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.171327590942383\n",
      "Epoch 1190: : Loss: T_50.366 V_29.171 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.036361694335938\n",
      "Epoch 1200: : Loss: T_40.145 V_29.036 | Acc: T_0.000) V_0.000\n",
      "Epoch 1210: : Loss: T_54.460 V_29.159 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.798221588134766\n",
      "Epoch 1220: : Loss: T_48.850 V_28.798 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_50.999 V_29.478 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_54.691 V_30.690 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_38.827 V_30.262 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_50.531 V_28.985 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.275625228881836\n",
      "Epoch 1270: : Loss: T_48.560 V_28.276 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_43.948 V_28.391 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_45.049 V_28.645 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_45.154 V_28.834 | Acc: T_0.000) V_0.000\n",
      "Epoch 1310: : Loss: T_53.800 V_28.798 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_48.103 V_28.844 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_48.609 V_28.474 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_47.875 V_28.767 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_44.867 V_28.583 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_44.412 V_28.950 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_48.272 V_29.120 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_59.145 V_28.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_48.454 V_28.371 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_43.075 V_28.460 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_40.074 V_28.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.833044052124023\n",
      "Epoch 1420: : Loss: T_49.683 V_27.833 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.737979888916016\n",
      "Epoch 1430: : Loss: T_45.388 V_27.738 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_43.567 V_27.896 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.559555053710938\n",
      "Epoch 1450: : Loss: T_48.732 V_27.560 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_46.664 V_27.742 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_45.336 V_28.599 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_46.916 V_28.661 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_45.085 V_27.887 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_41.339 V_28.158 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_43.393 V_28.333 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_42.472 V_28.407 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_48.286 V_28.161 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_41.379 V_28.172 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_45.594 V_28.535 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_47.669 V_29.506 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_56.475 V_28.602 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_38.024 V_27.936 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_46.116 V_27.563 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_43.451 V_28.080 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_46.989 V_28.489 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_40.755 V_29.335 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_50.060 V_29.330 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_45.292 V_28.950 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_41.392 V_28.874 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_47.638 V_28.350 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_43.508 V_28.601 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_45.002 V_28.677 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_47.322 V_28.812 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_49.250 V_28.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_50.610 V_28.663 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_41.907 V_29.125 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_43.658 V_28.366 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_44.578 V_27.773 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_47.344 V_27.605 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_40.886 V_29.172 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_40.885 V_29.233 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_55.548 V_28.872 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_50.179 V_28.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_47.464 V_28.314 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_52.707 V_28.637 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_44.640 V_28.679 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_46.530 V_29.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_47.503 V_29.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_39.540 V_29.150 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_48.575 V_29.033 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_46.730 V_29.081 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_46.321 V_28.892 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_38.389 V_29.136 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_41.100 V_29.732 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_46.358 V_30.013 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_48.729 V_30.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_43.770 V_29.614 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_45.353 V_29.171 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_47.233 V_28.408 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.40696144104004\n",
      "Epoch 1960: : Loss: T_38.876 V_27.407 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_46.646 V_28.086 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_39.457 V_28.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_37.522 V_27.878 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_48.262 V_28.715 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_43.091 V_28.350 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_49.521 V_28.210 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_46.893 V_28.390 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_45.914 V_29.003 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_52.540 V_28.531 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_38.652 V_28.594 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_45.791 V_29.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_39.952 V_29.606 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_48.021 V_29.380 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_52.621 V_29.523 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_47.547 V_29.513 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_42.679 V_29.378 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_52.463 V_28.855 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_43.056 V_28.888 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_40.665 V_29.027 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_45.774 V_29.337 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_45.099 V_29.371 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_47.770 V_29.165 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_41.978 V_29.607 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_45.358 V_30.228 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_41.936 V_30.303 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_39.944 V_30.217 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_38.482 V_30.333 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_42.151 V_30.358 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_46.610 V_30.794 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_49.479 V_30.605 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_44.057 V_30.545 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_40.795 V_30.602 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_41.393 V_29.882 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_42.296 V_29.471 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_50.409 V_29.498 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_41.573 V_29.374 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_44.319 V_29.223 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_41.425 V_29.273 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_43.810 V_29.347 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_48.141 V_29.552 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_58.374 V_29.914 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_44.372 V_30.401 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_46.693 V_30.180 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_41.416 V_30.831 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_48.477 V_30.599 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_33.887 V_30.529 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_35.460 V_30.279 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_36.021 V_30.252 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_43.144 V_30.116 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_36.779 V_30.239 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_41.628 V_30.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_45.263 V_30.546 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_45.422 V_30.675 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_38.628 V_30.248 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_39.393 V_30.274 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_47.344 V_30.397 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_46.159 V_30.249 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_45.716 V_30.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_50.069 V_31.275 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_44.040 V_31.282 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_40.964 V_31.309 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_45.360 V_31.192 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_44.270 V_30.376 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_38.381 V_30.906 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_42.082 V_30.700 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_45.234 V_30.486 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_46.223 V_30.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_49.297 V_30.718 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_45.629 V_30.852 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_42.598 V_31.191 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_47.474 V_31.750 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_40.362 V_31.026 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_39.733 V_30.124 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_42.088 V_30.122 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_45.585 V_30.057 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_41.881 V_30.858 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_43.358 V_30.889 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_47.019 V_31.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_39.900 V_31.634 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_40.636 V_31.268 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_43.796 V_30.532 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_41.682 V_30.792 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_40.857 V_30.693 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_51.593 V_30.819 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_36.657 V_30.208 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_40.182 V_30.320 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_40.758 V_30.741 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_40.884 V_30.640 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_48.355 V_30.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_51.332 V_30.029 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_39.285 V_29.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_47.449 V_29.788 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_41.467 V_29.649 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_43.277 V_29.786 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_36.586 V_29.982 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_41.360 V_30.811 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_48.968 V_30.801 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_40.635 V_30.905 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_38.299 V_30.719 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_40.169 V_30.393 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_41.840 V_30.702 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_48.672 V_30.772 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_49.109 V_31.169 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_37.703 V_31.119 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_32.939 V_31.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_38.910 V_31.024 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_47.643 V_31.502 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_48.368 V_31.526 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_43.539 V_31.264 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_41.600 V_31.913 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_47.367 V_32.630 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_48.583 V_31.856 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_40.628 V_31.143 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_51.681 V_31.775 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_44.100 V_31.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_33.437 V_30.946 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_40.611 V_30.650 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_40.295 V_29.922 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_35.295 V_29.961 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_39.582 V_31.049 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_48.798 V_31.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_37.026 V_31.229 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_41.377 V_31.193 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_35.256 V_30.852 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_42.421 V_29.992 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_40.831 V_30.944 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_40.357 V_31.244 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_45.057 V_30.873 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_44.965 V_30.054 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_41.608 V_29.471 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_42.031 V_30.465 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_43.591 V_31.153 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_41.823 V_31.204 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_51.429 V_31.007 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_35.412 V_31.097 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_38.019 V_31.497 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_41.750 V_30.953 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_45.390 V_30.678 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_38.405 V_30.986 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_39.060 V_31.359 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_40.125 V_31.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_35.993 V_31.674 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_39.945 V_32.092 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_39.653 V_32.036 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_35.556 V_31.714 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_42.134 V_31.126 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_41.190 V_30.976 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_40.286 V_30.768 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_45.495 V_30.753 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_45.826 V_30.615 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_36.661 V_31.615 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_47.549 V_31.473 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_40.538 V_32.044 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_39.734 V_31.780 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  602.556884765625\n",
      "Epoch 010: : Loss: T_594.797 V_602.557 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  596.7599487304688\n",
      "Epoch 020: : Loss: T_593.953 V_596.760 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  594.0513916015625\n",
      "Epoch 030: : Loss: T_581.349 V_594.051 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  592.3767700195312\n",
      "Epoch 040: : Loss: T_582.381 V_592.377 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  589.6123657226562\n",
      "Epoch 050: : Loss: T_577.765 V_589.612 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  587.392822265625\n",
      "Epoch 060: : Loss: T_568.044 V_587.393 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  582.5250854492188\n",
      "Epoch 070: : Loss: T_565.116 V_582.525 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  576.707763671875\n",
      "Epoch 080: : Loss: T_562.945 V_576.708 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  571.87109375\n",
      "Epoch 090: : Loss: T_549.739 V_571.871 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  570.1054077148438\n",
      "Epoch 100: : Loss: T_552.306 V_570.105 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  563.8245849609375\n",
      "Epoch 110: : Loss: T_542.491 V_563.825 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  557.4934692382812\n",
      "Epoch 120: : Loss: T_540.313 V_557.493 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  550.7699584960938\n",
      "Epoch 130: : Loss: T_528.801 V_550.770 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  544.0952758789062\n",
      "Epoch 140: : Loss: T_525.978 V_544.095 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  543.6203002929688\n",
      "Epoch 150: : Loss: T_514.732 V_543.620 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  533.3169555664062\n",
      "Epoch 160: : Loss: T_513.741 V_533.317 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  527.3134765625\n",
      "Epoch 170: : Loss: T_501.272 V_527.313 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  517.075927734375\n",
      "Epoch 180: : Loss: T_501.332 V_517.076 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  506.6465148925781\n",
      "Epoch 190: : Loss: T_486.589 V_506.647 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  497.39678955078125\n",
      "Epoch 200: : Loss: T_477.406 V_497.397 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  493.2558898925781\n",
      "Epoch 210: : Loss: T_476.347 V_493.256 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  488.7298583984375\n",
      "Epoch 220: : Loss: T_461.771 V_488.730 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  482.48187255859375\n",
      "Epoch 230: : Loss: T_465.798 V_482.482 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  479.3349304199219\n",
      "Epoch 240: : Loss: T_457.096 V_479.335 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  463.8387451171875\n",
      "Epoch 250: : Loss: T_447.317 V_463.839 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  455.71490478515625\n",
      "Epoch 260: : Loss: T_441.973 V_455.715 | Acc: T_0.000) V_0.000\n",
      "Epoch 270: : Loss: T_429.930 V_457.829 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  451.15277099609375\n",
      "Epoch 280: : Loss: T_430.556 V_451.153 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  441.7937927246094\n",
      "Epoch 290: : Loss: T_419.297 V_441.794 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  426.5787658691406\n",
      "Epoch 300: : Loss: T_419.083 V_426.579 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  419.12615966796875\n",
      "Epoch 310: : Loss: T_398.850 V_419.126 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  413.3387756347656\n",
      "Epoch 320: : Loss: T_392.091 V_413.339 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  393.4792175292969\n",
      "Epoch 330: : Loss: T_389.103 V_393.479 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  390.2441101074219\n",
      "Epoch 340: : Loss: T_379.091 V_390.244 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  386.37060546875\n",
      "Epoch 350: : Loss: T_372.513 V_386.371 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  385.40423583984375\n",
      "Epoch 360: : Loss: T_372.162 V_385.404 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  373.42596435546875\n",
      "Epoch 370: : Loss: T_353.214 V_373.426 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  360.74700927734375\n",
      "Epoch 380: : Loss: T_349.029 V_360.747 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  347.31390380859375\n",
      "Epoch 390: : Loss: T_348.920 V_347.314 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  339.7357177734375\n",
      "Epoch 400: : Loss: T_336.946 V_339.736 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  326.807373046875\n",
      "Epoch 410: : Loss: T_342.349 V_326.807 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  318.5185546875\n",
      "Epoch 420: : Loss: T_317.136 V_318.519 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  316.40692138671875\n",
      "Epoch 430: : Loss: T_314.834 V_316.407 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  308.5065612792969\n",
      "Epoch 440: : Loss: T_293.691 V_308.507 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  287.29083251953125\n",
      "Epoch 450: : Loss: T_293.268 V_287.291 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  275.5894775390625\n",
      "Epoch 460: : Loss: T_292.810 V_275.589 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_276.262 V_278.699 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  265.8081359863281\n",
      "Epoch 480: : Loss: T_272.963 V_265.808 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  258.9188537597656\n",
      "Epoch 490: : Loss: T_261.283 V_258.919 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  245.5433807373047\n",
      "Epoch 500: : Loss: T_251.487 V_245.543 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  232.04588317871094\n",
      "Epoch 510: : Loss: T_246.225 V_232.046 | Acc: T_0.000) V_0.000\n",
      "Epoch 520: : Loss: T_237.610 V_235.371 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  225.60459899902344\n",
      "Epoch 530: : Loss: T_225.275 V_225.605 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  214.41265869140625\n",
      "Epoch 540: : Loss: T_219.626 V_214.413 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  204.00096130371094\n",
      "Epoch 550: : Loss: T_214.674 V_204.001 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  201.2426300048828\n",
      "Epoch 560: : Loss: T_202.952 V_201.243 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  194.8752899169922\n",
      "Epoch 570: : Loss: T_209.094 V_194.875 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  176.09068298339844\n",
      "Epoch 580: : Loss: T_188.280 V_176.091 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  169.69839477539062\n",
      "Epoch 590: : Loss: T_189.963 V_169.698 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  167.3673858642578\n",
      "Epoch 600: : Loss: T_173.987 V_167.367 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  158.71029663085938\n",
      "Epoch 610: : Loss: T_182.129 V_158.710 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  154.939697265625\n",
      "Epoch 620: : Loss: T_171.950 V_154.940 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  151.2696990966797\n",
      "Epoch 630: : Loss: T_159.402 V_151.270 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  142.34442138671875\n",
      "Epoch 640: : Loss: T_156.481 V_142.344 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  131.6839599609375\n",
      "Epoch 650: : Loss: T_147.579 V_131.684 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  128.78607177734375\n",
      "Epoch 660: : Loss: T_154.525 V_128.786 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  125.41717529296875\n",
      "Epoch 670: : Loss: T_141.898 V_125.417 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  120.24358367919922\n",
      "Epoch 680: : Loss: T_128.914 V_120.244 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  117.41185760498047\n",
      "Epoch 690: : Loss: T_124.245 V_117.412 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  110.1878890991211\n",
      "Epoch 700: : Loss: T_113.441 V_110.188 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  102.76092529296875\n",
      "Epoch 710: : Loss: T_120.741 V_102.761 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  96.75646209716797\n",
      "Epoch 720: : Loss: T_118.133 V_96.756 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  92.24156188964844\n",
      "Epoch 730: : Loss: T_106.681 V_92.242 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  86.27156066894531\n",
      "Epoch 740: : Loss: T_105.993 V_86.272 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  84.2950668334961\n",
      "Epoch 750: : Loss: T_96.701 V_84.295 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  82.8775405883789\n",
      "Epoch 760: : Loss: T_95.795 V_82.878 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  77.98751068115234\n",
      "Epoch 770: : Loss: T_87.786 V_77.988 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  75.75638580322266\n",
      "Epoch 780: : Loss: T_84.402 V_75.756 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  72.6120376586914\n",
      "Epoch 790: : Loss: T_87.068 V_72.612 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  69.40164184570312\n",
      "Epoch 800: : Loss: T_86.709 V_69.402 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  67.82254028320312\n",
      "Epoch 810: : Loss: T_87.317 V_67.823 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  64.92979431152344\n",
      "Epoch 820: : Loss: T_78.055 V_64.930 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  62.0370979309082\n",
      "Epoch 830: : Loss: T_72.585 V_62.037 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  60.124698638916016\n",
      "Epoch 840: : Loss: T_78.139 V_60.125 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  58.652896881103516\n",
      "Epoch 850: : Loss: T_62.522 V_58.653 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  55.88644790649414\n",
      "Epoch 860: : Loss: T_66.721 V_55.886 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  51.21619415283203\n",
      "Epoch 870: : Loss: T_61.877 V_51.216 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  49.84797286987305\n",
      "Epoch 880: : Loss: T_72.410 V_49.848 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.102046966552734\n",
      "Epoch 890: : Loss: T_62.630 V_48.102 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.02125930786133\n",
      "Epoch 900: : Loss: T_59.035 V_48.021 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  45.47599792480469\n",
      "Epoch 910: : Loss: T_69.082 V_45.476 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  43.369659423828125\n",
      "Epoch 920: : Loss: T_51.459 V_43.370 | Acc: T_0.000) V_0.000\n",
      "Epoch 930: : Loss: T_53.502 V_43.527 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.06612777709961\n",
      "Epoch 940: : Loss: T_50.501 V_42.066 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.92723846435547\n",
      "Epoch 950: : Loss: T_58.870 V_40.927 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.60149383544922\n",
      "Epoch 960: : Loss: T_50.402 V_40.601 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.012210845947266\n",
      "Epoch 970: : Loss: T_60.033 V_39.012 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.88184356689453\n",
      "Epoch 980: : Loss: T_64.175 V_37.882 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.373111724853516\n",
      "Epoch 990: : Loss: T_57.880 V_36.373 | Acc: T_0.000) V_0.000\n",
      "Epoch 1000: : Loss: T_55.096 V_36.681 | Acc: T_0.000) V_0.000\n",
      "Epoch 1010: : Loss: T_52.645 V_36.466 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.141666412353516\n",
      "Epoch 1020: : Loss: T_57.616 V_36.142 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.435367584228516\n",
      "Epoch 1030: : Loss: T_50.021 V_35.435 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.6479606628418\n",
      "Epoch 1040: : Loss: T_48.466 V_34.648 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.15421676635742\n",
      "Epoch 1050: : Loss: T_60.392 V_33.154 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.5625114440918\n",
      "Epoch 1060: : Loss: T_53.155 V_32.563 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.39535903930664\n",
      "Epoch 1070: : Loss: T_49.609 V_32.395 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.137943267822266\n",
      "Epoch 1080: : Loss: T_53.567 V_31.138 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.001117706298828\n",
      "Epoch 1090: : Loss: T_48.892 V_31.001 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.473045349121094\n",
      "Epoch 1100: : Loss: T_46.132 V_30.473 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.834545135498047\n",
      "Epoch 1110: : Loss: T_43.253 V_29.835 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_55.666 V_31.748 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_45.139 V_31.166 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_50.072 V_30.446 | Acc: T_0.000) V_0.000\n",
      "Epoch 1150: : Loss: T_49.921 V_30.188 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_50.516 V_29.993 | Acc: T_0.000) V_0.000\n",
      "Epoch 1170: : Loss: T_42.012 V_30.374 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.6447811126709\n",
      "Epoch 1180: : Loss: T_51.743 V_29.645 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_47.454 V_29.672 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.35246467590332\n",
      "Epoch 1200: : Loss: T_53.289 V_29.352 | Acc: T_0.000) V_0.000\n",
      "Epoch 1210: : Loss: T_41.277 V_30.034 | Acc: T_0.000) V_0.000\n",
      "Epoch 1220: : Loss: T_46.254 V_29.613 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.24001693725586\n",
      "Epoch 1230: : Loss: T_60.125 V_29.240 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.963563919067383\n",
      "Epoch 1240: : Loss: T_46.985 V_28.964 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.451332092285156\n",
      "Epoch 1250: : Loss: T_47.811 V_28.451 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.450029373168945\n",
      "Epoch 1260: : Loss: T_38.952 V_28.450 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.65711784362793\n",
      "Epoch 1270: : Loss: T_50.547 V_27.657 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_51.836 V_27.712 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_47.473 V_28.225 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_48.513 V_28.321 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.461612701416016\n",
      "Epoch 1310: : Loss: T_39.773 V_27.462 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_42.659 V_28.054 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_49.849 V_28.445 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.39166831970215\n",
      "Epoch 1340: : Loss: T_49.972 V_27.392 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_45.263 V_28.007 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_46.261 V_28.316 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_47.502 V_27.643 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_49.789 V_27.697 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_49.034 V_27.674 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_45.977 V_28.169 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_37.439 V_28.902 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_47.957 V_27.930 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.370962142944336\n",
      "Epoch 1430: : Loss: T_39.165 V_27.371 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_38.565 V_28.304 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_45.537 V_28.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_48.311 V_28.339 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_47.943 V_28.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_46.993 V_27.839 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_45.441 V_28.570 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_46.403 V_28.876 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_50.152 V_28.062 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_40.661 V_28.342 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_50.925 V_27.887 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.236635208129883\n",
      "Epoch 1540: : Loss: T_36.549 V_27.237 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.145751953125\n",
      "Epoch 1550: : Loss: T_40.009 V_27.146 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.894182205200195\n",
      "Epoch 1560: : Loss: T_44.558 V_26.894 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_42.411 V_27.147 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.57967185974121\n",
      "Epoch 1580: : Loss: T_53.902 V_26.580 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.463132858276367\n",
      "Epoch 1590: : Loss: T_45.492 V_26.463 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_48.625 V_27.122 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_43.935 V_27.496 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_40.007 V_27.224 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_48.208 V_26.852 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_45.393 V_27.816 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_51.920 V_27.752 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_51.268 V_27.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_45.227 V_27.539 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_49.417 V_27.082 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_48.504 V_27.712 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_50.113 V_27.892 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_42.127 V_28.349 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_43.039 V_28.511 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_39.808 V_28.137 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_58.838 V_27.810 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_48.518 V_28.378 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_42.130 V_28.086 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_46.898 V_28.018 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_56.082 V_28.118 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_43.608 V_27.309 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_44.971 V_26.770 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_43.383 V_27.404 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_45.940 V_27.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_46.078 V_27.929 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_45.290 V_27.202 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_37.379 V_27.118 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_44.828 V_27.187 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_41.160 V_27.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_39.172 V_27.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_44.359 V_27.428 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_38.834 V_27.810 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_40.447 V_27.537 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_43.900 V_27.153 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_49.294 V_28.100 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_42.088 V_27.978 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_43.846 V_27.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_48.287 V_27.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_41.615 V_26.934 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_42.039 V_27.551 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_45.298 V_28.292 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_50.009 V_28.019 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_40.738 V_27.063 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_41.368 V_27.009 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_40.319 V_27.311 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_43.880 V_27.638 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_47.164 V_28.644 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_50.817 V_29.286 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_45.123 V_28.139 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_49.397 V_27.404 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_48.433 V_27.192 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_40.474 V_27.788 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_47.691 V_28.313 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_47.420 V_28.480 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_40.995 V_28.723 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_42.451 V_27.838 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_37.919 V_28.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_41.728 V_29.040 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_36.713 V_28.328 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_46.560 V_27.697 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_40.691 V_28.406 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_38.723 V_27.603 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_49.740 V_27.350 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_42.056 V_28.124 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_41.063 V_28.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_42.762 V_28.473 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_45.675 V_28.247 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_41.809 V_29.007 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_41.622 V_28.482 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_40.253 V_28.358 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_49.918 V_28.856 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_43.714 V_28.742 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_41.731 V_29.302 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_44.780 V_29.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_42.183 V_28.894 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_40.616 V_29.817 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_41.239 V_28.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_48.317 V_27.826 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_46.632 V_27.248 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_48.584 V_27.368 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_38.520 V_28.186 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_45.194 V_28.507 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_41.743 V_28.279 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_41.573 V_27.494 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_37.315 V_27.283 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_40.752 V_28.610 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_41.295 V_28.940 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_42.048 V_29.372 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_39.401 V_29.313 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_46.572 V_27.224 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_36.471 V_27.599 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_51.886 V_28.315 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_38.349 V_28.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_36.504 V_27.859 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_35.925 V_27.471 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_38.116 V_27.286 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_51.066 V_27.328 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_41.952 V_28.790 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_44.890 V_28.834 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_47.579 V_28.460 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_42.952 V_28.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_39.206 V_28.217 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_40.975 V_27.675 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_43.302 V_27.868 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_41.584 V_27.827 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_43.087 V_28.047 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_40.174 V_27.824 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_41.838 V_28.724 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_52.116 V_29.590 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_39.744 V_29.772 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_44.562 V_28.710 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_44.519 V_28.995 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_37.285 V_28.019 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_45.874 V_28.877 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_37.962 V_29.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_38.159 V_29.008 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_49.199 V_29.132 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_41.282 V_29.769 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_49.012 V_28.385 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_42.014 V_28.135 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_42.529 V_28.470 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_37.623 V_27.972 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_40.927 V_29.207 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_37.360 V_30.085 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_41.057 V_29.262 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_45.515 V_30.294 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_36.589 V_29.299 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_38.091 V_29.651 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_37.296 V_30.249 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_42.604 V_30.422 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_41.448 V_29.225 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_39.230 V_28.422 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_46.542 V_30.004 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_41.323 V_29.863 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_51.793 V_29.374 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_46.939 V_29.699 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_31.884 V_29.998 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_38.708 V_30.807 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_36.001 V_29.920 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_43.952 V_29.789 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_36.475 V_29.472 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_46.340 V_29.213 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_41.786 V_29.104 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_39.625 V_27.708 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_45.847 V_28.900 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_43.902 V_29.373 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_38.669 V_29.322 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_37.581 V_29.029 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_37.835 V_29.479 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_42.086 V_29.279 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_38.472 V_27.991 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_39.899 V_29.669 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_53.132 V_29.577 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_36.920 V_29.042 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_39.037 V_28.230 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_42.518 V_27.991 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_45.764 V_29.863 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_47.082 V_30.709 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_48.744 V_29.286 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_43.099 V_28.492 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_37.489 V_27.496 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_48.521 V_27.875 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_39.280 V_28.397 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_40.054 V_27.877 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_43.905 V_28.586 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_38.986 V_28.752 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_38.025 V_27.831 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_37.792 V_28.051 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_47.537 V_28.291 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_48.815 V_27.862 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_33.639 V_28.022 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_41.251 V_27.856 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_43.915 V_28.269 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_48.524 V_29.957 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_35.966 V_28.379 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_41.804 V_27.616 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_45.709 V_29.724 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_39.859 V_30.295 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_45.597 V_30.074 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_45.903 V_28.946 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_43.083 V_29.543 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_39.660 V_29.136 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_37.696 V_28.283 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_43.493 V_30.031 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_41.437 V_29.244 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_38.890 V_29.374 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_44.320 V_28.681 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_37.752 V_27.953 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_42.878 V_28.471 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_40.128 V_28.926 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_45.641 V_28.268 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_38.525 V_28.707 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  608.241455078125\n",
      "Epoch 010: : Loss: T_559.060 V_608.241 | Acc: T_0.000) V_0.000\n",
      "Epoch 020: : Loss: T_557.712 V_609.036 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  608.0650024414062\n",
      "Epoch 030: : Loss: T_552.902 V_608.065 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  605.1318359375\n",
      "Epoch 040: : Loss: T_549.782 V_605.132 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  602.6989135742188\n",
      "Epoch 050: : Loss: T_542.616 V_602.699 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  597.9462890625\n",
      "Epoch 060: : Loss: T_538.834 V_597.946 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  592.933349609375\n",
      "Epoch 070: : Loss: T_534.834 V_592.933 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  587.6950073242188\n",
      "Epoch 080: : Loss: T_531.300 V_587.695 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  582.2579345703125\n",
      "Epoch 090: : Loss: T_530.077 V_582.258 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  578.088623046875\n",
      "Epoch 100: : Loss: T_523.413 V_578.089 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  574.2398071289062\n",
      "Epoch 110: : Loss: T_516.324 V_574.240 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  567.5662231445312\n",
      "Epoch 120: : Loss: T_510.038 V_567.566 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  559.124755859375\n",
      "Epoch 130: : Loss: T_506.693 V_559.125 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  551.8788452148438\n",
      "Epoch 140: : Loss: T_500.837 V_551.879 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  544.028564453125\n",
      "Epoch 150: : Loss: T_491.817 V_544.029 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  532.7189331054688\n",
      "Epoch 160: : Loss: T_490.878 V_532.719 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  519.4169311523438\n",
      "Epoch 170: : Loss: T_484.500 V_519.417 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  516.715087890625\n",
      "Epoch 180: : Loss: T_473.634 V_516.715 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  511.3428955078125\n",
      "Epoch 190: : Loss: T_461.284 V_511.343 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  496.66497802734375\n",
      "Epoch 200: : Loss: T_461.420 V_496.665 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  485.23834228515625\n",
      "Epoch 210: : Loss: T_454.585 V_485.238 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  479.51446533203125\n",
      "Epoch 220: : Loss: T_450.011 V_479.514 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  479.5072326660156\n",
      "Epoch 230: : Loss: T_444.516 V_479.507 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  476.512451171875\n",
      "Epoch 240: : Loss: T_434.555 V_476.512 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  458.1269226074219\n",
      "Epoch 250: : Loss: T_429.079 V_458.127 | Acc: T_0.000) V_0.000\n",
      "Epoch 260: : Loss: T_416.987 V_459.017 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  449.8070068359375\n",
      "Epoch 270: : Loss: T_416.122 V_449.807 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  432.5408020019531\n",
      "Epoch 280: : Loss: T_413.347 V_432.541 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  413.17303466796875\n",
      "Epoch 290: : Loss: T_401.611 V_413.173 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  400.5103759765625\n",
      "Epoch 300: : Loss: T_387.795 V_400.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_380.513 V_402.593 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  387.94024658203125\n",
      "Epoch 320: : Loss: T_368.460 V_387.940 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  381.9743347167969\n",
      "Epoch 330: : Loss: T_380.718 V_381.974 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  381.16619873046875\n",
      "Epoch 340: : Loss: T_358.356 V_381.166 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  357.4234619140625\n",
      "Epoch 350: : Loss: T_364.163 V_357.423 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  351.2411804199219\n",
      "Epoch 360: : Loss: T_345.624 V_351.241 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_345.026 V_356.341 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  348.08782958984375\n",
      "Epoch 380: : Loss: T_332.102 V_348.088 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  320.81610107421875\n",
      "Epoch 390: : Loss: T_329.193 V_320.816 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  316.23919677734375\n",
      "Epoch 400: : Loss: T_315.708 V_316.239 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  315.160888671875\n",
      "Epoch 410: : Loss: T_301.883 V_315.161 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  309.1101379394531\n",
      "Epoch 420: : Loss: T_294.731 V_309.110 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  292.41644287109375\n",
      "Epoch 430: : Loss: T_292.640 V_292.416 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  279.87127685546875\n",
      "Epoch 440: : Loss: T_281.065 V_279.871 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  273.4913635253906\n",
      "Epoch 450: : Loss: T_272.126 V_273.491 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  266.6199035644531\n",
      "Epoch 460: : Loss: T_270.648 V_266.620 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  258.398681640625\n",
      "Epoch 470: : Loss: T_263.140 V_258.399 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  252.74942016601562\n",
      "Epoch 480: : Loss: T_245.870 V_252.749 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  251.23199462890625\n",
      "Epoch 490: : Loss: T_259.336 V_251.232 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  239.19644165039062\n",
      "Epoch 500: : Loss: T_243.210 V_239.196 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  229.22459411621094\n",
      "Epoch 510: : Loss: T_233.447 V_229.225 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  220.90538024902344\n",
      "Epoch 520: : Loss: T_231.229 V_220.905 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  210.8342742919922\n",
      "Epoch 530: : Loss: T_229.631 V_210.834 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  203.24530029296875\n",
      "Epoch 540: : Loss: T_210.741 V_203.245 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  197.6503143310547\n",
      "Epoch 550: : Loss: T_197.176 V_197.650 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  188.05186462402344\n",
      "Epoch 560: : Loss: T_186.437 V_188.052 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  177.4118194580078\n",
      "Epoch 570: : Loss: T_196.144 V_177.412 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  170.8611602783203\n",
      "Epoch 580: : Loss: T_185.661 V_170.861 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  168.28358459472656\n",
      "Epoch 590: : Loss: T_176.502 V_168.284 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  160.48556518554688\n",
      "Epoch 600: : Loss: T_163.979 V_160.486 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  155.27572631835938\n",
      "Epoch 610: : Loss: T_154.422 V_155.276 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  139.8797149658203\n",
      "Epoch 620: : Loss: T_155.498 V_139.880 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  136.6406707763672\n",
      "Epoch 630: : Loss: T_151.432 V_136.641 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  130.69253540039062\n",
      "Epoch 640: : Loss: T_152.496 V_130.693 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  127.22986602783203\n",
      "Epoch 650: : Loss: T_126.944 V_127.230 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  120.35611724853516\n",
      "Epoch 660: : Loss: T_130.881 V_120.356 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  120.35597229003906\n",
      "Epoch 670: : Loss: T_124.814 V_120.356 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  112.47244262695312\n",
      "Epoch 680: : Loss: T_123.186 V_112.472 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  99.04171752929688\n",
      "Epoch 690: : Loss: T_122.316 V_99.042 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  96.47211456298828\n",
      "Epoch 700: : Loss: T_113.680 V_96.472 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  95.29653930664062\n",
      "Epoch 710: : Loss: T_112.147 V_95.297 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  93.13938903808594\n",
      "Epoch 720: : Loss: T_105.186 V_93.139 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  88.76338958740234\n",
      "Epoch 730: : Loss: T_105.517 V_88.763 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  80.30967712402344\n",
      "Epoch 740: : Loss: T_100.442 V_80.310 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  72.9593505859375\n",
      "Epoch 750: : Loss: T_88.758 V_72.959 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  68.71795654296875\n",
      "Epoch 760: : Loss: T_82.627 V_68.718 | Acc: T_0.000) V_0.000\n",
      "Epoch 770: : Loss: T_86.277 V_69.035 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  66.48524475097656\n",
      "Epoch 780: : Loss: T_87.630 V_66.485 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  61.09625244140625\n",
      "Epoch 790: : Loss: T_86.410 V_61.096 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  54.523406982421875\n",
      "Epoch 800: : Loss: T_80.686 V_54.523 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  53.17506408691406\n",
      "Epoch 810: : Loss: T_72.890 V_53.175 | Acc: T_0.000) V_0.000\n",
      "Epoch 820: : Loss: T_76.200 V_53.464 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  51.21181869506836\n",
      "Epoch 830: : Loss: T_76.010 V_51.212 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  47.95515823364258\n",
      "Epoch 840: : Loss: T_67.773 V_47.955 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  44.84300994873047\n",
      "Epoch 850: : Loss: T_59.389 V_44.843 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.64621353149414\n",
      "Epoch 860: : Loss: T_63.018 V_42.646 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.46487808227539\n",
      "Epoch 870: : Loss: T_63.395 V_42.465 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.69234848022461\n",
      "Epoch 880: : Loss: T_51.415 V_40.692 | Acc: T_0.000) V_0.000\n",
      "Epoch 890: : Loss: T_61.996 V_40.901 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.72843933105469\n",
      "Epoch 900: : Loss: T_58.519 V_38.728 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.554351806640625\n",
      "Epoch 910: : Loss: T_64.313 V_38.554 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.244972229003906\n",
      "Epoch 920: : Loss: T_53.289 V_34.245 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.33139419555664\n",
      "Epoch 930: : Loss: T_54.039 V_32.331 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.450653076171875\n",
      "Epoch 940: : Loss: T_59.422 V_31.451 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.951765060424805\n",
      "Epoch 950: : Loss: T_53.604 V_30.952 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.19236183166504\n",
      "Epoch 960: : Loss: T_48.509 V_30.192 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.850200653076172\n",
      "Epoch 970: : Loss: T_50.038 V_29.850 | Acc: T_0.000) V_0.000\n",
      "Epoch 980: : Loss: T_44.874 V_30.356 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.674129486083984\n",
      "Epoch 990: : Loss: T_51.345 V_28.674 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.48358917236328\n",
      "Epoch 1000: : Loss: T_48.401 V_27.484 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.025711059570312\n",
      "Epoch 1010: : Loss: T_58.452 V_27.026 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.050901412963867\n",
      "Epoch 1020: : Loss: T_44.334 V_26.051 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.305418014526367\n",
      "Epoch 1030: : Loss: T_49.712 V_25.305 | Acc: T_0.000) V_0.000\n",
      "Epoch 1040: : Loss: T_50.893 V_25.686 | Acc: T_0.000) V_0.000\n",
      "Epoch 1050: : Loss: T_43.002 V_26.084 | Acc: T_0.000) V_0.000\n",
      "Epoch 1060: : Loss: T_50.178 V_26.047 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.92178726196289\n",
      "Epoch 1070: : Loss: T_49.456 V_24.922 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.42149543762207\n",
      "Epoch 1080: : Loss: T_47.538 V_24.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 1090: : Loss: T_53.180 V_24.571 | Acc: T_0.000) V_0.000\n",
      "Epoch 1100: : Loss: T_46.836 V_25.139 | Acc: T_0.000) V_0.000\n",
      "Epoch 1110: : Loss: T_47.756 V_25.021 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_53.253 V_25.053 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_51.132 V_24.989 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_46.506 V_25.034 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  23.3465576171875\n",
      "Epoch 1150: : Loss: T_47.412 V_23.347 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_46.619 V_23.674 | Acc: T_0.000) V_0.000\n",
      "Epoch 1170: : Loss: T_48.920 V_23.504 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  23.16730499267578\n",
      "Epoch 1180: : Loss: T_44.472 V_23.167 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  22.346460342407227\n",
      "Epoch 1190: : Loss: T_45.510 V_22.346 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  22.265830993652344\n",
      "Epoch 1200: : Loss: T_59.488 V_22.266 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  21.92409324645996\n",
      "Epoch 1210: : Loss: T_52.485 V_21.924 | Acc: T_0.000) V_0.000\n",
      "Epoch 1220: : Loss: T_46.334 V_22.248 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_42.997 V_22.385 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_50.130 V_22.399 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_45.536 V_22.126 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  21.838642120361328\n",
      "Epoch 1260: : Loss: T_47.678 V_21.839 | Acc: T_0.000) V_0.000\n",
      "Epoch 1270: : Loss: T_45.344 V_22.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_52.029 V_22.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_46.766 V_23.497 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_40.724 V_22.750 | Acc: T_0.000) V_0.000\n",
      "Epoch 1310: : Loss: T_43.057 V_22.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_46.751 V_22.326 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_42.106 V_23.039 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_42.072 V_22.774 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_48.665 V_22.208 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  21.669816970825195\n",
      "Epoch 1360: : Loss: T_43.394 V_21.670 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  21.502986907958984\n",
      "Epoch 1370: : Loss: T_41.786 V_21.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_41.479 V_22.157 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_52.162 V_22.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_38.707 V_22.052 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  21.43022346496582\n",
      "Epoch 1410: : Loss: T_44.995 V_21.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_39.527 V_21.659 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_42.666 V_22.365 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_43.854 V_22.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_39.000 V_23.281 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_43.282 V_23.233 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_49.554 V_22.374 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_40.555 V_22.320 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_46.428 V_22.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_42.140 V_22.175 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_36.149 V_22.153 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_40.746 V_22.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_36.436 V_21.830 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_38.715 V_22.655 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_56.613 V_23.210 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_45.025 V_23.266 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_42.322 V_22.752 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_44.706 V_21.980 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_51.398 V_22.014 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_42.045 V_21.652 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_42.399 V_22.176 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_45.890 V_23.544 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_45.285 V_23.348 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_46.168 V_23.062 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_43.172 V_22.260 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_49.284 V_22.796 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_47.476 V_22.795 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_43.021 V_22.982 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_46.140 V_22.628 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_41.933 V_22.077 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_42.604 V_21.568 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_35.753 V_22.185 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_43.774 V_23.068 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_46.627 V_23.353 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_42.504 V_24.051 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_49.692 V_24.234 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_46.335 V_23.373 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_43.663 V_21.911 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_60.359 V_21.688 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_42.097 V_23.485 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_37.506 V_22.130 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  21.388456344604492\n",
      "Epoch 1820: : Loss: T_37.121 V_21.388 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  20.818876266479492\n",
      "Epoch 1830: : Loss: T_42.011 V_20.819 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_37.362 V_21.091 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_44.883 V_21.145 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_41.819 V_21.494 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_43.313 V_22.908 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_47.408 V_23.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_41.793 V_23.037 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_41.880 V_22.203 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_42.650 V_22.163 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_43.753 V_22.385 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_34.647 V_23.261 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_41.348 V_23.332 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_43.447 V_23.170 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_44.777 V_22.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_43.038 V_21.985 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_49.373 V_21.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_35.248 V_22.203 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_38.985 V_23.107 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_38.339 V_22.457 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_48.647 V_22.282 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_41.668 V_23.060 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_37.971 V_23.108 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_48.867 V_22.648 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_38.869 V_22.907 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_40.223 V_22.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_37.397 V_22.387 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_37.738 V_23.096 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_41.576 V_22.878 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_43.059 V_22.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_45.518 V_23.349 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_45.375 V_23.700 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_38.214 V_23.734 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_43.256 V_22.993 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_41.542 V_23.027 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_41.369 V_23.308 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_41.718 V_23.082 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_36.080 V_23.075 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_50.480 V_23.303 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_42.871 V_23.322 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_44.256 V_22.651 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_40.822 V_22.626 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_46.950 V_22.230 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_48.445 V_22.775 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_40.650 V_22.911 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_36.604 V_22.203 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_37.932 V_22.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_41.363 V_22.823 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_45.777 V_22.723 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_36.601 V_22.667 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_45.151 V_23.517 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_37.004 V_22.742 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_49.639 V_22.387 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_39.804 V_22.222 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_40.053 V_23.003 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_37.424 V_22.581 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_47.397 V_22.525 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_38.110 V_22.374 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_41.765 V_23.264 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_35.429 V_23.774 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_39.930 V_23.231 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_44.293 V_23.944 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_33.971 V_23.650 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_43.588 V_23.717 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_39.481 V_23.550 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_38.822 V_22.733 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_43.153 V_22.054 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_32.816 V_22.601 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_43.176 V_22.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_45.119 V_22.374 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_43.038 V_22.481 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_40.672 V_22.774 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_39.599 V_23.748 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_41.455 V_23.454 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_44.445 V_23.355 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_42.484 V_23.089 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_48.276 V_23.133 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_46.294 V_23.155 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_39.228 V_23.044 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_44.038 V_23.522 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_45.983 V_23.246 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_40.480 V_23.715 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_44.222 V_23.356 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_36.579 V_23.034 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_39.456 V_23.123 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_40.241 V_21.918 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_33.961 V_21.520 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_42.403 V_21.959 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_35.414 V_22.479 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_43.307 V_22.407 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_44.024 V_22.102 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_38.450 V_22.307 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_38.610 V_22.583 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_36.988 V_21.876 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_36.578 V_21.811 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_36.179 V_22.122 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_39.141 V_22.948 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_42.736 V_23.500 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_35.728 V_24.345 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_39.636 V_24.606 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_39.891 V_24.002 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_40.557 V_23.888 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_40.321 V_23.153 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_39.358 V_22.476 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_43.902 V_22.536 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_38.809 V_23.021 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_37.870 V_22.782 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_38.988 V_22.726 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_39.872 V_22.562 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_50.337 V_22.828 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_44.614 V_22.492 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_45.091 V_21.834 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_43.062 V_21.950 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_40.247 V_22.005 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_40.961 V_22.244 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_37.893 V_21.742 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_48.244 V_22.489 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_41.865 V_22.960 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_40.028 V_23.690 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_33.345 V_23.555 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_38.591 V_23.235 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_47.650 V_22.548 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_42.116 V_22.011 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_43.559 V_22.666 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_40.153 V_22.123 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_37.230 V_22.625 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_41.976 V_22.946 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_41.588 V_23.254 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_36.852 V_22.697 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_36.137 V_22.890 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_39.560 V_22.720 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_38.523 V_22.875 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_42.460 V_22.821 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_40.267 V_22.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_40.115 V_22.282 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_44.717 V_22.620 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_39.602 V_22.296 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_33.990 V_22.178 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_40.491 V_22.390 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_39.284 V_22.391 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_41.380 V_22.406 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_39.282 V_22.925 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_42.257 V_22.964 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_39.557 V_23.348 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_37.815 V_22.916 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_38.274 V_22.837 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_40.123 V_22.180 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_42.858 V_21.795 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_38.113 V_22.673 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_33.341 V_22.383 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_34.449 V_22.922 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_39.071 V_22.747 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_41.709 V_22.735 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_34.688 V_22.783 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_37.013 V_22.839 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_32.775 V_23.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_40.454 V_22.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_43.933 V_22.224 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_37.088 V_22.481 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_44.533 V_23.042 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_43.833 V_23.501 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_41.587 V_23.945 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_37.672 V_23.591 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_43.670 V_23.781 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_36.550 V_23.740 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_37.409 V_23.346 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_42.896 V_23.819 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_42.545 V_23.802 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_30.069 V_23.289 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  584.7314453125\n",
      "Epoch 010: : Loss: T_519.483 V_584.731 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  574.4747314453125\n",
      "Epoch 020: : Loss: T_512.190 V_574.475 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  570.057373046875\n",
      "Epoch 030: : Loss: T_509.594 V_570.057 | Acc: T_0.000) V_0.000\n",
      "Epoch 040: : Loss: T_504.297 V_570.987 | Acc: T_0.000) V_0.000\n",
      "Epoch 050: : Loss: T_502.962 V_571.371 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  569.7788696289062\n",
      "Epoch 060: : Loss: T_500.461 V_569.779 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  568.7957763671875\n",
      "Epoch 070: : Loss: T_495.258 V_568.796 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  566.2047729492188\n",
      "Epoch 080: : Loss: T_491.156 V_566.205 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  562.4268798828125\n",
      "Epoch 090: : Loss: T_483.510 V_562.427 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  557.6250610351562\n",
      "Epoch 100: : Loss: T_481.295 V_557.625 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  546.9336547851562\n",
      "Epoch 110: : Loss: T_474.265 V_546.934 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  536.2769165039062\n",
      "Epoch 120: : Loss: T_471.729 V_536.277 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  534.9765014648438\n",
      "Epoch 130: : Loss: T_468.057 V_534.977 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  529.6165771484375\n",
      "Epoch 140: : Loss: T_461.954 V_529.617 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  517.6324462890625\n",
      "Epoch 150: : Loss: T_455.208 V_517.632 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  513.9942626953125\n",
      "Epoch 160: : Loss: T_459.031 V_513.994 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  506.6777648925781\n",
      "Epoch 170: : Loss: T_438.251 V_506.678 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  500.343994140625\n",
      "Epoch 180: : Loss: T_438.701 V_500.344 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  488.47039794921875\n",
      "Epoch 190: : Loss: T_427.465 V_488.470 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  483.13189697265625\n",
      "Epoch 200: : Loss: T_435.663 V_483.132 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  474.8296813964844\n",
      "Epoch 210: : Loss: T_408.683 V_474.830 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  463.0803527832031\n",
      "Epoch 220: : Loss: T_414.906 V_463.080 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  454.7474365234375\n",
      "Epoch 230: : Loss: T_410.226 V_454.747 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  447.20452880859375\n",
      "Epoch 240: : Loss: T_402.813 V_447.205 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  440.2712707519531\n",
      "Epoch 250: : Loss: T_398.954 V_440.271 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  430.80560302734375\n",
      "Epoch 260: : Loss: T_393.732 V_430.806 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  419.2088623046875\n",
      "Epoch 270: : Loss: T_375.413 V_419.209 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  406.1355285644531\n",
      "Epoch 280: : Loss: T_373.524 V_406.136 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  398.3525695800781\n",
      "Epoch 290: : Loss: T_365.456 V_398.353 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  386.82196044921875\n",
      "Epoch 300: : Loss: T_359.509 V_386.822 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  370.24847412109375\n",
      "Epoch 310: : Loss: T_341.740 V_370.248 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  367.8978271484375\n",
      "Epoch 320: : Loss: T_348.894 V_367.898 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  354.6260681152344\n",
      "Epoch 330: : Loss: T_329.931 V_354.626 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  337.9403991699219\n",
      "Epoch 340: : Loss: T_341.230 V_337.940 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  336.9568786621094\n",
      "Epoch 350: : Loss: T_329.026 V_336.957 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  329.01153564453125\n",
      "Epoch 360: : Loss: T_310.616 V_329.012 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  319.2436218261719\n",
      "Epoch 370: : Loss: T_314.641 V_319.244 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  310.7296142578125\n",
      "Epoch 380: : Loss: T_300.985 V_310.730 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  300.5108642578125\n",
      "Epoch 390: : Loss: T_299.075 V_300.511 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  293.2493896484375\n",
      "Epoch 400: : Loss: T_289.210 V_293.249 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  274.4519348144531\n",
      "Epoch 410: : Loss: T_277.358 V_274.452 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  263.9906311035156\n",
      "Epoch 420: : Loss: T_268.618 V_263.991 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  256.3761901855469\n",
      "Epoch 430: : Loss: T_265.229 V_256.376 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  251.62196350097656\n",
      "Epoch 440: : Loss: T_251.508 V_251.622 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  236.4418182373047\n",
      "Epoch 450: : Loss: T_253.143 V_236.442 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  230.96575927734375\n",
      "Epoch 460: : Loss: T_237.981 V_230.966 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  223.3671875\n",
      "Epoch 470: : Loss: T_230.582 V_223.367 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  212.0424346923828\n",
      "Epoch 480: : Loss: T_221.697 V_212.042 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  207.756103515625\n",
      "Epoch 490: : Loss: T_208.461 V_207.756 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  201.5003662109375\n",
      "Epoch 500: : Loss: T_222.148 V_201.500 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  187.96304321289062\n",
      "Epoch 510: : Loss: T_214.795 V_187.963 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  179.84475708007812\n",
      "Epoch 520: : Loss: T_199.727 V_179.845 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  172.50518798828125\n",
      "Epoch 530: : Loss: T_187.150 V_172.505 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  170.020751953125\n",
      "Epoch 540: : Loss: T_182.416 V_170.021 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  161.5823516845703\n",
      "Epoch 550: : Loss: T_170.427 V_161.582 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  153.85487365722656\n",
      "Epoch 560: : Loss: T_174.521 V_153.855 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  144.326416015625\n",
      "Epoch 570: : Loss: T_159.956 V_144.326 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  138.921630859375\n",
      "Epoch 580: : Loss: T_160.309 V_138.922 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  134.0762481689453\n",
      "Epoch 590: : Loss: T_150.595 V_134.076 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  126.59013366699219\n",
      "Epoch 600: : Loss: T_148.829 V_126.590 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  119.1455307006836\n",
      "Epoch 610: : Loss: T_136.806 V_119.146 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  116.40948486328125\n",
      "Epoch 620: : Loss: T_136.934 V_116.409 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  112.14935302734375\n",
      "Epoch 630: : Loss: T_122.238 V_112.149 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  104.09669494628906\n",
      "Epoch 640: : Loss: T_121.078 V_104.097 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  99.33902740478516\n",
      "Epoch 650: : Loss: T_117.768 V_99.339 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  95.0582504272461\n",
      "Epoch 660: : Loss: T_124.153 V_95.058 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  90.60820770263672\n",
      "Epoch 670: : Loss: T_110.629 V_90.608 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  88.04553985595703\n",
      "Epoch 680: : Loss: T_102.590 V_88.046 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  83.63034057617188\n",
      "Epoch 690: : Loss: T_101.721 V_83.630 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  80.26533508300781\n",
      "Epoch 700: : Loss: T_99.477 V_80.265 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  76.59436798095703\n",
      "Epoch 710: : Loss: T_88.120 V_76.594 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  71.84266662597656\n",
      "Epoch 720: : Loss: T_90.940 V_71.843 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  67.53266143798828\n",
      "Epoch 730: : Loss: T_90.336 V_67.533 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  63.647762298583984\n",
      "Epoch 740: : Loss: T_79.179 V_63.648 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  61.95952224731445\n",
      "Epoch 750: : Loss: T_88.551 V_61.960 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  57.65675735473633\n",
      "Epoch 760: : Loss: T_75.810 V_57.657 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  55.85093688964844\n",
      "Epoch 770: : Loss: T_76.721 V_55.851 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  54.151336669921875\n",
      "Epoch 780: : Loss: T_69.077 V_54.151 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  53.944190979003906\n",
      "Epoch 790: : Loss: T_62.991 V_53.944 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  52.847076416015625\n",
      "Epoch 800: : Loss: T_69.022 V_52.847 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  50.848331451416016\n",
      "Epoch 810: : Loss: T_62.250 V_50.848 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  45.8642578125\n",
      "Epoch 820: : Loss: T_65.592 V_45.864 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  43.12310028076172\n",
      "Epoch 830: : Loss: T_66.785 V_43.123 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  43.07137680053711\n",
      "Epoch 840: : Loss: T_55.713 V_43.071 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.44060516357422\n",
      "Epoch 850: : Loss: T_63.423 V_42.441 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.53963088989258\n",
      "Epoch 860: : Loss: T_59.358 V_40.540 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.64999771118164\n",
      "Epoch 870: : Loss: T_57.535 V_38.650 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.16855239868164\n",
      "Epoch 880: : Loss: T_51.695 V_37.169 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.55269241333008\n",
      "Epoch 890: : Loss: T_55.360 V_36.553 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.484554290771484\n",
      "Epoch 900: : Loss: T_59.517 V_35.485 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.07248306274414\n",
      "Epoch 910: : Loss: T_51.325 V_34.072 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.00153350830078\n",
      "Epoch 920: : Loss: T_49.008 V_34.002 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.15610122680664\n",
      "Epoch 930: : Loss: T_50.783 V_33.156 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.75641632080078\n",
      "Epoch 940: : Loss: T_51.343 V_32.756 | Acc: T_0.000) V_0.000\n",
      "Epoch 950: : Loss: T_43.759 V_33.304 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.690128326416016\n",
      "Epoch 960: : Loss: T_43.416 V_32.690 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.107383728027344\n",
      "Epoch 970: : Loss: T_42.701 V_31.107 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.079845428466797\n",
      "Epoch 980: : Loss: T_50.568 V_30.080 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.879823684692383\n",
      "Epoch 990: : Loss: T_40.439 V_29.880 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.608762741088867\n",
      "Epoch 1000: : Loss: T_51.807 V_29.609 | Acc: T_0.000) V_0.000\n",
      "Epoch 1010: : Loss: T_49.206 V_30.595 | Acc: T_0.000) V_0.000\n",
      "Epoch 1020: : Loss: T_46.415 V_30.221 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.44355583190918\n",
      "Epoch 1030: : Loss: T_48.311 V_29.444 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.133554458618164\n",
      "Epoch 1040: : Loss: T_54.182 V_29.134 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.02154541015625\n",
      "Epoch 1050: : Loss: T_48.324 V_29.022 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.64690589904785\n",
      "Epoch 1060: : Loss: T_37.265 V_27.647 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.47930335998535\n",
      "Epoch 1070: : Loss: T_36.754 V_27.479 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.131818771362305\n",
      "Epoch 1080: : Loss: T_50.403 V_27.132 | Acc: T_0.000) V_0.000\n",
      "Epoch 1090: : Loss: T_41.677 V_27.799 | Acc: T_0.000) V_0.000\n",
      "Epoch 1100: : Loss: T_44.873 V_27.936 | Acc: T_0.000) V_0.000\n",
      "Epoch 1110: : Loss: T_51.912 V_27.246 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_42.808 V_27.623 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_40.265 V_27.743 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_42.249 V_27.799 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.558639526367188\n",
      "Epoch 1150: : Loss: T_40.252 V_26.559 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_43.081 V_26.958 | Acc: T_0.000) V_0.000\n",
      "Epoch 1170: : Loss: T_40.972 V_26.814 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.88098907470703\n",
      "Epoch 1180: : Loss: T_42.037 V_25.881 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_44.877 V_26.623 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_45.583 V_27.198 | Acc: T_0.000) V_0.000\n",
      "Epoch 1210: : Loss: T_38.513 V_26.235 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.69839096069336\n",
      "Epoch 1220: : Loss: T_47.918 V_25.698 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_41.531 V_26.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_39.548 V_27.079 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_36.434 V_27.063 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_35.566 V_25.904 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.185434341430664\n",
      "Epoch 1270: : Loss: T_35.460 V_25.185 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_46.431 V_25.922 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_40.110 V_26.057 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_47.873 V_26.047 | Acc: T_0.000) V_0.000\n",
      "Epoch 1310: : Loss: T_43.426 V_26.488 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_39.807 V_26.335 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_32.988 V_25.810 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_41.800 V_26.081 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_44.692 V_26.214 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_41.207 V_25.759 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_48.138 V_25.917 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_37.597 V_25.615 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_40.315 V_25.282 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_41.268 V_26.175 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_42.540 V_26.228 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_40.714 V_25.934 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_39.445 V_25.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_45.222 V_26.221 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_36.844 V_26.647 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_42.241 V_25.786 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_45.975 V_26.876 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_41.909 V_26.423 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_42.876 V_26.379 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_43.890 V_26.022 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_36.246 V_26.553 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_43.907 V_25.787 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_40.527 V_26.435 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_44.990 V_26.344 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_43.400 V_26.491 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_36.315 V_26.137 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_43.517 V_25.818 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_46.667 V_26.563 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_34.534 V_25.791 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_37.802 V_25.545 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_44.459 V_26.095 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_41.858 V_26.078 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_42.299 V_25.908 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_41.234 V_25.862 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_40.713 V_25.473 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_43.300 V_26.192 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_39.991 V_26.773 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_35.458 V_26.794 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_38.269 V_26.340 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_40.041 V_26.219 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_38.408 V_25.916 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_37.073 V_26.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_35.132 V_26.240 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_43.533 V_26.602 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_33.079 V_26.942 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_38.213 V_27.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_39.996 V_27.024 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_34.993 V_26.485 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_34.155 V_26.825 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_36.807 V_26.388 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_39.316 V_26.654 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_38.991 V_26.922 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_45.896 V_26.834 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_36.117 V_27.115 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_37.881 V_27.265 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_37.549 V_27.187 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_43.544 V_26.817 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_40.412 V_26.465 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_34.138 V_26.632 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_44.210 V_27.431 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_45.657 V_27.182 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_47.513 V_27.112 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_34.516 V_27.144 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_36.715 V_27.384 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_41.913 V_27.381 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_41.408 V_27.607 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_37.631 V_26.529 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_44.641 V_26.200 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_37.743 V_26.277 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_41.629 V_26.370 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_34.398 V_26.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_40.657 V_26.573 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_43.495 V_27.353 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_39.376 V_26.915 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_38.871 V_26.091 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_37.861 V_26.382 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_32.670 V_26.807 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_37.143 V_26.922 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_45.541 V_26.530 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_43.763 V_26.247 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_49.295 V_25.898 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_36.393 V_26.757 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_37.917 V_27.041 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_33.331 V_26.979 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_37.288 V_26.813 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_36.669 V_27.140 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_32.312 V_26.995 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_40.750 V_26.779 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_43.105 V_27.354 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_38.948 V_27.140 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_40.253 V_26.832 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_38.410 V_27.333 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_34.084 V_27.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_41.071 V_27.726 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_45.010 V_27.613 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_40.544 V_27.877 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_40.765 V_27.576 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_37.796 V_27.104 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_38.816 V_27.215 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_46.989 V_27.236 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_34.373 V_27.350 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_34.894 V_27.478 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_32.048 V_27.064 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_45.404 V_27.070 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_42.703 V_26.900 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_42.605 V_26.500 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_37.606 V_26.960 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_37.388 V_27.009 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_38.499 V_26.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_33.152 V_26.924 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_37.215 V_26.984 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_40.016 V_26.836 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_40.628 V_26.745 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_38.356 V_27.495 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_31.690 V_26.997 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_38.354 V_26.554 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_34.808 V_27.010 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_35.230 V_26.386 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_38.247 V_27.087 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_40.131 V_27.477 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_38.511 V_26.446 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_37.657 V_26.597 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_34.132 V_26.993 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_35.776 V_27.222 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_40.714 V_26.552 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_39.472 V_26.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_32.639 V_26.649 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_37.084 V_26.881 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_36.616 V_26.845 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_38.926 V_26.779 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_37.841 V_26.802 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_32.958 V_26.454 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_36.802 V_26.023 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_39.121 V_26.511 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_35.970 V_26.868 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_35.520 V_26.840 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_33.738 V_27.159 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_34.656 V_26.799 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_36.628 V_27.038 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_40.865 V_26.888 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_34.825 V_27.094 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_44.230 V_27.373 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_33.929 V_27.685 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_38.039 V_27.196 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_37.616 V_26.902 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_39.075 V_26.465 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_31.942 V_26.636 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_40.788 V_27.112 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_40.320 V_27.038 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_40.828 V_26.747 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_33.577 V_25.974 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_38.876 V_26.142 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_38.485 V_26.777 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_38.207 V_27.143 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_35.234 V_27.083 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_43.131 V_26.950 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_35.166 V_26.712 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_38.223 V_26.622 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_38.731 V_26.462 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_36.702 V_26.371 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_38.943 V_26.695 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_31.536 V_27.147 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_35.082 V_26.835 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_32.247 V_26.311 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_40.795 V_26.278 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_36.173 V_26.385 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_31.467 V_26.056 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_36.835 V_26.350 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_38.742 V_26.766 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_35.360 V_26.377 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_36.476 V_26.629 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_40.147 V_27.239 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_34.114 V_27.249 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_39.048 V_26.760 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_36.531 V_25.941 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_39.100 V_26.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_36.161 V_26.734 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_37.159 V_26.554 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_39.078 V_26.476 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_38.216 V_26.099 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_41.243 V_26.327 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_35.069 V_26.708 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_36.696 V_27.078 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_41.078 V_26.762 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_37.154 V_26.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_34.375 V_26.401 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_47.744 V_26.668 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_40.786 V_26.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_38.737 V_26.285 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_36.031 V_26.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_34.998 V_26.659 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_36.529 V_26.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_28.658 V_26.402 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_33.794 V_26.630 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_36.682 V_26.636 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_34.639 V_25.914 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_33.353 V_26.253 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_34.906 V_25.996 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_33.598 V_26.953 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_36.661 V_26.292 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_31.147 V_27.121 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_35.993 V_26.653 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_33.554 V_26.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_33.993 V_26.986 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_43.078 V_26.666 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_39.950 V_26.442 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_39.154 V_26.336 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_35.571 V_26.307 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_36.212 V_25.882 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_43.437 V_26.492 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_36.576 V_26.831 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_42.392 V_26.119 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_43.389 V_26.358 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_45.274 V_26.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_35.720 V_25.866 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_37.581 V_26.766 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_37.725 V_27.073 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_36.938 V_26.572 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_34.305 V_26.159 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_34.021 V_26.839 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  600.5738525390625\n",
      "Epoch 010: : Loss: T_653.709 V_600.574 | Acc: T_0.000) V_0.000\n",
      "Epoch 020: : Loss: T_650.142 V_602.959 | Acc: T_0.000) V_0.000\n",
      "Epoch 030: : Loss: T_645.639 V_604.223 | Acc: T_0.000) V_0.000\n",
      "Epoch 040: : Loss: T_641.026 V_603.177 | Acc: T_0.000) V_0.000\n",
      "Epoch 050: : Loss: T_637.318 V_601.530 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  599.8265380859375\n",
      "Epoch 060: : Loss: T_632.759 V_599.827 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  596.7196044921875\n",
      "Epoch 070: : Loss: T_620.990 V_596.720 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  591.940673828125\n",
      "Epoch 080: : Loss: T_625.323 V_591.941 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  588.236083984375\n",
      "Epoch 090: : Loss: T_614.660 V_588.236 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  585.0657958984375\n",
      "Epoch 100: : Loss: T_618.966 V_585.066 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  580.0615234375\n",
      "Epoch 110: : Loss: T_601.435 V_580.062 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  575.248779296875\n",
      "Epoch 120: : Loss: T_590.870 V_575.249 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  566.0070190429688\n",
      "Epoch 130: : Loss: T_584.817 V_566.007 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  557.3713989257812\n",
      "Epoch 140: : Loss: T_584.984 V_557.371 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  552.1744384765625\n",
      "Epoch 150: : Loss: T_573.769 V_552.174 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  552.0707397460938\n",
      "Epoch 160: : Loss: T_566.741 V_552.071 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  542.8489379882812\n",
      "Epoch 170: : Loss: T_556.050 V_542.849 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  535.094970703125\n",
      "Epoch 180: : Loss: T_549.604 V_535.095 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  526.5213012695312\n",
      "Epoch 190: : Loss: T_534.319 V_526.521 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  517.6426391601562\n",
      "Epoch 200: : Loss: T_532.608 V_517.643 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  514.4055786132812\n",
      "Epoch 210: : Loss: T_526.552 V_514.406 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  508.9045715332031\n",
      "Epoch 220: : Loss: T_513.018 V_508.905 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  499.3300476074219\n",
      "Epoch 230: : Loss: T_508.162 V_499.330 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  497.9636535644531\n",
      "Epoch 240: : Loss: T_498.812 V_497.964 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  489.82476806640625\n",
      "Epoch 250: : Loss: T_489.482 V_489.825 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  478.54888916015625\n",
      "Epoch 260: : Loss: T_498.037 V_478.549 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  472.4183654785156\n",
      "Epoch 270: : Loss: T_477.283 V_472.418 | Acc: T_0.000) V_0.000\n",
      "Epoch 280: : Loss: T_462.445 V_476.602 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  470.2624816894531\n",
      "Epoch 290: : Loss: T_449.891 V_470.262 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  449.3808288574219\n",
      "Epoch 300: : Loss: T_455.166 V_449.381 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  420.8363342285156\n",
      "Epoch 310: : Loss: T_440.959 V_420.836 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  407.8122253417969\n",
      "Epoch 320: : Loss: T_443.091 V_407.812 | Acc: T_0.000) V_0.000\n",
      "Epoch 330: : Loss: T_417.544 V_414.754 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  401.320556640625\n",
      "Epoch 340: : Loss: T_417.755 V_401.321 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  394.2309875488281\n",
      "Epoch 350: : Loss: T_409.021 V_394.231 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  388.8375549316406\n",
      "Epoch 360: : Loss: T_387.615 V_388.838 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  374.6827392578125\n",
      "Epoch 370: : Loss: T_394.747 V_374.683 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  361.52203369140625\n",
      "Epoch 380: : Loss: T_385.035 V_361.522 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  355.3927917480469\n",
      "Epoch 390: : Loss: T_393.806 V_355.393 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  352.74603271484375\n",
      "Epoch 400: : Loss: T_358.418 V_352.746 | Acc: T_0.000) V_0.000\n",
      "Epoch 410: : Loss: T_370.526 V_355.138 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_349.625 V_356.236 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  341.1949462890625\n",
      "Epoch 430: : Loss: T_339.961 V_341.195 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  326.21136474609375\n",
      "Epoch 440: : Loss: T_330.602 V_326.211 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  317.65814208984375\n",
      "Epoch 450: : Loss: T_322.298 V_317.658 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  305.4547119140625\n",
      "Epoch 460: : Loss: T_319.052 V_305.455 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  294.2392578125\n",
      "Epoch 470: : Loss: T_302.135 V_294.239 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  289.2859191894531\n",
      "Epoch 480: : Loss: T_308.806 V_289.286 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  285.0688781738281\n",
      "Epoch 490: : Loss: T_306.714 V_285.069 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  278.8291015625\n",
      "Epoch 500: : Loss: T_288.124 V_278.829 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  272.9167785644531\n",
      "Epoch 510: : Loss: T_289.874 V_272.917 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  263.03326416015625\n",
      "Epoch 520: : Loss: T_276.758 V_263.033 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  255.4186248779297\n",
      "Epoch 530: : Loss: T_269.398 V_255.419 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  244.71844482421875\n",
      "Epoch 540: : Loss: T_258.563 V_244.718 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  233.55477905273438\n",
      "Epoch 550: : Loss: T_242.352 V_233.555 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  231.83111572265625\n",
      "Epoch 560: : Loss: T_240.085 V_231.831 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  227.230712890625\n",
      "Epoch 570: : Loss: T_237.062 V_227.231 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  217.29808044433594\n",
      "Epoch 580: : Loss: T_228.382 V_217.298 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  206.504638671875\n",
      "Epoch 590: : Loss: T_218.046 V_206.505 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  198.6736297607422\n",
      "Epoch 600: : Loss: T_222.330 V_198.674 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  193.42628479003906\n",
      "Epoch 610: : Loss: T_208.219 V_193.426 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  190.6536865234375\n",
      "Epoch 620: : Loss: T_195.070 V_190.654 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  179.56423950195312\n",
      "Epoch 630: : Loss: T_185.362 V_179.564 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  171.06259155273438\n",
      "Epoch 640: : Loss: T_191.851 V_171.063 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  166.3791046142578\n",
      "Epoch 650: : Loss: T_166.064 V_166.379 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  162.01084899902344\n",
      "Epoch 660: : Loss: T_172.533 V_162.011 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  152.0033721923828\n",
      "Epoch 670: : Loss: T_157.498 V_152.003 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  141.52243041992188\n",
      "Epoch 680: : Loss: T_156.728 V_141.522 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  134.44407653808594\n",
      "Epoch 690: : Loss: T_147.688 V_134.444 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  128.46022033691406\n",
      "Epoch 700: : Loss: T_145.594 V_128.460 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  125.22333526611328\n",
      "Epoch 710: : Loss: T_152.984 V_125.223 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  122.30020141601562\n",
      "Epoch 720: : Loss: T_136.234 V_122.300 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  117.5330581665039\n",
      "Epoch 730: : Loss: T_130.441 V_117.533 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  112.68236541748047\n",
      "Epoch 740: : Loss: T_127.682 V_112.682 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  105.44068145751953\n",
      "Epoch 750: : Loss: T_123.334 V_105.441 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  101.9192123413086\n",
      "Epoch 760: : Loss: T_109.445 V_101.919 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  95.4638900756836\n",
      "Epoch 770: : Loss: T_100.801 V_95.464 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  91.63168334960938\n",
      "Epoch 780: : Loss: T_100.516 V_91.632 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  90.29924011230469\n",
      "Epoch 790: : Loss: T_90.054 V_90.299 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  86.80259704589844\n",
      "Epoch 800: : Loss: T_89.806 V_86.803 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  82.01663970947266\n",
      "Epoch 810: : Loss: T_86.534 V_82.017 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  77.54090881347656\n",
      "Epoch 820: : Loss: T_80.775 V_77.541 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  74.34078979492188\n",
      "Epoch 830: : Loss: T_84.343 V_74.341 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  70.94564056396484\n",
      "Epoch 840: : Loss: T_89.004 V_70.946 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  68.29071807861328\n",
      "Epoch 850: : Loss: T_95.397 V_68.291 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  67.26976776123047\n",
      "Epoch 860: : Loss: T_82.450 V_67.270 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  65.57649993896484\n",
      "Epoch 870: : Loss: T_73.350 V_65.576 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  62.595638275146484\n",
      "Epoch 880: : Loss: T_79.012 V_62.596 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  57.39119338989258\n",
      "Epoch 890: : Loss: T_74.797 V_57.391 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  55.59232711791992\n",
      "Epoch 900: : Loss: T_66.740 V_55.592 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  54.21785354614258\n",
      "Epoch 910: : Loss: T_71.245 V_54.218 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  52.670692443847656\n",
      "Epoch 920: : Loss: T_69.323 V_52.671 | Acc: T_0.000) V_0.000\n",
      "Epoch 930: : Loss: T_70.267 V_53.583 | Acc: T_0.000) V_0.000\n",
      "Epoch 940: : Loss: T_62.251 V_54.307 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  51.36152648925781\n",
      "Epoch 950: : Loss: T_63.044 V_51.362 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  49.9975471496582\n",
      "Epoch 960: : Loss: T_61.120 V_49.998 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  47.05524826049805\n",
      "Epoch 970: : Loss: T_61.202 V_47.055 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  45.23724365234375\n",
      "Epoch 980: : Loss: T_56.815 V_45.237 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  44.15897750854492\n",
      "Epoch 990: : Loss: T_61.660 V_44.159 | Acc: T_0.000) V_0.000\n",
      "Epoch 1000: : Loss: T_49.530 V_44.164 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.89478302001953\n",
      "Epoch 1010: : Loss: T_67.451 V_42.895 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.66352844238281\n",
      "Epoch 1020: : Loss: T_63.555 V_42.664 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  41.2488899230957\n",
      "Epoch 1030: : Loss: T_64.711 V_41.249 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.94505310058594\n",
      "Epoch 1040: : Loss: T_55.548 V_39.945 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.5233039855957\n",
      "Epoch 1050: : Loss: T_65.267 V_39.523 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.357669830322266\n",
      "Epoch 1060: : Loss: T_60.586 V_39.358 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.804664611816406\n",
      "Epoch 1070: : Loss: T_52.352 V_38.805 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.66657638549805\n",
      "Epoch 1080: : Loss: T_56.853 V_37.667 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.07503890991211\n",
      "Epoch 1090: : Loss: T_44.408 V_37.075 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.07007598876953\n",
      "Epoch 1100: : Loss: T_54.052 V_37.070 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.88867950439453\n",
      "Epoch 1110: : Loss: T_58.614 V_36.889 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_54.152 V_36.922 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.79832077026367\n",
      "Epoch 1130: : Loss: T_58.884 V_35.798 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_44.514 V_36.090 | Acc: T_0.000) V_0.000\n",
      "Epoch 1150: : Loss: T_52.932 V_36.652 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_57.166 V_36.019 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.65768051147461\n",
      "Epoch 1170: : Loss: T_47.980 V_34.658 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.22392654418945\n",
      "Epoch 1180: : Loss: T_56.435 V_34.224 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.7210807800293\n",
      "Epoch 1190: : Loss: T_51.243 V_33.721 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_53.078 V_33.986 | Acc: T_0.000) V_0.000\n",
      "Epoch 1210: : Loss: T_52.423 V_34.128 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.28792953491211\n",
      "Epoch 1220: : Loss: T_60.219 V_33.288 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.227115631103516\n",
      "Epoch 1230: : Loss: T_46.503 V_33.227 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.04060745239258\n",
      "Epoch 1240: : Loss: T_51.464 V_33.041 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_51.513 V_33.297 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.87287902832031\n",
      "Epoch 1260: : Loss: T_61.108 V_32.873 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.38817596435547\n",
      "Epoch 1270: : Loss: T_55.790 V_32.388 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_45.825 V_32.843 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_47.707 V_32.887 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_54.710 V_32.677 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.32187271118164\n",
      "Epoch 1310: : Loss: T_46.191 V_32.322 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_53.779 V_32.955 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_40.857 V_33.103 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_52.122 V_32.863 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_52.590 V_33.077 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_44.276 V_32.929 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_45.509 V_33.327 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_50.841 V_33.196 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_46.363 V_33.520 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_49.014 V_34.133 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_49.070 V_33.676 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_47.801 V_33.492 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_52.799 V_32.855 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_45.166 V_32.861 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.1153564453125\n",
      "Epoch 1450: : Loss: T_43.653 V_32.115 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.78429412841797\n",
      "Epoch 1460: : Loss: T_44.154 V_31.784 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_39.153 V_32.008 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_51.494 V_32.243 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_52.437 V_32.263 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_44.315 V_32.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_55.563 V_32.770 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_46.903 V_32.729 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_51.093 V_32.813 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_42.644 V_32.560 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_56.306 V_32.535 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_55.865 V_32.352 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_57.666 V_32.044 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_45.696 V_32.241 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_49.519 V_32.531 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_52.167 V_33.041 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_60.775 V_33.133 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_51.160 V_33.176 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_50.262 V_33.485 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_43.158 V_33.397 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_40.040 V_33.040 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_49.307 V_33.024 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_64.698 V_33.183 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_46.098 V_32.055 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_50.455 V_32.287 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_54.051 V_32.339 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_53.544 V_32.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_46.015 V_32.798 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_52.085 V_33.449 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_56.119 V_33.866 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_54.493 V_33.862 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_47.487 V_33.990 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_57.751 V_33.740 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_57.103 V_33.018 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_51.048 V_32.301 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_48.595 V_32.490 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_51.407 V_32.146 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_49.954 V_32.078 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_51.057 V_32.223 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_41.384 V_32.307 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_41.037 V_32.462 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_47.179 V_32.936 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_50.693 V_32.978 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_59.232 V_32.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_51.192 V_32.544 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_47.345 V_32.892 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_48.095 V_33.206 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_46.249 V_33.343 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_51.698 V_33.548 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_51.737 V_33.753 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_54.908 V_33.700 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_48.371 V_33.201 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_43.961 V_33.045 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_47.404 V_32.649 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_47.858 V_32.238 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_41.865 V_32.967 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_48.452 V_33.129 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_45.952 V_32.563 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_55.937 V_32.698 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_52.715 V_32.076 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.66095542907715\n",
      "Epoch 2050: : Loss: T_47.288 V_31.661 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_49.405 V_32.568 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_45.338 V_33.196 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_51.949 V_33.071 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_52.955 V_33.068 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_55.609 V_32.775 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_56.250 V_32.460 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_50.728 V_33.166 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_45.884 V_33.487 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_48.035 V_33.270 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_51.920 V_33.145 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_47.727 V_33.244 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_51.891 V_33.056 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_58.634 V_32.952 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_55.726 V_32.710 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_49.591 V_32.487 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_44.670 V_32.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_39.339 V_32.498 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_51.094 V_32.321 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_45.681 V_32.389 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_50.361 V_32.609 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_48.170 V_32.275 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_48.251 V_32.478 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_44.805 V_32.189 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_46.010 V_32.173 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_43.861 V_31.984 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_54.286 V_32.249 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_50.009 V_32.294 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_54.950 V_32.102 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_48.205 V_32.584 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_41.934 V_31.854 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_48.228 V_31.787 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_49.112 V_31.723 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_41.272 V_32.034 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_57.109 V_31.801 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_53.774 V_32.795 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_50.057 V_32.814 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_49.835 V_32.434 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_48.365 V_32.953 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_50.860 V_32.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_44.530 V_31.791 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.65485382080078\n",
      "Epoch 2460: : Loss: T_50.670 V_31.655 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_53.922 V_32.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_48.471 V_32.927 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_52.929 V_32.843 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_48.149 V_32.687 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_45.760 V_32.484 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_46.163 V_32.866 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_50.231 V_32.455 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_41.747 V_32.241 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_44.148 V_32.603 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_47.738 V_33.249 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_43.469 V_33.064 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_46.550 V_33.375 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_45.086 V_32.363 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_45.325 V_32.608 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_51.861 V_32.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_49.630 V_32.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_53.171 V_32.711 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_55.287 V_32.561 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.50341796875\n",
      "Epoch 2650: : Loss: T_42.821 V_31.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_40.696 V_31.662 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_45.265 V_32.531 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_50.140 V_32.315 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_52.350 V_32.764 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_53.500 V_32.389 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_44.643 V_32.116 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_47.377 V_32.131 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_43.649 V_31.940 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_48.886 V_32.396 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_38.269 V_32.525 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_46.650 V_32.657 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_47.143 V_32.571 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_45.703 V_32.875 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_40.842 V_32.149 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_46.651 V_32.824 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_47.142 V_33.100 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_52.030 V_33.279 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_49.555 V_33.815 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_44.151 V_33.188 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_56.417 V_33.224 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_40.210 V_33.951 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_48.537 V_34.425 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_44.554 V_34.046 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_39.190 V_33.735 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_49.173 V_33.499 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_41.100 V_33.079 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_50.594 V_32.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_48.315 V_32.315 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_45.192 V_32.905 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_48.222 V_32.771 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_47.069 V_32.667 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_45.149 V_32.814 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_41.890 V_32.692 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_51.878 V_32.871 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_51.428 V_32.874 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_47.464 V_32.866 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_44.698 V_33.071 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_45.679 V_33.212 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_50.989 V_33.275 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_51.310 V_33.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_44.847 V_33.026 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_49.779 V_33.127 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_44.890 V_33.255 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_47.006 V_33.270 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_51.844 V_33.522 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_46.263 V_33.212 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_43.163 V_33.209 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_45.124 V_33.175 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_45.792 V_32.939 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_46.378 V_32.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_45.266 V_32.711 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_44.266 V_33.183 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_52.631 V_33.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_46.156 V_33.500 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_40.630 V_32.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_57.362 V_32.697 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_46.112 V_33.293 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_45.693 V_33.679 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_52.850 V_33.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_54.452 V_33.465 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_43.025 V_33.099 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_37.818 V_32.741 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_41.121 V_33.604 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_46.365 V_33.564 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_44.571 V_33.928 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_45.740 V_33.837 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_53.441 V_33.227 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_49.957 V_32.332 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_43.657 V_32.663 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_42.984 V_32.778 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_52.976 V_33.049 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_45.102 V_33.067 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_43.739 V_33.495 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_44.684 V_33.723 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_48.048 V_33.119 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_48.586 V_32.336 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_45.649 V_32.674 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_39.873 V_33.560 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_50.404 V_33.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_43.214 V_33.457 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_46.817 V_32.974 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_46.164 V_32.838 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_40.385 V_33.187 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_53.970 V_32.971 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_40.123 V_33.123 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  590.8682861328125\n",
      "Epoch 010: : Loss: T_615.541 V_590.868 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  590.3665161132812\n",
      "Epoch 020: : Loss: T_611.488 V_590.367 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  588.0640258789062\n",
      "Epoch 030: : Loss: T_607.310 V_588.064 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  585.11669921875\n",
      "Epoch 040: : Loss: T_599.651 V_585.117 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  580.163330078125\n",
      "Epoch 050: : Loss: T_602.303 V_580.163 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  573.266845703125\n",
      "Epoch 060: : Loss: T_591.039 V_573.267 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  566.9195556640625\n",
      "Epoch 070: : Loss: T_587.027 V_566.920 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  560.1431274414062\n",
      "Epoch 080: : Loss: T_586.056 V_560.143 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  553.3721313476562\n",
      "Epoch 090: : Loss: T_578.539 V_553.372 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  549.1159057617188\n",
      "Epoch 100: : Loss: T_565.528 V_549.116 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  541.645263671875\n",
      "Epoch 110: : Loss: T_562.922 V_541.645 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  535.818603515625\n",
      "Epoch 120: : Loss: T_554.395 V_535.819 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  525.2920532226562\n",
      "Epoch 130: : Loss: T_548.775 V_525.292 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  516.9916381835938\n",
      "Epoch 140: : Loss: T_540.797 V_516.992 | Acc: T_0.000) V_0.000\n",
      "Epoch 150: : Loss: T_532.890 V_525.406 | Acc: T_0.000) V_0.000\n",
      "Epoch 160: : Loss: T_520.237 V_520.175 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  509.2587890625\n",
      "Epoch 170: : Loss: T_519.470 V_509.259 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  508.92303466796875\n",
      "Epoch 180: : Loss: T_514.950 V_508.923 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  506.5837097167969\n",
      "Epoch 190: : Loss: T_512.908 V_506.584 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  495.6453552246094\n",
      "Epoch 200: : Loss: T_506.149 V_495.645 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  491.38494873046875\n",
      "Epoch 210: : Loss: T_489.741 V_491.385 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  476.38531494140625\n",
      "Epoch 220: : Loss: T_486.280 V_476.385 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  475.848388671875\n",
      "Epoch 230: : Loss: T_477.914 V_475.848 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  468.49066162109375\n",
      "Epoch 240: : Loss: T_474.362 V_468.491 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  458.9282531738281\n",
      "Epoch 250: : Loss: T_472.770 V_458.928 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  449.4173583984375\n",
      "Epoch 260: : Loss: T_454.437 V_449.417 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  446.3670959472656\n",
      "Epoch 270: : Loss: T_448.413 V_446.367 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  439.52923583984375\n",
      "Epoch 280: : Loss: T_445.162 V_439.529 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  434.88653564453125\n",
      "Epoch 290: : Loss: T_434.450 V_434.887 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  426.0028991699219\n",
      "Epoch 300: : Loss: T_428.701 V_426.003 | Acc: T_0.000) V_0.000\n",
      "Epoch 310: : Loss: T_418.029 V_427.291 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  415.2600402832031\n",
      "Epoch 320: : Loss: T_400.665 V_415.260 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  407.72503662109375\n",
      "Epoch 330: : Loss: T_394.253 V_407.725 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  399.5540771484375\n",
      "Epoch 340: : Loss: T_386.745 V_399.554 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  396.6122741699219\n",
      "Epoch 350: : Loss: T_381.958 V_396.612 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  390.1404113769531\n",
      "Epoch 360: : Loss: T_391.505 V_390.140 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  376.39984130859375\n",
      "Epoch 370: : Loss: T_361.353 V_376.400 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  362.5446472167969\n",
      "Epoch 380: : Loss: T_374.161 V_362.545 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  347.18243408203125\n",
      "Epoch 390: : Loss: T_356.885 V_347.182 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  340.2439270019531\n",
      "Epoch 400: : Loss: T_344.290 V_340.244 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  333.5600280761719\n",
      "Epoch 410: : Loss: T_341.363 V_333.560 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_332.043 V_334.245 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  319.9283752441406\n",
      "Epoch 430: : Loss: T_333.576 V_319.928 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  312.147216796875\n",
      "Epoch 440: : Loss: T_321.265 V_312.147 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  309.9833068847656\n",
      "Epoch 450: : Loss: T_302.647 V_309.983 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  297.6654357910156\n",
      "Epoch 460: : Loss: T_298.045 V_297.665 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  291.37066650390625\n",
      "Epoch 470: : Loss: T_282.909 V_291.371 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  283.1362609863281\n",
      "Epoch 480: : Loss: T_287.894 V_283.136 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  277.7620544433594\n",
      "Epoch 490: : Loss: T_280.774 V_277.762 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  267.75775146484375\n",
      "Epoch 500: : Loss: T_270.611 V_267.758 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  251.16693115234375\n",
      "Epoch 510: : Loss: T_262.774 V_251.167 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  247.69850158691406\n",
      "Epoch 520: : Loss: T_250.684 V_247.699 | Acc: T_0.000) V_0.000\n",
      "Epoch 530: : Loss: T_251.686 V_249.359 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  245.2377471923828\n",
      "Epoch 540: : Loss: T_244.563 V_245.238 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  231.65750122070312\n",
      "Epoch 550: : Loss: T_231.594 V_231.658 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  222.4088134765625\n",
      "Epoch 560: : Loss: T_222.442 V_222.409 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  212.67889404296875\n",
      "Epoch 570: : Loss: T_223.678 V_212.679 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  202.19607543945312\n",
      "Epoch 580: : Loss: T_213.810 V_202.196 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  193.59812927246094\n",
      "Epoch 590: : Loss: T_198.350 V_193.598 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  191.2354278564453\n",
      "Epoch 600: : Loss: T_192.361 V_191.235 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  186.2254180908203\n",
      "Epoch 610: : Loss: T_190.779 V_186.225 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  177.6560821533203\n",
      "Epoch 620: : Loss: T_169.170 V_177.656 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  167.28660583496094\n",
      "Epoch 630: : Loss: T_161.079 V_167.287 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  158.48696899414062\n",
      "Epoch 640: : Loss: T_169.451 V_158.487 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  153.9256591796875\n",
      "Epoch 650: : Loss: T_151.143 V_153.926 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  145.7976531982422\n",
      "Epoch 660: : Loss: T_168.634 V_145.798 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  137.80381774902344\n",
      "Epoch 670: : Loss: T_149.137 V_137.804 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  133.31655883789062\n",
      "Epoch 680: : Loss: T_142.997 V_133.317 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  125.84723663330078\n",
      "Epoch 690: : Loss: T_135.658 V_125.847 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  122.94032287597656\n",
      "Epoch 700: : Loss: T_140.560 V_122.940 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  119.340087890625\n",
      "Epoch 710: : Loss: T_129.868 V_119.340 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  113.77896118164062\n",
      "Epoch 720: : Loss: T_120.138 V_113.779 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  106.52127075195312\n",
      "Epoch 730: : Loss: T_113.027 V_106.521 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  100.88793182373047\n",
      "Epoch 740: : Loss: T_103.704 V_100.888 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  95.90328216552734\n",
      "Epoch 750: : Loss: T_110.388 V_95.903 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  91.39095306396484\n",
      "Epoch 760: : Loss: T_105.941 V_91.391 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  89.43681335449219\n",
      "Epoch 770: : Loss: T_94.303 V_89.437 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  85.14027404785156\n",
      "Epoch 780: : Loss: T_100.574 V_85.140 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  79.91893768310547\n",
      "Epoch 790: : Loss: T_93.262 V_79.919 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  74.26443481445312\n",
      "Epoch 800: : Loss: T_90.488 V_74.264 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  70.22908020019531\n",
      "Epoch 810: : Loss: T_84.607 V_70.229 | Acc: T_0.000) V_0.000\n",
      "Epoch 820: : Loss: T_73.172 V_70.850 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  68.5226058959961\n",
      "Epoch 830: : Loss: T_79.393 V_68.523 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  64.90728759765625\n",
      "Epoch 840: : Loss: T_75.248 V_64.907 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  62.67057418823242\n",
      "Epoch 850: : Loss: T_76.965 V_62.671 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  58.70738220214844\n",
      "Epoch 860: : Loss: T_75.770 V_58.707 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  58.65644454956055\n",
      "Epoch 870: : Loss: T_80.212 V_58.656 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  56.81898498535156\n",
      "Epoch 880: : Loss: T_78.797 V_56.819 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  53.557701110839844\n",
      "Epoch 890: : Loss: T_64.850 V_53.558 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  52.16859436035156\n",
      "Epoch 900: : Loss: T_74.111 V_52.169 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.09898376464844\n",
      "Epoch 910: : Loss: T_69.593 V_48.099 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  45.99256896972656\n",
      "Epoch 920: : Loss: T_67.271 V_45.993 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  45.09056854248047\n",
      "Epoch 930: : Loss: T_56.997 V_45.091 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  43.62802505493164\n",
      "Epoch 940: : Loss: T_64.049 V_43.628 | Acc: T_0.000) V_0.000\n",
      "Epoch 950: : Loss: T_51.021 V_43.832 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  41.59199142456055\n",
      "Epoch 960: : Loss: T_64.598 V_41.592 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.55735397338867\n",
      "Epoch 970: : Loss: T_58.487 V_39.557 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.58582305908203\n",
      "Epoch 980: : Loss: T_54.477 V_38.586 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.73922348022461\n",
      "Epoch 990: : Loss: T_53.685 V_37.739 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.077880859375\n",
      "Epoch 1000: : Loss: T_49.974 V_36.078 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.27632522583008\n",
      "Epoch 1010: : Loss: T_59.438 V_35.276 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.22807693481445\n",
      "Epoch 1020: : Loss: T_48.717 V_35.228 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.81263732910156\n",
      "Epoch 1030: : Loss: T_52.394 V_34.813 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.30416488647461\n",
      "Epoch 1040: : Loss: T_54.383 V_34.304 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.551509857177734\n",
      "Epoch 1050: : Loss: T_57.043 V_33.552 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.30656433105469\n",
      "Epoch 1060: : Loss: T_50.109 V_32.307 | Acc: T_0.000) V_0.000\n",
      "Epoch 1070: : Loss: T_53.144 V_32.326 | Acc: T_0.000) V_0.000\n",
      "Epoch 1080: : Loss: T_52.395 V_32.854 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.35306167602539\n",
      "Epoch 1090: : Loss: T_51.210 V_31.353 | Acc: T_0.000) V_0.000\n",
      "Epoch 1100: : Loss: T_48.099 V_32.055 | Acc: T_0.000) V_0.000\n",
      "Epoch 1110: : Loss: T_58.055 V_32.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_49.372 V_31.446 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.92702293395996\n",
      "Epoch 1130: : Loss: T_48.221 V_30.927 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.82560920715332\n",
      "Epoch 1140: : Loss: T_53.605 V_30.826 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.01082420349121\n",
      "Epoch 1150: : Loss: T_54.197 V_30.011 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.09247589111328\n",
      "Epoch 1160: : Loss: T_48.366 V_29.092 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.40374183654785\n",
      "Epoch 1170: : Loss: T_49.033 V_28.404 | Acc: T_0.000) V_0.000\n",
      "Epoch 1180: : Loss: T_53.672 V_28.673 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_59.844 V_29.854 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_47.309 V_30.166 | Acc: T_0.000) V_0.000\n",
      "Epoch 1210: : Loss: T_49.643 V_29.174 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.749553680419922\n",
      "Epoch 1220: : Loss: T_54.821 V_27.750 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_52.578 V_27.846 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.692171096801758\n",
      "Epoch 1240: : Loss: T_49.375 V_27.692 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_55.471 V_28.240 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_59.063 V_28.142 | Acc: T_0.000) V_0.000\n",
      "Epoch 1270: : Loss: T_51.535 V_28.165 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_48.039 V_27.934 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.538583755493164\n",
      "Epoch 1290: : Loss: T_57.930 V_27.539 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.886266708374023\n",
      "Epoch 1300: : Loss: T_52.903 V_26.886 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.72765350341797\n",
      "Epoch 1310: : Loss: T_58.693 V_26.728 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.68820571899414\n",
      "Epoch 1320: : Loss: T_52.838 V_26.688 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.464561462402344\n",
      "Epoch 1330: : Loss: T_49.198 V_26.465 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.203472137451172\n",
      "Epoch 1340: : Loss: T_38.700 V_26.203 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_42.023 V_26.326 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_42.439 V_26.941 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_53.522 V_27.249 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_58.162 V_27.336 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_43.014 V_27.677 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_43.113 V_27.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_52.591 V_27.808 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_49.921 V_27.621 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_43.325 V_26.747 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_51.242 V_26.920 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_46.401 V_26.284 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_43.027 V_26.764 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_50.684 V_26.579 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_51.657 V_26.530 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_52.562 V_26.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_58.297 V_26.250 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_43.383 V_26.589 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_51.691 V_26.973 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_47.856 V_26.671 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.748254776000977\n",
      "Epoch 1540: : Loss: T_45.519 V_25.748 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_44.243 V_25.864 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_52.166 V_26.446 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_46.393 V_26.501 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_46.755 V_26.506 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_50.446 V_26.644 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_49.715 V_26.673 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_51.154 V_27.041 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_43.514 V_27.571 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_55.474 V_27.390 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_46.341 V_28.175 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_42.066 V_28.164 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_47.645 V_26.944 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_58.836 V_26.588 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_57.282 V_27.649 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_57.962 V_27.168 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_52.248 V_27.133 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_51.872 V_27.269 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_42.432 V_26.997 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_43.316 V_27.059 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_42.942 V_27.302 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_43.752 V_27.280 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_45.321 V_26.712 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_47.448 V_26.507 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_49.548 V_26.376 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_46.628 V_26.372 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_51.934 V_26.962 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_43.652 V_27.084 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_47.361 V_27.276 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_40.491 V_27.544 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_51.649 V_27.844 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_50.878 V_27.125 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_43.965 V_27.361 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_46.161 V_27.306 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_44.085 V_27.754 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_44.195 V_27.583 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_41.707 V_27.525 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_51.615 V_27.069 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_45.005 V_27.303 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_50.339 V_27.208 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_44.276 V_26.786 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_42.935 V_26.672 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_45.899 V_26.716 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_44.125 V_27.034 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_48.918 V_27.071 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_48.573 V_27.331 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_43.656 V_27.429 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_50.472 V_26.635 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_50.234 V_26.958 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_35.923 V_27.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_50.182 V_27.903 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_48.841 V_28.238 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_36.062 V_27.225 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_50.326 V_26.707 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_49.282 V_26.953 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_46.370 V_27.076 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_41.704 V_26.263 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_52.915 V_26.490 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_46.523 V_26.381 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_51.749 V_26.258 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_45.262 V_27.322 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_40.873 V_27.484 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_51.098 V_27.162 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_57.361 V_25.960 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_49.269 V_26.293 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_47.061 V_27.249 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_39.389 V_27.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_50.238 V_27.116 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_46.304 V_26.804 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_51.528 V_27.333 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_45.491 V_27.188 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_44.674 V_26.876 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_53.409 V_26.729 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_44.558 V_26.715 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_51.223 V_27.271 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_48.927 V_28.005 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_47.437 V_27.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_44.176 V_27.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_38.432 V_27.732 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_52.521 V_27.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_43.660 V_26.902 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_50.872 V_26.543 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_50.673 V_27.360 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_42.487 V_27.581 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_46.393 V_27.144 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_41.492 V_27.707 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_47.441 V_28.064 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_58.045 V_27.656 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_41.920 V_27.872 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_44.646 V_28.094 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_48.150 V_27.691 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_34.998 V_27.502 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_52.970 V_26.787 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_46.212 V_26.045 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_48.519 V_25.998 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_46.418 V_26.254 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_35.804 V_26.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_41.294 V_27.576 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_46.447 V_28.109 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_39.261 V_28.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_44.403 V_27.566 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_39.991 V_27.556 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_45.299 V_28.206 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_42.038 V_27.219 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_53.471 V_27.854 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_38.043 V_27.013 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_40.514 V_26.582 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_49.185 V_26.614 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_45.953 V_26.689 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_38.549 V_27.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_47.515 V_27.108 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_54.488 V_27.178 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_45.319 V_26.898 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_45.283 V_26.990 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_51.626 V_27.804 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_49.628 V_27.557 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_42.612 V_28.221 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_42.984 V_28.110 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_40.495 V_28.548 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_45.067 V_28.969 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_41.421 V_28.681 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_37.826 V_28.034 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_41.156 V_27.843 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_43.298 V_27.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_42.615 V_26.724 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_44.927 V_26.824 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_47.750 V_26.866 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_40.870 V_27.056 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_47.764 V_27.621 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_55.909 V_27.617 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_43.997 V_26.186 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_48.680 V_26.056 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_47.342 V_26.655 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_47.514 V_27.159 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_40.811 V_26.447 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_37.393 V_26.556 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_48.356 V_26.446 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_42.127 V_26.999 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_51.677 V_27.711 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_43.399 V_27.833 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_45.443 V_27.856 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_40.524 V_28.143 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_52.448 V_28.042 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_46.731 V_27.601 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_42.816 V_27.500 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_50.768 V_26.879 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_47.278 V_26.814 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_45.476 V_26.806 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_41.471 V_27.475 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_43.611 V_29.230 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_45.501 V_29.257 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_48.072 V_28.583 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_37.357 V_27.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_43.487 V_27.543 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_39.397 V_28.219 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_39.264 V_28.316 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_40.068 V_29.240 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_41.775 V_29.353 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_46.524 V_27.763 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_44.410 V_27.365 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_48.057 V_27.167 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_50.148 V_26.774 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_50.351 V_27.295 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_40.009 V_27.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_41.546 V_27.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_56.556 V_27.272 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_45.461 V_27.155 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_41.128 V_27.548 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_46.451 V_27.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_48.377 V_27.811 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_40.834 V_27.906 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_42.365 V_28.400 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_46.456 V_28.469 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_43.649 V_28.179 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_40.147 V_28.155 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_41.102 V_27.231 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_46.814 V_26.521 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_44.013 V_26.318 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_49.904 V_27.364 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_45.057 V_28.746 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_40.819 V_28.280 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_47.337 V_27.531 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_39.157 V_26.668 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_47.937 V_27.118 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_46.632 V_27.215 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_38.748 V_26.838 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_44.326 V_27.063 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_50.534 V_28.406 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_47.374 V_28.590 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_40.523 V_27.641 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_41.673 V_27.630 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_48.467 V_28.410 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_36.571 V_27.816 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_38.895 V_27.529 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_37.539 V_28.074 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_41.221 V_28.211 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_46.432 V_27.495 | Acc: T_0.000) V_0.000\n"
     ]
    }
   ],
   "source": [
    "best_models = []\n",
    "for i in range(NUM_ENSEMBLE_MODELS):\n",
    "    model = BasicRegressor()\n",
    "    model.to(device)\n",
    "\n",
    "    # criterion = nn.L1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    bagg_indices = np.random.choice(range(len(x_train)), len(x_train), replace=True)\n",
    "\n",
    "    x_train_bagg = x_train[bagg_indices, :]\n",
    "    y_train_bagg = y_train[bagg_indices, :]\n",
    "    # train_data = TrainData(torch.FloatTensor(x_train), torch.FloatTensor(y_train))\n",
    "    train_data = TrainData(torch.FloatTensor(x_train_bagg), torch.FloatTensor(y_train_bagg))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=2048, shuffle=True)\n",
    "\n",
    "\n",
    "    num_train_data = len(train_loader)\n",
    "    num_eval_data = len(valid_loader)\n",
    "\n",
    "\n",
    "    elapsed_time_basic_ann = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    best_model = train_model(num_train_data, num_eval_data)\n",
    "\n",
    "    best_models.append(best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "sum_output = np.zeros(y_test.shape)\n",
    "\n",
    "for best_model in best_models:\n",
    "    best_model.eval()\n",
    "    output = best_model(data)\n",
    "    sum_output += output.cpu().detach().numpy()\n",
    "\n",
    "avg_output = sum_output / len(best_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Loss  7.076410788076895\n",
      "Normal Loss  2.062248190671312\n",
      "Total Loss  2.504674302207099\n"
     ]
    }
   ],
   "source": [
    "rare_loss, normal_loss, total_loss = calc_l1_loss_by_shots(avg_output, answer.cpu().detach().numpy())\n",
    "print(\"Rare Loss \", rare_loss)\n",
    "print(\"Normal Loss \", normal_loss)\n",
    "print(\"Total Loss \", total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble MLP with REBAGG (REsampling Bagging Method, 2018, P Branco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_value = threshold_rare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_indicies = np.where(y_train>threshold_rare)[0]\n",
    "normal_indicies = np.where(y_train<=threshold_rare)[0]\n",
    "\n",
    "ov_rare_indicies = np.random.choice(range(len(rare_indicies)), len(normal_indicies), replace=True)\n",
    "\n",
    "x_train_normal_bagg = x_train[normal_indicies, :]\n",
    "y_train_normal_bagg = y_train[normal_indicies, :]\n",
    "\n",
    "\n",
    "x_train_rare_bagg = x_train[ov_rare_indicies, :]\n",
    "y_train_rare_bagg = y_train[ov_rare_indicies, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model is copied - Best Loss :  586.3316040039062\n",
      "Epoch 010: : Loss: T_580.549 V_586.332 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  581.8206176757812\n",
      "Epoch 020: : Loss: T_576.885 V_581.821 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  579.318359375\n",
      "Epoch 030: : Loss: T_571.753 V_579.318 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  576.6486206054688\n",
      "Epoch 040: : Loss: T_570.049 V_576.649 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  573.351806640625\n",
      "Epoch 050: : Loss: T_568.845 V_573.352 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  569.99462890625\n",
      "Epoch 060: : Loss: T_560.911 V_569.995 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  568.5916137695312\n",
      "Epoch 070: : Loss: T_558.895 V_568.592 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  565.3417358398438\n",
      "Epoch 080: : Loss: T_554.017 V_565.342 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  560.4815063476562\n",
      "Epoch 090: : Loss: T_549.092 V_560.482 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  555.288818359375\n",
      "Epoch 100: : Loss: T_542.395 V_555.289 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  553.2428588867188\n",
      "Epoch 110: : Loss: T_541.372 V_553.243 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  548.712890625\n",
      "Epoch 120: : Loss: T_531.105 V_548.713 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  540.1207885742188\n",
      "Epoch 130: : Loss: T_522.530 V_540.121 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  536.086181640625\n",
      "Epoch 140: : Loss: T_518.696 V_536.086 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  529.8851318359375\n",
      "Epoch 150: : Loss: T_505.007 V_529.885 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  523.6303100585938\n",
      "Epoch 160: : Loss: T_504.592 V_523.630 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  516.8643798828125\n",
      "Epoch 170: : Loss: T_497.959 V_516.864 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  509.91241455078125\n",
      "Epoch 180: : Loss: T_491.901 V_509.912 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  500.645263671875\n",
      "Epoch 190: : Loss: T_486.753 V_500.645 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  490.02325439453125\n",
      "Epoch 200: : Loss: T_487.724 V_490.023 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  485.0808410644531\n",
      "Epoch 210: : Loss: T_476.330 V_485.081 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  482.19232177734375\n",
      "Epoch 220: : Loss: T_467.793 V_482.192 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  467.5444641113281\n",
      "Epoch 230: : Loss: T_456.993 V_467.544 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  455.5886535644531\n",
      "Epoch 240: : Loss: T_459.940 V_455.589 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  435.76214599609375\n",
      "Epoch 250: : Loss: T_447.149 V_435.762 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  431.6936340332031\n",
      "Epoch 260: : Loss: T_435.868 V_431.694 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  421.5939025878906\n",
      "Epoch 270: : Loss: T_424.459 V_421.594 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  416.92120361328125\n",
      "Epoch 280: : Loss: T_426.348 V_416.921 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  404.4917297363281\n",
      "Epoch 290: : Loss: T_413.505 V_404.492 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  401.5583190917969\n",
      "Epoch 300: : Loss: T_401.022 V_401.558 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  391.2225036621094\n",
      "Epoch 310: : Loss: T_402.314 V_391.223 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  386.0953063964844\n",
      "Epoch 320: : Loss: T_387.731 V_386.095 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  379.54736328125\n",
      "Epoch 330: : Loss: T_377.470 V_379.547 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  362.3460693359375\n",
      "Epoch 340: : Loss: T_365.045 V_362.346 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  353.3581237792969\n",
      "Epoch 350: : Loss: T_373.987 V_353.358 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  352.7066955566406\n",
      "Epoch 360: : Loss: T_362.262 V_352.707 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  340.79071044921875\n",
      "Epoch 370: : Loss: T_355.266 V_340.791 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  323.51904296875\n",
      "Epoch 380: : Loss: T_352.495 V_323.519 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  320.2068786621094\n",
      "Epoch 390: : Loss: T_328.262 V_320.207 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  307.19085693359375\n",
      "Epoch 400: : Loss: T_321.971 V_307.191 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  292.2124328613281\n",
      "Epoch 410: : Loss: T_308.522 V_292.212 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  284.89984130859375\n",
      "Epoch 420: : Loss: T_322.806 V_284.900 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_304.266 V_286.015 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  284.1120300292969\n",
      "Epoch 440: : Loss: T_291.861 V_284.112 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  270.7671813964844\n",
      "Epoch 450: : Loss: T_290.475 V_270.767 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  264.5193786621094\n",
      "Epoch 460: : Loss: T_283.463 V_264.519 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  253.58511352539062\n",
      "Epoch 470: : Loss: T_269.310 V_253.585 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  233.5491943359375\n",
      "Epoch 480: : Loss: T_258.701 V_233.549 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  229.26596069335938\n",
      "Epoch 490: : Loss: T_251.843 V_229.266 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  223.887451171875\n",
      "Epoch 500: : Loss: T_241.421 V_223.887 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  212.96595764160156\n",
      "Epoch 510: : Loss: T_237.320 V_212.966 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  199.72584533691406\n",
      "Epoch 520: : Loss: T_229.967 V_199.726 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  194.53506469726562\n",
      "Epoch 530: : Loss: T_233.131 V_194.535 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  192.63047790527344\n",
      "Epoch 540: : Loss: T_222.496 V_192.630 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  183.05406188964844\n",
      "Epoch 550: : Loss: T_219.637 V_183.054 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  175.36656188964844\n",
      "Epoch 560: : Loss: T_200.227 V_175.367 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  171.7296600341797\n",
      "Epoch 570: : Loss: T_186.178 V_171.730 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  165.15512084960938\n",
      "Epoch 580: : Loss: T_180.756 V_165.155 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  155.8050537109375\n",
      "Epoch 590: : Loss: T_182.762 V_155.805 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  155.3489532470703\n",
      "Epoch 600: : Loss: T_187.911 V_155.349 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  148.62388610839844\n",
      "Epoch 610: : Loss: T_173.281 V_148.624 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  140.5167999267578\n",
      "Epoch 620: : Loss: T_161.435 V_140.517 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  129.5304412841797\n",
      "Epoch 630: : Loss: T_153.641 V_129.530 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  120.45845794677734\n",
      "Epoch 640: : Loss: T_148.904 V_120.458 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  112.2752456665039\n",
      "Epoch 650: : Loss: T_148.967 V_112.275 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  109.38365936279297\n",
      "Epoch 660: : Loss: T_133.263 V_109.384 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  106.76437377929688\n",
      "Epoch 670: : Loss: T_134.475 V_106.764 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  105.24124908447266\n",
      "Epoch 680: : Loss: T_126.743 V_105.241 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  99.9762954711914\n",
      "Epoch 690: : Loss: T_120.222 V_99.976 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  93.62992095947266\n",
      "Epoch 700: : Loss: T_118.370 V_93.630 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  90.45320892333984\n",
      "Epoch 710: : Loss: T_126.178 V_90.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  87.45294189453125\n",
      "Epoch 720: : Loss: T_106.183 V_87.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  83.76336669921875\n",
      "Epoch 730: : Loss: T_94.720 V_83.763 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  80.4266128540039\n",
      "Epoch 740: : Loss: T_93.748 V_80.427 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  75.78450775146484\n",
      "Epoch 750: : Loss: T_107.261 V_75.785 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  71.135498046875\n",
      "Epoch 760: : Loss: T_105.640 V_71.135 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  68.57051849365234\n",
      "Epoch 770: : Loss: T_97.671 V_68.571 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  64.9066390991211\n",
      "Epoch 780: : Loss: T_85.987 V_64.907 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  61.992549896240234\n",
      "Epoch 790: : Loss: T_84.854 V_61.993 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  60.960445404052734\n",
      "Epoch 800: : Loss: T_86.322 V_60.960 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  57.82734298706055\n",
      "Epoch 810: : Loss: T_78.613 V_57.827 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  55.580692291259766\n",
      "Epoch 820: : Loss: T_68.603 V_55.581 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  52.449073791503906\n",
      "Epoch 830: : Loss: T_78.041 V_52.449 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  51.20119094848633\n",
      "Epoch 840: : Loss: T_72.743 V_51.201 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  49.8875617980957\n",
      "Epoch 850: : Loss: T_67.054 V_49.888 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  49.3360710144043\n",
      "Epoch 860: : Loss: T_69.359 V_49.336 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  46.86020278930664\n",
      "Epoch 870: : Loss: T_63.632 V_46.860 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  44.58799362182617\n",
      "Epoch 880: : Loss: T_67.316 V_44.588 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.61172103881836\n",
      "Epoch 890: : Loss: T_55.223 V_42.612 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  41.1222038269043\n",
      "Epoch 900: : Loss: T_66.361 V_41.122 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.6576042175293\n",
      "Epoch 910: : Loss: T_64.417 V_40.658 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.296844482421875\n",
      "Epoch 920: : Loss: T_61.263 V_39.297 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.893104553222656\n",
      "Epoch 930: : Loss: T_58.844 V_38.893 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.38398361206055\n",
      "Epoch 940: : Loss: T_53.516 V_38.384 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.650455474853516\n",
      "Epoch 950: : Loss: T_59.929 V_36.650 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.31900405883789\n",
      "Epoch 960: : Loss: T_56.339 V_35.319 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.61458206176758\n",
      "Epoch 970: : Loss: T_53.159 V_34.615 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.336570739746094\n",
      "Epoch 980: : Loss: T_53.033 V_33.337 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.94147300720215\n",
      "Epoch 990: : Loss: T_61.518 V_31.941 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.517244338989258\n",
      "Epoch 1000: : Loss: T_50.711 V_31.517 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.398630142211914\n",
      "Epoch 1010: : Loss: T_53.930 V_31.399 | Acc: T_0.000) V_0.000\n",
      "Epoch 1020: : Loss: T_54.150 V_31.528 | Acc: T_0.000) V_0.000\n",
      "Epoch 1030: : Loss: T_49.337 V_31.847 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.309118270874023\n",
      "Epoch 1040: : Loss: T_47.524 V_31.309 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.05599021911621\n",
      "Epoch 1050: : Loss: T_46.130 V_30.056 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.84259796142578\n",
      "Epoch 1060: : Loss: T_49.347 V_29.843 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.674924850463867\n",
      "Epoch 1070: : Loss: T_48.119 V_29.675 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.125545501708984\n",
      "Epoch 1080: : Loss: T_45.606 V_29.126 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.86112403869629\n",
      "Epoch 1090: : Loss: T_58.250 V_28.861 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.929489135742188\n",
      "Epoch 1100: : Loss: T_57.385 V_27.929 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.458478927612305\n",
      "Epoch 1110: : Loss: T_54.399 V_27.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_48.491 V_27.677 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_54.600 V_27.774 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_55.294 V_27.783 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.21360206604004\n",
      "Epoch 1150: : Loss: T_49.259 V_27.214 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.64392852783203\n",
      "Epoch 1160: : Loss: T_51.236 V_26.644 | Acc: T_0.000) V_0.000\n",
      "Epoch 1170: : Loss: T_44.864 V_27.179 | Acc: T_0.000) V_0.000\n",
      "Epoch 1180: : Loss: T_47.693 V_27.416 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_58.572 V_27.154 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_48.373 V_27.239 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.49884796142578\n",
      "Epoch 1210: : Loss: T_49.657 V_26.499 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.369216918945312\n",
      "Epoch 1220: : Loss: T_44.247 V_26.369 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.190719604492188\n",
      "Epoch 1230: : Loss: T_42.299 V_26.191 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_48.455 V_26.457 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.110694885253906\n",
      "Epoch 1250: : Loss: T_47.757 V_26.111 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_44.206 V_26.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 1270: : Loss: T_47.368 V_26.853 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_41.604 V_26.840 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_36.764 V_26.537 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.040292739868164\n",
      "Epoch 1300: : Loss: T_53.941 V_26.040 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.746889114379883\n",
      "Epoch 1310: : Loss: T_50.206 V_25.747 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_48.171 V_25.971 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_45.197 V_26.915 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_49.979 V_26.602 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_55.534 V_26.512 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_47.117 V_26.758 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_43.770 V_26.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_45.055 V_26.088 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.53841781616211\n",
      "Epoch 1390: : Loss: T_60.984 V_25.538 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_48.900 V_25.759 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_48.256 V_26.217 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.5278377532959\n",
      "Epoch 1420: : Loss: T_44.929 V_25.528 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.678966522216797\n",
      "Epoch 1430: : Loss: T_48.134 V_24.679 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.631031036376953\n",
      "Epoch 1440: : Loss: T_47.456 V_24.631 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.609006881713867\n",
      "Epoch 1450: : Loss: T_42.032 V_24.609 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_51.206 V_25.303 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_46.403 V_25.137 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_45.139 V_24.667 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_43.602 V_24.713 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_51.603 V_25.328 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_49.595 V_24.909 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.09259796142578\n",
      "Epoch 1520: : Loss: T_47.240 V_24.093 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_47.811 V_24.218 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_52.341 V_25.075 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_45.367 V_25.334 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_41.903 V_24.516 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_46.118 V_24.808 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_41.196 V_24.693 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_51.886 V_24.502 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_44.925 V_24.280 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_46.517 V_24.237 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_43.930 V_24.229 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_41.598 V_24.472 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_49.729 V_24.683 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_51.864 V_24.832 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_44.519 V_24.722 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_38.532 V_25.169 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_44.516 V_25.124 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_52.970 V_25.486 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_45.652 V_25.771 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_44.956 V_25.829 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_44.495 V_25.080 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_39.336 V_24.984 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_44.397 V_25.257 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_55.256 V_24.615 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_41.389 V_25.355 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_44.563 V_26.032 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_43.170 V_25.605 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_44.582 V_25.000 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_50.486 V_25.213 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_47.482 V_25.135 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_47.875 V_25.196 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_48.723 V_24.754 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_46.447 V_25.888 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_47.465 V_26.176 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_39.536 V_26.939 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_48.773 V_26.987 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_44.723 V_26.792 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_43.084 V_26.924 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_46.703 V_26.567 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_41.423 V_26.661 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_51.353 V_25.845 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_42.120 V_25.762 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_44.882 V_26.177 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_44.434 V_25.306 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_45.282 V_25.385 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_45.683 V_25.462 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_45.252 V_25.293 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_38.149 V_25.497 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_49.521 V_26.727 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_45.692 V_26.928 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_42.895 V_26.641 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_49.773 V_25.912 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_52.617 V_25.813 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_43.842 V_26.143 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_48.145 V_26.184 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_37.445 V_25.678 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_50.340 V_25.795 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_46.043 V_26.384 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_47.012 V_26.260 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_37.692 V_25.917 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_50.222 V_25.728 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_41.301 V_25.536 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_47.327 V_24.965 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_43.782 V_24.845 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_43.807 V_25.260 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_49.987 V_26.007 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_42.707 V_26.040 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_43.572 V_26.469 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_53.582 V_26.920 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_45.165 V_26.677 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_43.448 V_26.992 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_51.590 V_27.104 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_39.888 V_27.323 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_44.666 V_27.069 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_38.632 V_26.878 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_41.622 V_27.303 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_37.506 V_27.559 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_39.017 V_27.920 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_39.717 V_27.370 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_39.167 V_26.785 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_41.779 V_26.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_55.289 V_26.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_49.201 V_26.138 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_39.779 V_26.820 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_41.091 V_26.369 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_48.565 V_26.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_41.421 V_26.726 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_39.177 V_26.977 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_47.007 V_26.222 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_38.056 V_27.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_38.390 V_26.831 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_41.401 V_26.686 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_43.726 V_26.909 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_45.114 V_27.064 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_43.695 V_27.142 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_41.071 V_27.314 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_41.598 V_26.994 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_46.770 V_27.178 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_43.973 V_27.617 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_41.554 V_27.010 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_36.714 V_27.031 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_38.578 V_27.196 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_48.789 V_27.247 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_38.639 V_27.363 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_38.030 V_26.847 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_39.116 V_27.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_46.737 V_27.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_41.550 V_27.226 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_40.214 V_27.199 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_43.906 V_26.976 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_36.481 V_25.744 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_45.169 V_25.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_43.795 V_26.772 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_43.620 V_26.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_36.276 V_27.069 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_44.259 V_27.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_41.120 V_27.079 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_44.373 V_27.348 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_48.526 V_27.844 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_42.866 V_28.216 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_47.389 V_28.482 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_43.617 V_28.264 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_41.646 V_27.753 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_47.319 V_27.544 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_41.826 V_27.340 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_39.757 V_27.922 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_43.318 V_28.289 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_41.139 V_28.364 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_39.683 V_28.581 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_41.475 V_28.374 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_44.381 V_28.012 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_42.397 V_28.288 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_42.730 V_27.839 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_41.502 V_27.234 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_36.803 V_27.250 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_46.878 V_27.955 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_48.359 V_28.355 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_49.931 V_27.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_38.420 V_26.809 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_43.428 V_27.417 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_43.455 V_27.759 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_41.412 V_28.411 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_47.267 V_27.891 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_41.480 V_27.561 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_41.607 V_26.803 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_49.275 V_27.364 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_45.233 V_27.220 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_43.033 V_27.797 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_42.956 V_27.284 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_39.013 V_27.146 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_44.849 V_26.978 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_37.018 V_27.568 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_41.668 V_28.121 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_36.133 V_28.934 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_44.033 V_28.552 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_43.454 V_28.579 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_44.342 V_29.498 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_41.594 V_29.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_35.760 V_29.172 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_39.683 V_28.853 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_43.445 V_28.551 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_40.433 V_27.336 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_39.170 V_28.265 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_41.794 V_28.682 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_36.577 V_28.581 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_44.688 V_29.273 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_45.659 V_29.589 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_40.144 V_29.372 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_43.454 V_28.929 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_42.450 V_28.794 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_38.294 V_29.345 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_45.997 V_29.504 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_49.870 V_29.325 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_45.962 V_29.066 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_40.988 V_29.239 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_45.471 V_29.065 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_47.887 V_28.439 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_48.245 V_28.151 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_36.590 V_28.236 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_39.801 V_28.162 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_43.810 V_28.681 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_40.898 V_27.846 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_40.102 V_27.949 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_42.629 V_28.537 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_38.115 V_28.777 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_49.134 V_29.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_41.188 V_29.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_48.104 V_28.826 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_39.704 V_28.955 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_47.524 V_28.990 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_36.044 V_28.649 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_41.538 V_29.553 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_37.447 V_29.439 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_39.380 V_29.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_41.806 V_29.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_43.547 V_29.600 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_36.810 V_29.995 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_39.013 V_29.999 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_45.122 V_29.296 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  610.6947021484375\n",
      "Epoch 010: : Loss: T_600.428 V_610.695 | Acc: T_0.000) V_0.000\n",
      "Epoch 020: : Loss: T_592.199 V_613.121 | Acc: T_0.000) V_0.000\n",
      "Epoch 030: : Loss: T_589.813 V_613.087 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  609.7391357421875\n",
      "Epoch 040: : Loss: T_585.939 V_609.739 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  603.4304809570312\n",
      "Epoch 050: : Loss: T_582.066 V_603.430 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  597.1825561523438\n",
      "Epoch 060: : Loss: T_577.109 V_597.183 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  590.8402099609375\n",
      "Epoch 070: : Loss: T_573.734 V_590.840 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  582.5985717773438\n",
      "Epoch 080: : Loss: T_564.556 V_582.599 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  574.8153076171875\n",
      "Epoch 090: : Loss: T_561.997 V_574.815 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  567.2263793945312\n",
      "Epoch 100: : Loss: T_552.800 V_567.226 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  557.8514404296875\n",
      "Epoch 110: : Loss: T_551.141 V_557.851 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  553.3109741210938\n",
      "Epoch 120: : Loss: T_544.859 V_553.311 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  546.2249145507812\n",
      "Epoch 130: : Loss: T_534.117 V_546.225 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  539.0294799804688\n",
      "Epoch 140: : Loss: T_535.207 V_539.029 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  531.8914184570312\n",
      "Epoch 150: : Loss: T_527.341 V_531.891 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  530.397216796875\n",
      "Epoch 160: : Loss: T_511.622 V_530.397 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  521.2335205078125\n",
      "Epoch 170: : Loss: T_499.113 V_521.234 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  515.4213256835938\n",
      "Epoch 180: : Loss: T_501.832 V_515.421 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  505.66131591796875\n",
      "Epoch 190: : Loss: T_494.993 V_505.661 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  489.3746032714844\n",
      "Epoch 200: : Loss: T_494.311 V_489.375 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  485.4277648925781\n",
      "Epoch 210: : Loss: T_487.479 V_485.428 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  479.26080322265625\n",
      "Epoch 220: : Loss: T_472.365 V_479.261 | Acc: T_0.000) V_0.000\n",
      "Epoch 230: : Loss: T_472.094 V_479.756 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  469.1922912597656\n",
      "Epoch 240: : Loss: T_458.826 V_469.192 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  459.3759765625\n",
      "Epoch 250: : Loss: T_449.850 V_459.376 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  450.2790222167969\n",
      "Epoch 260: : Loss: T_442.196 V_450.279 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  441.7774963378906\n",
      "Epoch 270: : Loss: T_436.496 V_441.777 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  431.3170471191406\n",
      "Epoch 280: : Loss: T_432.628 V_431.317 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  430.4742431640625\n",
      "Epoch 290: : Loss: T_431.945 V_430.474 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  427.78326416015625\n",
      "Epoch 300: : Loss: T_420.319 V_427.783 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  408.4728088378906\n",
      "Epoch 310: : Loss: T_415.869 V_408.473 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  394.4627380371094\n",
      "Epoch 320: : Loss: T_412.428 V_394.463 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  371.4179382324219\n",
      "Epoch 330: : Loss: T_392.602 V_371.418 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  366.8195495605469\n",
      "Epoch 340: : Loss: T_387.912 V_366.820 | Acc: T_0.000) V_0.000\n",
      "Epoch 350: : Loss: T_376.794 V_369.858 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  356.7969665527344\n",
      "Epoch 360: : Loss: T_374.540 V_356.797 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  347.45123291015625\n",
      "Epoch 370: : Loss: T_369.048 V_347.451 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  338.5846862792969\n",
      "Epoch 380: : Loss: T_351.463 V_338.585 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  330.9023132324219\n",
      "Epoch 390: : Loss: T_335.013 V_330.902 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  327.20355224609375\n",
      "Epoch 400: : Loss: T_347.313 V_327.204 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  315.0760498046875\n",
      "Epoch 410: : Loss: T_330.962 V_315.076 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  302.24029541015625\n",
      "Epoch 420: : Loss: T_316.071 V_302.240 | Acc: T_0.000) V_0.000\n",
      "Epoch 430: : Loss: T_314.788 V_315.391 | Acc: T_0.000) V_0.000\n",
      "Epoch 440: : Loss: T_311.869 V_308.904 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  286.5699768066406\n",
      "Epoch 450: : Loss: T_295.491 V_286.570 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  270.4427490234375\n",
      "Epoch 460: : Loss: T_282.651 V_270.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 470: : Loss: T_288.588 V_271.537 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  264.51678466796875\n",
      "Epoch 480: : Loss: T_276.033 V_264.517 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  253.33856201171875\n",
      "Epoch 490: : Loss: T_279.940 V_253.339 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  243.19627380371094\n",
      "Epoch 500: : Loss: T_259.603 V_243.196 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  239.211181640625\n",
      "Epoch 510: : Loss: T_247.653 V_239.211 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  231.07350158691406\n",
      "Epoch 520: : Loss: T_247.415 V_231.074 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  217.46221923828125\n",
      "Epoch 530: : Loss: T_241.027 V_217.462 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  210.28514099121094\n",
      "Epoch 540: : Loss: T_223.651 V_210.285 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  208.4026336669922\n",
      "Epoch 550: : Loss: T_217.527 V_208.403 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  200.6350555419922\n",
      "Epoch 560: : Loss: T_210.280 V_200.635 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  190.18373107910156\n",
      "Epoch 570: : Loss: T_211.214 V_190.184 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  183.39060974121094\n",
      "Epoch 580: : Loss: T_201.157 V_183.391 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  178.1829376220703\n",
      "Epoch 590: : Loss: T_192.775 V_178.183 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  170.40306091308594\n",
      "Epoch 600: : Loss: T_180.589 V_170.403 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  166.45166015625\n",
      "Epoch 610: : Loss: T_192.826 V_166.452 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  156.06080627441406\n",
      "Epoch 620: : Loss: T_173.197 V_156.061 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  148.92922973632812\n",
      "Epoch 630: : Loss: T_179.542 V_148.929 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  145.03529357910156\n",
      "Epoch 640: : Loss: T_161.466 V_145.035 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  137.39903259277344\n",
      "Epoch 650: : Loss: T_157.173 V_137.399 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  132.7537078857422\n",
      "Epoch 660: : Loss: T_145.667 V_132.754 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  126.06439208984375\n",
      "Epoch 670: : Loss: T_140.063 V_126.064 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  119.33101654052734\n",
      "Epoch 680: : Loss: T_140.207 V_119.331 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  115.88575744628906\n",
      "Epoch 690: : Loss: T_128.717 V_115.886 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  109.5077133178711\n",
      "Epoch 700: : Loss: T_123.373 V_109.508 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  104.21009063720703\n",
      "Epoch 710: : Loss: T_112.505 V_104.210 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  97.5760269165039\n",
      "Epoch 720: : Loss: T_108.478 V_97.576 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  93.32355499267578\n",
      "Epoch 730: : Loss: T_121.900 V_93.324 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  88.74116516113281\n",
      "Epoch 740: : Loss: T_112.810 V_88.741 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  81.96369934082031\n",
      "Epoch 750: : Loss: T_102.905 V_81.964 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  80.66168975830078\n",
      "Epoch 760: : Loss: T_115.355 V_80.662 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  78.52649688720703\n",
      "Epoch 770: : Loss: T_88.305 V_78.526 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  73.96754455566406\n",
      "Epoch 780: : Loss: T_106.747 V_73.968 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  71.05217742919922\n",
      "Epoch 790: : Loss: T_96.772 V_71.052 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  68.44867706298828\n",
      "Epoch 800: : Loss: T_91.827 V_68.449 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  63.909637451171875\n",
      "Epoch 810: : Loss: T_91.521 V_63.910 | Acc: T_0.000) V_0.000\n",
      "Epoch 820: : Loss: T_78.538 V_64.615 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  62.397396087646484\n",
      "Epoch 830: : Loss: T_92.809 V_62.397 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  58.274993896484375\n",
      "Epoch 840: : Loss: T_74.285 V_58.275 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  54.611473083496094\n",
      "Epoch 850: : Loss: T_72.163 V_54.611 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  51.68033218383789\n",
      "Epoch 860: : Loss: T_77.892 V_51.680 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.97835922241211\n",
      "Epoch 870: : Loss: T_71.096 V_48.978 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.13838195800781\n",
      "Epoch 880: : Loss: T_76.599 V_48.138 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  46.40203094482422\n",
      "Epoch 890: : Loss: T_69.149 V_46.402 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  43.08194351196289\n",
      "Epoch 900: : Loss: T_64.495 V_43.082 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.39106369018555\n",
      "Epoch 910: : Loss: T_61.163 V_40.391 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.816917419433594\n",
      "Epoch 920: : Loss: T_70.591 V_39.817 | Acc: T_0.000) V_0.000\n",
      "Epoch 930: : Loss: T_56.360 V_40.726 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.01583480834961\n",
      "Epoch 940: : Loss: T_57.282 V_39.016 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.42366409301758\n",
      "Epoch 950: : Loss: T_59.416 V_37.424 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.85260009765625\n",
      "Epoch 960: : Loss: T_51.927 V_36.853 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.26443099975586\n",
      "Epoch 970: : Loss: T_56.125 V_36.264 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.669219970703125\n",
      "Epoch 980: : Loss: T_59.012 V_35.669 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.33537292480469\n",
      "Epoch 990: : Loss: T_59.191 V_34.335 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.95383834838867\n",
      "Epoch 1000: : Loss: T_55.760 V_32.954 | Acc: T_0.000) V_0.000\n",
      "Epoch 1010: : Loss: T_51.963 V_33.384 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.297794342041016\n",
      "Epoch 1020: : Loss: T_55.795 V_32.298 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.027448654174805\n",
      "Epoch 1030: : Loss: T_43.076 V_31.027 | Acc: T_0.000) V_0.000\n",
      "Epoch 1040: : Loss: T_60.685 V_31.922 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.036705017089844\n",
      "Epoch 1050: : Loss: T_37.867 V_30.037 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.333406448364258\n",
      "Epoch 1060: : Loss: T_59.592 V_29.333 | Acc: T_0.000) V_0.000\n",
      "Epoch 1070: : Loss: T_55.605 V_30.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 1080: : Loss: T_53.077 V_31.433 | Acc: T_0.000) V_0.000\n",
      "Epoch 1090: : Loss: T_60.371 V_30.907 | Acc: T_0.000) V_0.000\n",
      "Epoch 1100: : Loss: T_54.956 V_29.495 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.988643646240234\n",
      "Epoch 1110: : Loss: T_49.931 V_28.989 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.80452537536621\n",
      "Epoch 1120: : Loss: T_50.289 V_27.805 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_42.939 V_27.848 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_55.033 V_28.061 | Acc: T_0.000) V_0.000\n",
      "Epoch 1150: : Loss: T_49.792 V_27.828 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_45.053 V_27.851 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.998088836669922\n",
      "Epoch 1170: : Loss: T_52.243 V_26.998 | Acc: T_0.000) V_0.000\n",
      "Epoch 1180: : Loss: T_46.976 V_27.480 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_51.122 V_27.497 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_45.559 V_27.416 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.34554672241211\n",
      "Epoch 1210: : Loss: T_44.616 V_26.346 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.80581283569336\n",
      "Epoch 1220: : Loss: T_46.730 V_25.806 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_47.713 V_26.073 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_40.732 V_26.176 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_50.862 V_27.113 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_45.048 V_27.129 | Acc: T_0.000) V_0.000\n",
      "Epoch 1270: : Loss: T_44.703 V_27.323 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_55.573 V_26.741 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_46.399 V_27.547 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_47.312 V_26.756 | Acc: T_0.000) V_0.000\n",
      "Epoch 1310: : Loss: T_50.792 V_27.256 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_49.627 V_26.627 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.63297462463379\n",
      "Epoch 1330: : Loss: T_51.406 V_25.633 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_47.370 V_27.220 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_41.889 V_26.390 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_54.125 V_26.088 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_47.467 V_26.066 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_42.059 V_25.814 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_45.101 V_25.722 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_48.379 V_26.741 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_52.946 V_26.121 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_54.302 V_25.755 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_52.090 V_26.064 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_47.527 V_26.267 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_44.965 V_26.050 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_48.327 V_26.254 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_54.255 V_25.664 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_42.914 V_26.363 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_47.811 V_26.581 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_46.628 V_26.587 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_45.571 V_26.166 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_50.647 V_26.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_49.739 V_26.149 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.434293746948242\n",
      "Epoch 1540: : Loss: T_50.447 V_25.434 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.979337692260742\n",
      "Epoch 1550: : Loss: T_51.797 V_24.979 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_44.363 V_25.975 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_49.791 V_26.245 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_47.767 V_26.667 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_47.721 V_26.544 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_43.872 V_26.192 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_44.383 V_25.977 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_49.323 V_26.291 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_56.133 V_26.572 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_40.080 V_26.801 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_40.898 V_26.690 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_52.490 V_25.979 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_46.510 V_25.960 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_49.119 V_25.946 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_47.311 V_25.780 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_49.056 V_26.244 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_56.967 V_26.059 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_39.944 V_26.806 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_48.054 V_26.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_50.666 V_26.471 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_46.211 V_25.962 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_51.706 V_26.640 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_45.199 V_27.271 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_41.184 V_27.089 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_57.570 V_27.518 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_45.549 V_27.196 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_48.725 V_27.282 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_57.993 V_26.818 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_51.606 V_26.783 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_39.384 V_27.313 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_45.638 V_26.421 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_51.032 V_27.845 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_47.841 V_27.498 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_47.155 V_26.548 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_54.700 V_25.809 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_44.713 V_26.091 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_46.503 V_26.255 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_49.457 V_26.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_44.694 V_26.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_46.135 V_26.000 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_44.000 V_26.385 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_49.906 V_27.348 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_41.490 V_27.563 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_49.131 V_27.336 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_42.903 V_27.383 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_56.364 V_28.092 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_45.982 V_28.157 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_52.170 V_28.181 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_39.269 V_27.804 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_45.027 V_27.109 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_34.975 V_27.513 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_46.825 V_27.137 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_47.840 V_26.498 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_51.599 V_26.078 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_44.202 V_25.025 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_48.124 V_25.803 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_47.234 V_26.187 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_39.411 V_26.488 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_47.271 V_26.490 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_46.198 V_27.393 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_48.455 V_27.562 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_51.866 V_27.839 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_51.250 V_27.643 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_48.718 V_27.567 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_44.235 V_26.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_51.542 V_26.781 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_46.773 V_27.230 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_58.030 V_27.006 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_44.996 V_26.392 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_50.491 V_26.500 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_45.122 V_26.786 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_47.906 V_27.299 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_42.064 V_26.603 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_41.847 V_26.525 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_46.995 V_27.017 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_44.309 V_27.248 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_45.646 V_27.124 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_37.110 V_26.681 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_42.303 V_26.993 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_44.281 V_27.594 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_41.947 V_27.129 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_48.040 V_27.232 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_49.191 V_28.156 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_46.627 V_28.253 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_46.018 V_27.677 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_44.046 V_27.728 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_47.642 V_28.001 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_45.783 V_28.196 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_47.075 V_28.006 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_49.868 V_28.335 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_40.814 V_28.139 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_40.773 V_26.377 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_46.161 V_27.218 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_42.264 V_27.064 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_45.534 V_26.822 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_40.007 V_27.172 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_46.118 V_26.723 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_45.094 V_27.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_44.607 V_28.033 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_46.463 V_27.992 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_44.226 V_28.051 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_41.489 V_26.565 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_41.194 V_27.544 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_50.522 V_28.674 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_42.603 V_28.757 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_52.607 V_29.298 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_43.328 V_27.548 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_58.023 V_26.702 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_43.828 V_26.139 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_41.370 V_27.292 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_46.584 V_27.663 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_42.410 V_27.945 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_43.847 V_27.321 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_40.389 V_27.124 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_39.248 V_27.989 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_48.284 V_27.832 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_46.068 V_28.039 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_41.430 V_28.687 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_46.923 V_27.993 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_50.632 V_27.163 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_47.606 V_27.311 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_38.149 V_27.827 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_41.936 V_28.095 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_43.799 V_28.033 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_45.155 V_27.808 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_40.824 V_28.752 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_44.508 V_28.638 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_38.402 V_27.862 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_48.370 V_28.517 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_42.956 V_27.767 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_47.144 V_28.095 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_47.798 V_28.794 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_42.997 V_28.204 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_44.423 V_27.901 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_42.973 V_27.755 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_44.500 V_27.073 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_49.160 V_27.632 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_44.321 V_28.007 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_45.995 V_28.593 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_53.179 V_28.920 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_40.055 V_28.615 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_46.387 V_28.341 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_40.916 V_28.649 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_48.496 V_28.278 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_41.472 V_27.668 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_53.928 V_27.403 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_38.245 V_27.463 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_36.998 V_27.064 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_47.829 V_27.295 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_42.586 V_27.807 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_45.143 V_27.749 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_43.733 V_28.307 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_38.214 V_28.153 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_42.861 V_28.350 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_45.440 V_28.360 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_48.386 V_28.014 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_52.260 V_28.723 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_45.260 V_29.119 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_42.514 V_28.764 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_45.240 V_28.779 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_48.379 V_28.032 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_45.686 V_28.191 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_52.757 V_28.355 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_36.220 V_28.459 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_46.323 V_28.354 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_50.458 V_28.176 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_44.668 V_28.648 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_41.264 V_28.785 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_45.017 V_28.482 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_40.451 V_28.350 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_36.232 V_28.834 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_28.317 V_28.681 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_41.768 V_28.312 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_46.158 V_27.279 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_47.838 V_27.405 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_39.042 V_28.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_41.538 V_27.951 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_43.646 V_27.852 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_37.505 V_27.964 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_39.094 V_27.900 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_50.567 V_28.340 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_46.409 V_28.059 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_44.270 V_28.753 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_36.443 V_28.112 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_39.058 V_26.552 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_43.366 V_27.522 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_46.163 V_27.948 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_37.845 V_28.182 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_45.104 V_28.727 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_45.489 V_29.006 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_36.934 V_28.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_42.164 V_29.193 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_49.063 V_28.409 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_48.512 V_28.183 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_42.104 V_27.847 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_44.514 V_27.656 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  599.2432861328125\n",
      "Epoch 010: : Loss: T_593.938 V_599.243 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  593.4649658203125\n",
      "Epoch 020: : Loss: T_590.750 V_593.465 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  590.98583984375\n",
      "Epoch 030: : Loss: T_589.910 V_590.986 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  589.9188232421875\n",
      "Epoch 040: : Loss: T_583.157 V_589.919 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  589.0254516601562\n",
      "Epoch 050: : Loss: T_583.410 V_589.025 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  587.3031616210938\n",
      "Epoch 060: : Loss: T_578.127 V_587.303 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  585.4638671875\n",
      "Epoch 070: : Loss: T_574.083 V_585.464 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  582.024658203125\n",
      "Epoch 080: : Loss: T_570.473 V_582.025 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  578.6361694335938\n",
      "Epoch 090: : Loss: T_564.171 V_578.636 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  573.7893676757812\n",
      "Epoch 100: : Loss: T_562.013 V_573.789 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  570.8904418945312\n",
      "Epoch 110: : Loss: T_552.399 V_570.890 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  567.4810180664062\n",
      "Epoch 120: : Loss: T_545.835 V_567.481 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  561.9065551757812\n",
      "Epoch 130: : Loss: T_540.791 V_561.907 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  557.09716796875\n",
      "Epoch 140: : Loss: T_538.680 V_557.097 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  547.8018188476562\n",
      "Epoch 150: : Loss: T_523.542 V_547.802 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  537.4728393554688\n",
      "Epoch 160: : Loss: T_514.258 V_537.473 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  531.2227783203125\n",
      "Epoch 170: : Loss: T_520.190 V_531.223 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  528.04931640625\n",
      "Epoch 180: : Loss: T_497.294 V_528.049 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  521.9339599609375\n",
      "Epoch 190: : Loss: T_500.968 V_521.934 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  509.7419128417969\n",
      "Epoch 200: : Loss: T_494.401 V_509.742 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  498.91021728515625\n",
      "Epoch 210: : Loss: T_484.955 V_498.910 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  494.5803527832031\n",
      "Epoch 220: : Loss: T_481.448 V_494.580 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  485.7283630371094\n",
      "Epoch 230: : Loss: T_473.143 V_485.728 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  477.5133972167969\n",
      "Epoch 240: : Loss: T_468.574 V_477.513 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  476.1324157714844\n",
      "Epoch 250: : Loss: T_457.604 V_476.132 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  461.9302673339844\n",
      "Epoch 260: : Loss: T_449.264 V_461.930 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  451.76446533203125\n",
      "Epoch 270: : Loss: T_442.771 V_451.764 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  437.78326416015625\n",
      "Epoch 280: : Loss: T_426.969 V_437.783 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  436.74420166015625\n",
      "Epoch 290: : Loss: T_426.350 V_436.744 | Acc: T_0.000) V_0.000\n",
      "Epoch 300: : Loss: T_420.534 V_436.789 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  430.1939697265625\n",
      "Epoch 310: : Loss: T_411.788 V_430.194 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  407.9638671875\n",
      "Epoch 320: : Loss: T_397.709 V_407.964 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  389.1446838378906\n",
      "Epoch 330: : Loss: T_396.494 V_389.145 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  365.62152099609375\n",
      "Epoch 340: : Loss: T_383.901 V_365.622 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  361.20526123046875\n",
      "Epoch 350: : Loss: T_385.753 V_361.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_375.068 V_367.179 | Acc: T_0.000) V_0.000\n",
      "Epoch 370: : Loss: T_358.258 V_362.702 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  342.56707763671875\n",
      "Epoch 380: : Loss: T_349.688 V_342.567 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  332.05596923828125\n",
      "Epoch 390: : Loss: T_344.349 V_332.056 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  325.01776123046875\n",
      "Epoch 400: : Loss: T_332.215 V_325.018 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  307.1500549316406\n",
      "Epoch 410: : Loss: T_322.437 V_307.150 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  294.0687255859375\n",
      "Epoch 420: : Loss: T_317.220 V_294.069 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  287.09564208984375\n",
      "Epoch 430: : Loss: T_302.405 V_287.096 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  276.74200439453125\n",
      "Epoch 440: : Loss: T_310.789 V_276.742 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  272.7350158691406\n",
      "Epoch 450: : Loss: T_301.242 V_272.735 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  259.47265625\n",
      "Epoch 460: : Loss: T_283.738 V_259.473 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  250.68576049804688\n",
      "Epoch 470: : Loss: T_273.992 V_250.686 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  246.6546630859375\n",
      "Epoch 480: : Loss: T_270.802 V_246.655 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  239.8186798095703\n",
      "Epoch 490: : Loss: T_261.464 V_239.819 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  230.66064453125\n",
      "Epoch 500: : Loss: T_252.240 V_230.661 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  218.7955322265625\n",
      "Epoch 510: : Loss: T_244.003 V_218.796 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  208.74855041503906\n",
      "Epoch 520: : Loss: T_242.839 V_208.749 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  196.8323516845703\n",
      "Epoch 530: : Loss: T_225.260 V_196.832 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  190.94956970214844\n",
      "Epoch 540: : Loss: T_223.463 V_190.950 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  186.1713409423828\n",
      "Epoch 550: : Loss: T_213.181 V_186.171 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  177.51864624023438\n",
      "Epoch 560: : Loss: T_205.492 V_177.519 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  167.06275939941406\n",
      "Epoch 570: : Loss: T_206.527 V_167.063 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  165.23306274414062\n",
      "Epoch 580: : Loss: T_196.911 V_165.233 | Acc: T_0.000) V_0.000\n",
      "Epoch 590: : Loss: T_195.327 V_165.412 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  155.41192626953125\n",
      "Epoch 600: : Loss: T_184.418 V_155.412 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  147.86859130859375\n",
      "Epoch 610: : Loss: T_170.665 V_147.869 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  140.3762969970703\n",
      "Epoch 620: : Loss: T_162.057 V_140.376 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  128.51133728027344\n",
      "Epoch 630: : Loss: T_155.720 V_128.511 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  127.71930694580078\n",
      "Epoch 640: : Loss: T_166.090 V_127.719 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  122.56513214111328\n",
      "Epoch 650: : Loss: T_145.277 V_122.565 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  115.21788024902344\n",
      "Epoch 660: : Loss: T_136.626 V_115.218 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  113.59947967529297\n",
      "Epoch 670: : Loss: T_134.782 V_113.599 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  106.24544525146484\n",
      "Epoch 680: : Loss: T_137.481 V_106.245 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  104.7360610961914\n",
      "Epoch 690: : Loss: T_116.722 V_104.736 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  94.9261474609375\n",
      "Epoch 700: : Loss: T_123.921 V_94.926 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  91.30219268798828\n",
      "Epoch 710: : Loss: T_119.615 V_91.302 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  88.23641967773438\n",
      "Epoch 720: : Loss: T_112.847 V_88.236 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  81.11283874511719\n",
      "Epoch 730: : Loss: T_102.498 V_81.113 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  80.56073760986328\n",
      "Epoch 740: : Loss: T_104.574 V_80.561 | Acc: T_0.000) V_0.000\n",
      "Epoch 750: : Loss: T_110.253 V_80.899 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  72.5933609008789\n",
      "Epoch 760: : Loss: T_99.039 V_72.593 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  70.87338256835938\n",
      "Epoch 770: : Loss: T_92.264 V_70.873 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  67.13735961914062\n",
      "Epoch 780: : Loss: T_93.970 V_67.137 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  65.98967742919922\n",
      "Epoch 790: : Loss: T_89.695 V_65.990 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  59.83388900756836\n",
      "Epoch 800: : Loss: T_76.784 V_59.834 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  56.539302825927734\n",
      "Epoch 810: : Loss: T_88.094 V_56.539 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  55.04346466064453\n",
      "Epoch 820: : Loss: T_82.110 V_55.043 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  53.291473388671875\n",
      "Epoch 830: : Loss: T_78.054 V_53.291 | Acc: T_0.000) V_0.000\n",
      "Epoch 840: : Loss: T_81.280 V_53.451 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.22465896606445\n",
      "Epoch 850: : Loss: T_68.703 V_48.225 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  47.09627151489258\n",
      "Epoch 860: : Loss: T_63.045 V_47.096 | Acc: T_0.000) V_0.000\n",
      "Epoch 870: : Loss: T_75.101 V_47.802 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  46.754390716552734\n",
      "Epoch 880: : Loss: T_65.151 V_46.754 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  45.63498306274414\n",
      "Epoch 890: : Loss: T_64.222 V_45.635 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.147701263427734\n",
      "Epoch 900: : Loss: T_78.318 V_42.148 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.84459686279297\n",
      "Epoch 910: : Loss: T_70.890 V_39.845 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.62538528442383\n",
      "Epoch 920: : Loss: T_68.049 V_39.625 | Acc: T_0.000) V_0.000\n",
      "Epoch 930: : Loss: T_63.140 V_40.551 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.617042541503906\n",
      "Epoch 940: : Loss: T_67.404 V_39.617 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.13225173950195\n",
      "Epoch 950: : Loss: T_53.856 V_38.132 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.5826530456543\n",
      "Epoch 960: : Loss: T_63.674 V_37.583 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.13310241699219\n",
      "Epoch 970: : Loss: T_49.487 V_35.133 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.31804656982422\n",
      "Epoch 980: : Loss: T_62.706 V_33.318 | Acc: T_0.000) V_0.000\n",
      "Epoch 990: : Loss: T_52.378 V_33.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.54521560668945\n",
      "Epoch 1000: : Loss: T_59.623 V_32.545 | Acc: T_0.000) V_0.000\n",
      "Epoch 1010: : Loss: T_52.411 V_32.659 | Acc: T_0.000) V_0.000\n",
      "Epoch 1020: : Loss: T_53.204 V_33.804 | Acc: T_0.000) V_0.000\n",
      "Epoch 1030: : Loss: T_56.062 V_32.869 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.652814865112305\n",
      "Epoch 1040: : Loss: T_53.006 V_30.653 | Acc: T_0.000) V_0.000\n",
      "Epoch 1050: : Loss: T_56.070 V_31.926 | Acc: T_0.000) V_0.000\n",
      "Epoch 1060: : Loss: T_60.119 V_32.040 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.614233016967773\n",
      "Epoch 1070: : Loss: T_51.817 V_30.614 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.71961784362793\n",
      "Epoch 1080: : Loss: T_42.784 V_28.720 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.14545440673828\n",
      "Epoch 1090: : Loss: T_56.279 V_27.145 | Acc: T_0.000) V_0.000\n",
      "Epoch 1100: : Loss: T_54.754 V_28.897 | Acc: T_0.000) V_0.000\n",
      "Epoch 1110: : Loss: T_51.735 V_29.667 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.14290428161621\n",
      "Epoch 1120: : Loss: T_59.547 V_27.143 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.203821182250977\n",
      "Epoch 1130: : Loss: T_44.926 V_26.204 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_53.366 V_29.151 | Acc: T_0.000) V_0.000\n",
      "Epoch 1150: : Loss: T_45.691 V_28.271 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_53.046 V_28.476 | Acc: T_0.000) V_0.000\n",
      "Epoch 1170: : Loss: T_55.628 V_27.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 1180: : Loss: T_55.848 V_27.269 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_43.334 V_28.914 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_57.309 V_27.602 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.439531326293945\n",
      "Epoch 1210: : Loss: T_50.473 V_25.440 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.726621627807617\n",
      "Epoch 1220: : Loss: T_42.021 V_24.727 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_49.023 V_24.994 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_47.882 V_25.547 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_48.073 V_25.432 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_52.469 V_26.044 | Acc: T_0.000) V_0.000\n",
      "Epoch 1270: : Loss: T_43.125 V_26.549 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_63.350 V_27.545 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_51.027 V_27.116 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.182004928588867\n",
      "Epoch 1300: : Loss: T_57.515 V_24.182 | Acc: T_0.000) V_0.000\n",
      "Epoch 1310: : Loss: T_47.980 V_24.365 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_51.712 V_25.090 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_55.668 V_26.201 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_47.355 V_27.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_54.303 V_27.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_47.053 V_28.317 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_44.980 V_26.867 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_51.578 V_25.614 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_47.337 V_25.075 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_48.121 V_24.728 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_45.368 V_25.260 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_54.710 V_25.042 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  23.72808837890625\n",
      "Epoch 1430: : Loss: T_46.747 V_23.728 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_43.886 V_24.399 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_48.833 V_25.955 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_43.081 V_26.541 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_39.559 V_25.145 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_40.851 V_24.826 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_44.731 V_24.942 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_43.918 V_24.798 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_46.760 V_24.304 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_55.524 V_24.055 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_44.433 V_24.447 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_45.086 V_24.579 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_53.769 V_25.887 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_56.513 V_27.014 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_45.994 V_26.778 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_50.397 V_27.006 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_44.930 V_28.040 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_50.122 V_28.084 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_50.653 V_27.774 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_48.838 V_27.284 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_44.085 V_26.836 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_41.359 V_27.028 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_50.862 V_27.215 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_49.186 V_25.768 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_42.132 V_24.296 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_46.058 V_23.798 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_50.410 V_25.747 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_49.515 V_26.840 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_48.780 V_24.564 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_46.834 V_24.011 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_53.523 V_24.810 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_45.177 V_26.229 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_44.710 V_27.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_46.085 V_27.807 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_46.311 V_28.307 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_45.364 V_28.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_48.412 V_28.872 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_37.293 V_26.541 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_52.378 V_24.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_46.109 V_24.887 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_52.430 V_25.430 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_48.680 V_25.805 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_45.863 V_25.297 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_44.179 V_26.936 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_43.001 V_29.111 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_48.324 V_28.539 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_44.294 V_28.597 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_45.357 V_27.666 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_46.085 V_25.147 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_44.831 V_25.765 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_43.033 V_26.532 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_50.349 V_27.220 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_48.681 V_27.906 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_43.534 V_27.162 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_51.854 V_26.124 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_50.745 V_26.834 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_44.219 V_26.939 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_55.029 V_26.537 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_40.836 V_26.649 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_50.551 V_26.224 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_54.964 V_26.331 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_44.351 V_26.652 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_48.638 V_28.445 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_51.439 V_28.322 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_45.548 V_28.074 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_49.541 V_27.883 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_48.562 V_28.342 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_52.000 V_27.681 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_49.428 V_26.738 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_48.285 V_27.097 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_50.995 V_29.053 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_43.175 V_29.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_44.005 V_27.503 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_41.001 V_26.118 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_43.424 V_25.744 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_40.211 V_26.212 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_42.463 V_26.142 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_43.041 V_26.201 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_44.747 V_27.097 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_45.499 V_27.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_55.148 V_27.070 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_46.445 V_26.076 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_45.150 V_26.786 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_44.120 V_27.452 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_38.618 V_27.897 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_52.407 V_29.109 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_44.496 V_29.460 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_43.731 V_27.597 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_43.482 V_27.634 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_43.006 V_26.559 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_46.064 V_26.599 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_37.622 V_28.703 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_52.851 V_27.977 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_49.017 V_25.701 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_47.492 V_26.291 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_47.005 V_28.817 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_39.776 V_28.072 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_47.494 V_28.886 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_41.453 V_27.844 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_40.875 V_26.835 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_42.108 V_27.756 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_48.669 V_28.197 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_44.802 V_28.495 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_43.148 V_29.201 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_42.992 V_27.902 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_52.183 V_29.141 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_50.708 V_27.624 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_46.366 V_26.626 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_39.800 V_25.065 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_46.602 V_27.562 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_41.287 V_25.624 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_49.245 V_25.871 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_37.585 V_27.419 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_43.243 V_24.981 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_41.787 V_24.789 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_38.436 V_25.096 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_43.978 V_25.122 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_49.103 V_26.079 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_45.294 V_27.361 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_49.706 V_26.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_45.983 V_25.770 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_45.597 V_25.073 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_56.609 V_27.113 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_41.992 V_26.422 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_45.595 V_25.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_44.159 V_26.303 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_44.593 V_27.305 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_54.592 V_26.761 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_39.151 V_28.504 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_39.498 V_29.055 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_42.992 V_29.012 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_47.295 V_28.708 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_47.597 V_25.032 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_45.522 V_26.312 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_39.458 V_27.933 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_45.951 V_27.630 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_52.933 V_26.332 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_41.960 V_27.807 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_44.181 V_27.799 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_44.637 V_28.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_40.604 V_28.121 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_37.867 V_25.700 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_49.374 V_25.281 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_37.574 V_25.867 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_47.131 V_26.584 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_39.856 V_27.002 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_43.812 V_27.676 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_40.870 V_28.871 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_56.350 V_28.329 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_49.810 V_28.085 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_52.829 V_27.732 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_43.664 V_27.165 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_42.998 V_27.070 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_45.433 V_26.556 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_50.367 V_25.530 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_46.535 V_25.427 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_40.179 V_24.909 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_41.373 V_26.192 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_43.746 V_26.301 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_52.206 V_25.805 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_39.264 V_25.819 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_39.379 V_26.203 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_39.075 V_26.710 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_44.811 V_27.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_45.650 V_26.941 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_44.255 V_26.453 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_40.048 V_27.559 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_46.734 V_29.209 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_36.650 V_28.954 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_51.121 V_27.278 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_49.935 V_27.558 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_42.619 V_28.939 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_49.108 V_29.294 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_39.763 V_27.800 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_51.042 V_27.526 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_37.981 V_26.629 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_37.492 V_27.586 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_43.769 V_28.323 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_42.854 V_28.338 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_38.892 V_28.151 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_39.453 V_29.056 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_45.352 V_28.835 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_42.940 V_28.311 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_43.692 V_28.409 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_39.909 V_28.599 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_36.148 V_29.055 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_42.201 V_28.178 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_39.022 V_27.824 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_44.035 V_29.079 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_48.905 V_29.282 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_47.066 V_29.174 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_37.090 V_29.762 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_43.821 V_29.115 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_41.721 V_28.938 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_36.375 V_29.137 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_44.879 V_28.556 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_42.406 V_28.116 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_44.248 V_28.318 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_47.536 V_28.246 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_39.763 V_28.281 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_43.951 V_28.681 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_41.998 V_28.152 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_35.203 V_28.756 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_45.703 V_28.265 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_43.708 V_28.272 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_35.295 V_27.300 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_48.674 V_27.929 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_48.441 V_27.636 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  614.3724975585938\n",
      "Epoch 010: : Loss: T_597.161 V_614.372 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  613.9566040039062\n",
      "Epoch 020: : Loss: T_595.198 V_613.957 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  612.2132568359375\n",
      "Epoch 030: : Loss: T_593.379 V_612.213 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  608.4959106445312\n",
      "Epoch 040: : Loss: T_587.301 V_608.496 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  604.2567749023438\n",
      "Epoch 050: : Loss: T_582.989 V_604.257 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  599.6271362304688\n",
      "Epoch 060: : Loss: T_575.644 V_599.627 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  593.7547607421875\n",
      "Epoch 070: : Loss: T_573.069 V_593.755 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  586.2720947265625\n",
      "Epoch 080: : Loss: T_569.597 V_586.272 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  579.93994140625\n",
      "Epoch 090: : Loss: T_559.106 V_579.940 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  574.5570068359375\n",
      "Epoch 100: : Loss: T_554.986 V_574.557 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  567.2388916015625\n",
      "Epoch 110: : Loss: T_557.848 V_567.239 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  554.9634399414062\n",
      "Epoch 120: : Loss: T_548.029 V_554.963 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  546.375244140625\n",
      "Epoch 130: : Loss: T_540.126 V_546.375 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  536.7412719726562\n",
      "Epoch 140: : Loss: T_528.005 V_536.741 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  527.9877319335938\n",
      "Epoch 150: : Loss: T_530.270 V_527.988 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  518.602294921875\n",
      "Epoch 160: : Loss: T_520.418 V_518.602 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  515.9041137695312\n",
      "Epoch 170: : Loss: T_503.877 V_515.904 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  512.0947875976562\n",
      "Epoch 180: : Loss: T_505.096 V_512.095 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  506.6541442871094\n",
      "Epoch 190: : Loss: T_491.749 V_506.654 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  498.4279479980469\n",
      "Epoch 200: : Loss: T_482.509 V_498.428 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  488.2788391113281\n",
      "Epoch 210: : Loss: T_485.190 V_488.279 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  483.67205810546875\n",
      "Epoch 220: : Loss: T_474.211 V_483.672 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  467.9228515625\n",
      "Epoch 230: : Loss: T_470.949 V_467.923 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  467.07269287109375\n",
      "Epoch 240: : Loss: T_459.582 V_467.073 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  453.9435729980469\n",
      "Epoch 250: : Loss: T_445.003 V_453.944 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  440.6719665527344\n",
      "Epoch 260: : Loss: T_444.001 V_440.672 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  437.4314270019531\n",
      "Epoch 270: : Loss: T_422.688 V_437.431 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  422.7973937988281\n",
      "Epoch 280: : Loss: T_431.192 V_422.797 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  417.0592346191406\n",
      "Epoch 290: : Loss: T_424.899 V_417.059 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  410.1288146972656\n",
      "Epoch 300: : Loss: T_406.748 V_410.129 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  404.20037841796875\n",
      "Epoch 310: : Loss: T_409.931 V_404.200 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  397.7936096191406\n",
      "Epoch 320: : Loss: T_393.085 V_397.794 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  391.66790771484375\n",
      "Epoch 330: : Loss: T_401.709 V_391.668 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  382.14654541015625\n",
      "Epoch 340: : Loss: T_383.927 V_382.147 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  370.2381896972656\n",
      "Epoch 350: : Loss: T_372.680 V_370.238 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  350.73321533203125\n",
      "Epoch 360: : Loss: T_364.515 V_350.733 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  343.6183776855469\n",
      "Epoch 370: : Loss: T_362.787 V_343.618 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  334.93902587890625\n",
      "Epoch 380: : Loss: T_356.390 V_334.939 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  332.2632751464844\n",
      "Epoch 390: : Loss: T_353.143 V_332.263 | Acc: T_0.000) V_0.000\n",
      "Epoch 400: : Loss: T_342.012 V_335.144 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  320.2984313964844\n",
      "Epoch 410: : Loss: T_343.854 V_320.298 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  311.9388427734375\n",
      "Epoch 420: : Loss: T_314.172 V_311.939 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  307.3755187988281\n",
      "Epoch 430: : Loss: T_313.657 V_307.376 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  296.4132385253906\n",
      "Epoch 440: : Loss: T_314.593 V_296.413 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  288.18365478515625\n",
      "Epoch 450: : Loss: T_302.541 V_288.184 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  278.0572509765625\n",
      "Epoch 460: : Loss: T_290.996 V_278.057 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  269.4367370605469\n",
      "Epoch 470: : Loss: T_296.348 V_269.437 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  264.84033203125\n",
      "Epoch 480: : Loss: T_276.597 V_264.840 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  259.2839050292969\n",
      "Epoch 490: : Loss: T_274.571 V_259.284 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  248.59786987304688\n",
      "Epoch 500: : Loss: T_254.081 V_248.598 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  235.3320770263672\n",
      "Epoch 510: : Loss: T_268.256 V_235.332 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  233.6640625\n",
      "Epoch 520: : Loss: T_241.876 V_233.664 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  228.02381896972656\n",
      "Epoch 530: : Loss: T_237.369 V_228.024 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  215.24717712402344\n",
      "Epoch 540: : Loss: T_233.369 V_215.247 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  203.20013427734375\n",
      "Epoch 550: : Loss: T_224.644 V_203.200 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  197.4096221923828\n",
      "Epoch 560: : Loss: T_211.813 V_197.410 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  195.65237426757812\n",
      "Epoch 570: : Loss: T_210.368 V_195.652 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  193.39974975585938\n",
      "Epoch 580: : Loss: T_208.234 V_193.400 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  181.2294158935547\n",
      "Epoch 590: : Loss: T_195.858 V_181.229 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  168.41822814941406\n",
      "Epoch 600: : Loss: T_187.610 V_168.418 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  161.00132751464844\n",
      "Epoch 610: : Loss: T_185.302 V_161.001 | Acc: T_0.000) V_0.000\n",
      "Epoch 620: : Loss: T_167.691 V_161.259 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  156.87890625\n",
      "Epoch 630: : Loss: T_162.200 V_156.879 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  144.42604064941406\n",
      "Epoch 640: : Loss: T_161.738 V_144.426 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  138.30186462402344\n",
      "Epoch 650: : Loss: T_155.252 V_138.302 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  132.13674926757812\n",
      "Epoch 660: : Loss: T_151.305 V_132.137 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  127.66804504394531\n",
      "Epoch 670: : Loss: T_135.431 V_127.668 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  120.40724182128906\n",
      "Epoch 680: : Loss: T_139.563 V_120.407 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  116.50421905517578\n",
      "Epoch 690: : Loss: T_140.680 V_116.504 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  113.33533477783203\n",
      "Epoch 700: : Loss: T_122.647 V_113.335 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  106.31696319580078\n",
      "Epoch 710: : Loss: T_125.094 V_106.317 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  100.4393310546875\n",
      "Epoch 720: : Loss: T_107.745 V_100.439 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  96.86659240722656\n",
      "Epoch 730: : Loss: T_125.231 V_96.867 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  89.85704040527344\n",
      "Epoch 740: : Loss: T_107.441 V_89.857 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  83.11588287353516\n",
      "Epoch 750: : Loss: T_105.332 V_83.116 | Acc: T_0.000) V_0.000\n",
      "Epoch 760: : Loss: T_99.632 V_83.865 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  79.5887680053711\n",
      "Epoch 770: : Loss: T_95.095 V_79.589 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  74.75785827636719\n",
      "Epoch 780: : Loss: T_90.590 V_74.758 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  72.44730377197266\n",
      "Epoch 790: : Loss: T_103.069 V_72.447 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  70.25033569335938\n",
      "Epoch 800: : Loss: T_85.997 V_70.250 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  68.44497680664062\n",
      "Epoch 810: : Loss: T_89.403 V_68.445 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  65.98311614990234\n",
      "Epoch 820: : Loss: T_85.456 V_65.983 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  61.252376556396484\n",
      "Epoch 830: : Loss: T_81.934 V_61.252 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  58.18523406982422\n",
      "Epoch 840: : Loss: T_81.470 V_58.185 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  55.078548431396484\n",
      "Epoch 850: : Loss: T_75.880 V_55.079 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  53.48710632324219\n",
      "Epoch 860: : Loss: T_73.670 V_53.487 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  51.386199951171875\n",
      "Epoch 870: : Loss: T_69.051 V_51.386 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  49.683719635009766\n",
      "Epoch 880: : Loss: T_61.652 V_49.684 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  46.86985778808594\n",
      "Epoch 890: : Loss: T_68.985 V_46.870 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  44.9238166809082\n",
      "Epoch 900: : Loss: T_67.287 V_44.924 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  44.590030670166016\n",
      "Epoch 910: : Loss: T_64.452 V_44.590 | Acc: T_0.000) V_0.000\n",
      "Epoch 920: : Loss: T_64.457 V_44.889 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  42.56490707397461\n",
      "Epoch 930: : Loss: T_65.241 V_42.565 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.26375961303711\n",
      "Epoch 940: : Loss: T_54.456 V_40.264 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.537864685058594\n",
      "Epoch 950: : Loss: T_62.801 V_38.538 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.04834747314453\n",
      "Epoch 960: : Loss: T_56.394 V_38.048 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.18661117553711\n",
      "Epoch 970: : Loss: T_59.471 V_37.187 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.503761291503906\n",
      "Epoch 980: : Loss: T_49.223 V_35.504 | Acc: T_0.000) V_0.000\n",
      "Epoch 990: : Loss: T_55.584 V_35.581 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.82283020019531\n",
      "Epoch 1000: : Loss: T_62.315 V_34.823 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.34324264526367\n",
      "Epoch 1010: : Loss: T_50.337 V_33.343 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.0655517578125\n",
      "Epoch 1020: : Loss: T_46.896 V_32.066 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.202362060546875\n",
      "Epoch 1030: : Loss: T_56.130 V_31.202 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.590538024902344\n",
      "Epoch 1040: : Loss: T_49.205 V_30.591 | Acc: T_0.000) V_0.000\n",
      "Epoch 1050: : Loss: T_46.185 V_30.771 | Acc: T_0.000) V_0.000\n",
      "Epoch 1060: : Loss: T_51.414 V_30.797 | Acc: T_0.000) V_0.000\n",
      "Epoch 1070: : Loss: T_53.323 V_31.645 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.712987899780273\n",
      "Epoch 1080: : Loss: T_52.975 V_29.713 | Acc: T_0.000) V_0.000\n",
      "Epoch 1090: : Loss: T_50.217 V_29.807 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.530019760131836\n",
      "Epoch 1100: : Loss: T_57.256 V_29.530 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.201570510864258\n",
      "Epoch 1110: : Loss: T_73.796 V_28.202 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.602449417114258\n",
      "Epoch 1120: : Loss: T_52.695 V_27.602 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_44.319 V_27.679 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_54.381 V_28.277 | Acc: T_0.000) V_0.000\n",
      "Epoch 1150: : Loss: T_52.974 V_28.150 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_52.617 V_27.756 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.592914581298828\n",
      "Epoch 1170: : Loss: T_45.339 V_27.593 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.185163497924805\n",
      "Epoch 1180: : Loss: T_45.976 V_27.185 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_50.028 V_27.342 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_55.876 V_27.291 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.623180389404297\n",
      "Epoch 1210: : Loss: T_45.825 V_26.623 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.483736038208008\n",
      "Epoch 1220: : Loss: T_49.654 V_26.484 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_57.570 V_26.819 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_55.783 V_27.137 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_45.793 V_26.745 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.246715545654297\n",
      "Epoch 1260: : Loss: T_41.154 V_26.247 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.18376922607422\n",
      "Epoch 1270: : Loss: T_50.080 V_26.184 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_45.867 V_26.932 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_50.155 V_27.151 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_42.655 V_26.751 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.671781539916992\n",
      "Epoch 1310: : Loss: T_43.189 V_25.672 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.54307746887207\n",
      "Epoch 1320: : Loss: T_51.827 V_25.543 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.267406463623047\n",
      "Epoch 1330: : Loss: T_42.484 V_25.267 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.799144744873047\n",
      "Epoch 1340: : Loss: T_43.837 V_24.799 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_53.338 V_25.147 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_44.400 V_25.587 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_47.413 V_25.708 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_50.321 V_25.473 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_49.526 V_25.097 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.64737892150879\n",
      "Epoch 1400: : Loss: T_56.951 V_24.647 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_52.281 V_24.694 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_45.796 V_24.786 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_47.833 V_25.314 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_44.885 V_25.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_50.395 V_25.945 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_43.334 V_26.045 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_44.567 V_25.992 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_48.649 V_26.806 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_54.814 V_26.094 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_45.430 V_25.887 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_46.473 V_26.752 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_49.741 V_26.359 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_54.982 V_26.107 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_51.605 V_26.443 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_46.930 V_26.102 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_47.415 V_26.090 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_44.083 V_26.106 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_39.765 V_25.769 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_49.494 V_25.721 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_49.404 V_25.509 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_40.804 V_25.712 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_45.229 V_25.817 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_42.145 V_25.704 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_45.988 V_25.529 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.636985778808594\n",
      "Epoch 1650: : Loss: T_41.814 V_24.637 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_43.625 V_25.549 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_43.486 V_25.616 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_44.611 V_25.255 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_44.104 V_25.653 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_47.801 V_26.008 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_44.577 V_25.536 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_49.863 V_25.229 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_48.544 V_25.630 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_43.700 V_25.529 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_50.958 V_25.597 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_39.832 V_25.744 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_51.785 V_25.392 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_49.052 V_25.237 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_49.033 V_25.742 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_42.991 V_25.722 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_48.880 V_25.759 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_47.384 V_25.590 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_45.070 V_26.117 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_42.007 V_25.864 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_49.206 V_26.109 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_43.740 V_26.509 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_44.809 V_26.194 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_47.902 V_25.754 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_50.912 V_25.736 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_50.613 V_25.730 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_49.429 V_25.593 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_48.658 V_25.796 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_40.039 V_26.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_43.898 V_26.792 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_44.548 V_26.675 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_51.451 V_26.791 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_43.506 V_26.843 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_57.456 V_26.637 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_55.325 V_26.251 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_37.680 V_25.620 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_48.049 V_25.848 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_44.763 V_26.809 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_51.206 V_27.212 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_39.278 V_27.323 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_53.142 V_26.769 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_50.283 V_26.460 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_47.726 V_26.621 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_51.568 V_26.572 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_44.425 V_26.570 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_49.119 V_26.361 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_51.283 V_26.125 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_41.697 V_26.795 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_51.753 V_27.236 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_45.901 V_26.736 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_49.888 V_26.439 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_47.082 V_26.542 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_36.829 V_27.231 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_41.420 V_27.721 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_54.072 V_26.973 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_37.514 V_26.260 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_40.469 V_26.222 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_44.239 V_26.036 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_44.126 V_26.557 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_48.024 V_26.972 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_38.989 V_26.842 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_47.539 V_26.608 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_49.445 V_27.019 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_54.241 V_26.601 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_42.865 V_26.707 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_42.376 V_26.891 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_44.383 V_26.513 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_35.954 V_26.387 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_48.657 V_26.579 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_43.775 V_27.168 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_46.015 V_27.589 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_44.236 V_27.573 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_43.955 V_27.636 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_49.259 V_27.624 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_45.310 V_27.796 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_51.014 V_27.624 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_39.664 V_27.367 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_46.221 V_26.974 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_42.340 V_26.610 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_48.483 V_27.102 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_49.385 V_27.897 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_44.403 V_27.156 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_48.324 V_27.618 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_46.810 V_27.002 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_49.837 V_26.715 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_47.555 V_27.521 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_46.929 V_27.230 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_49.560 V_27.098 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_49.491 V_27.748 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_49.854 V_28.046 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_39.933 V_27.373 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_47.894 V_27.441 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_53.353 V_28.263 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_44.001 V_28.295 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_40.962 V_28.157 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_47.536 V_27.507 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_52.689 V_27.230 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_42.185 V_27.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_53.823 V_27.484 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_40.866 V_27.222 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_47.340 V_26.555 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_53.349 V_26.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_44.850 V_27.114 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_47.242 V_27.345 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_48.514 V_27.053 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_45.938 V_26.798 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_38.768 V_26.937 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_38.808 V_27.615 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_43.952 V_26.768 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_37.541 V_26.746 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_41.504 V_27.089 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_42.896 V_27.457 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_48.842 V_26.463 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_47.625 V_26.488 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_41.286 V_26.816 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_36.884 V_26.936 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_41.361 V_26.970 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_43.879 V_26.880 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_49.016 V_27.264 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_42.689 V_26.743 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_39.364 V_26.936 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_46.184 V_27.340 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_50.377 V_26.984 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_47.725 V_27.447 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_46.211 V_28.451 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_47.095 V_27.701 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_45.748 V_27.782 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_47.945 V_27.318 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_50.301 V_27.325 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_43.387 V_26.875 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_41.315 V_26.960 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_38.151 V_27.232 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_45.063 V_28.267 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_44.535 V_27.933 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_43.921 V_27.459 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_47.707 V_26.802 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_50.457 V_26.967 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_45.688 V_26.886 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_38.388 V_26.532 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_40.523 V_26.646 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_38.354 V_27.371 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_38.246 V_27.306 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_49.365 V_27.666 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_43.442 V_28.488 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_44.409 V_27.852 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_47.492 V_27.740 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_45.003 V_27.351 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_38.536 V_26.639 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_40.202 V_26.964 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_43.474 V_26.860 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_46.199 V_27.447 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_49.379 V_26.533 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_42.756 V_26.502 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_43.860 V_27.276 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_51.896 V_27.911 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_44.104 V_27.335 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_49.986 V_27.367 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_48.473 V_26.699 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_38.878 V_27.554 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_40.042 V_27.559 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_41.356 V_28.393 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_42.937 V_28.255 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_35.528 V_27.598 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_47.476 V_27.796 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_39.346 V_27.666 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_42.248 V_28.033 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_41.053 V_27.818 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_44.636 V_27.658 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_50.630 V_27.813 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_41.979 V_27.306 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_47.718 V_26.476 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_42.588 V_26.282 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_35.236 V_26.780 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_36.995 V_26.823 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_40.556 V_27.576 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_39.877 V_27.183 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_46.104 V_27.349 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_38.274 V_27.590 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_42.606 V_26.954 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_37.377 V_26.943 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_48.909 V_26.867 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_42.569 V_27.025 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_43.815 V_26.919 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_46.303 V_26.747 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_44.123 V_26.860 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_42.418 V_27.326 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  588.3536987304688\n",
      "Epoch 010: : Loss: T_586.550 V_588.354 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  580.7032470703125\n",
      "Epoch 020: : Loss: T_582.255 V_580.703 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  575.7254638671875\n",
      "Epoch 030: : Loss: T_581.125 V_575.725 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  572.5090942382812\n",
      "Epoch 040: : Loss: T_581.624 V_572.509 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  570.7063598632812\n",
      "Epoch 050: : Loss: T_572.600 V_570.706 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  568.9124145507812\n",
      "Epoch 060: : Loss: T_566.446 V_568.912 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  566.8140258789062\n",
      "Epoch 070: : Loss: T_565.250 V_566.814 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  563.9030151367188\n",
      "Epoch 080: : Loss: T_560.475 V_563.903 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  557.495849609375\n",
      "Epoch 090: : Loss: T_554.244 V_557.496 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  553.9622192382812\n",
      "Epoch 100: : Loss: T_548.789 V_553.962 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  550.9810791015625\n",
      "Epoch 110: : Loss: T_545.476 V_550.981 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  546.990966796875\n",
      "Epoch 120: : Loss: T_539.448 V_546.991 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  544.5007934570312\n",
      "Epoch 130: : Loss: T_533.456 V_544.501 | Acc: T_0.000) V_0.000\n",
      "Epoch 140: : Loss: T_523.556 V_545.687 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  538.810791015625\n",
      "Epoch 150: : Loss: T_519.106 V_538.811 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  533.7305908203125\n",
      "Epoch 160: : Loss: T_506.226 V_533.731 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  529.3221435546875\n",
      "Epoch 170: : Loss: T_509.020 V_529.322 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  522.303466796875\n",
      "Epoch 180: : Loss: T_498.554 V_522.303 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  507.8011169433594\n",
      "Epoch 190: : Loss: T_488.220 V_507.801 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  495.098388671875\n",
      "Epoch 200: : Loss: T_489.438 V_495.098 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  488.98114013671875\n",
      "Epoch 210: : Loss: T_478.215 V_488.981 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  487.87689208984375\n",
      "Epoch 220: : Loss: T_467.020 V_487.877 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  487.121826171875\n",
      "Epoch 230: : Loss: T_469.654 V_487.122 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  476.4112548828125\n",
      "Epoch 240: : Loss: T_458.021 V_476.411 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  462.9159851074219\n",
      "Epoch 250: : Loss: T_445.696 V_462.916 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  450.4295959472656\n",
      "Epoch 260: : Loss: T_448.237 V_450.430 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  439.1559753417969\n",
      "Epoch 270: : Loss: T_436.308 V_439.156 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  433.360595703125\n",
      "Epoch 280: : Loss: T_423.067 V_433.361 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  420.49365234375\n",
      "Epoch 290: : Loss: T_422.071 V_420.494 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  399.1675720214844\n",
      "Epoch 300: : Loss: T_418.697 V_399.168 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  393.71124267578125\n",
      "Epoch 310: : Loss: T_411.901 V_393.711 | Acc: T_0.000) V_0.000\n",
      "Epoch 320: : Loss: T_395.076 V_394.656 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  381.7519226074219\n",
      "Epoch 330: : Loss: T_386.650 V_381.752 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  364.5141296386719\n",
      "Epoch 340: : Loss: T_385.668 V_364.514 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  360.56256103515625\n",
      "Epoch 350: : Loss: T_370.127 V_360.563 | Acc: T_0.000) V_0.000\n",
      "Epoch 360: : Loss: T_362.526 V_360.918 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  348.45745849609375\n",
      "Epoch 370: : Loss: T_348.717 V_348.457 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  333.9236755371094\n",
      "Epoch 380: : Loss: T_339.974 V_333.924 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  326.17987060546875\n",
      "Epoch 390: : Loss: T_329.822 V_326.180 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  306.8393859863281\n",
      "Epoch 400: : Loss: T_338.319 V_306.839 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  299.3970031738281\n",
      "Epoch 410: : Loss: T_327.153 V_299.397 | Acc: T_0.000) V_0.000\n",
      "Epoch 420: : Loss: T_309.045 V_305.623 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  294.2696533203125\n",
      "Epoch 430: : Loss: T_308.893 V_294.270 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  278.25921630859375\n",
      "Epoch 440: : Loss: T_305.845 V_278.259 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  266.463623046875\n",
      "Epoch 450: : Loss: T_285.143 V_266.464 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  255.18104553222656\n",
      "Epoch 460: : Loss: T_285.286 V_255.181 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  252.0472869873047\n",
      "Epoch 470: : Loss: T_278.441 V_252.047 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  247.1663360595703\n",
      "Epoch 480: : Loss: T_258.791 V_247.166 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  235.52734375\n",
      "Epoch 490: : Loss: T_250.666 V_235.527 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  233.5198211669922\n",
      "Epoch 500: : Loss: T_245.029 V_233.520 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  219.67532348632812\n",
      "Epoch 510: : Loss: T_247.144 V_219.675 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  217.0262908935547\n",
      "Epoch 520: : Loss: T_231.166 V_217.026 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  206.71031188964844\n",
      "Epoch 530: : Loss: T_215.628 V_206.710 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  191.7081756591797\n",
      "Epoch 540: : Loss: T_224.376 V_191.708 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  182.48716735839844\n",
      "Epoch 550: : Loss: T_217.863 V_182.487 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  179.13356018066406\n",
      "Epoch 560: : Loss: T_210.329 V_179.134 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  175.62319946289062\n",
      "Epoch 570: : Loss: T_189.692 V_175.623 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  166.76910400390625\n",
      "Epoch 580: : Loss: T_186.549 V_166.769 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  158.71768188476562\n",
      "Epoch 590: : Loss: T_194.016 V_158.718 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  150.13992309570312\n",
      "Epoch 600: : Loss: T_172.572 V_150.140 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  145.92315673828125\n",
      "Epoch 610: : Loss: T_168.937 V_145.923 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  138.35072326660156\n",
      "Epoch 620: : Loss: T_171.291 V_138.351 | Acc: T_0.000) V_0.000\n",
      "Epoch 630: : Loss: T_160.118 V_139.113 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  130.73524475097656\n",
      "Epoch 640: : Loss: T_152.819 V_130.735 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  121.80815887451172\n",
      "Epoch 650: : Loss: T_146.167 V_121.808 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  109.15897369384766\n",
      "Epoch 660: : Loss: T_149.487 V_109.159 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  107.13423919677734\n",
      "Epoch 670: : Loss: T_146.579 V_107.134 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  103.78760528564453\n",
      "Epoch 680: : Loss: T_141.220 V_103.788 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  100.22539520263672\n",
      "Epoch 690: : Loss: T_126.689 V_100.225 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  98.54850006103516\n",
      "Epoch 700: : Loss: T_123.307 V_98.549 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  91.28166198730469\n",
      "Epoch 710: : Loss: T_111.619 V_91.282 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  85.42852783203125\n",
      "Epoch 720: : Loss: T_119.536 V_85.429 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  80.67694854736328\n",
      "Epoch 730: : Loss: T_109.773 V_80.677 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  77.98209381103516\n",
      "Epoch 740: : Loss: T_100.333 V_77.982 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  72.40597534179688\n",
      "Epoch 750: : Loss: T_112.902 V_72.406 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  67.85517120361328\n",
      "Epoch 760: : Loss: T_98.602 V_67.855 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  65.83879852294922\n",
      "Epoch 770: : Loss: T_92.023 V_65.839 | Acc: T_0.000) V_0.000\n",
      "Epoch 780: : Loss: T_83.576 V_68.112 | Acc: T_0.000) V_0.000\n",
      "Epoch 790: : Loss: T_87.521 V_66.589 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  62.34441375732422\n",
      "Epoch 800: : Loss: T_98.072 V_62.344 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  57.66255569458008\n",
      "Epoch 810: : Loss: T_73.703 V_57.663 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  56.35115432739258\n",
      "Epoch 820: : Loss: T_83.960 V_56.351 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  53.511165618896484\n",
      "Epoch 830: : Loss: T_82.584 V_53.511 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  49.98991775512695\n",
      "Epoch 840: : Loss: T_70.349 V_49.990 | Acc: T_0.000) V_0.000\n",
      "Epoch 850: : Loss: T_75.084 V_50.096 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  48.160640716552734\n",
      "Epoch 860: : Loss: T_64.969 V_48.161 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  46.63436508178711\n",
      "Epoch 870: : Loss: T_64.278 V_46.634 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  44.74538803100586\n",
      "Epoch 880: : Loss: T_60.621 V_44.745 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  43.142066955566406\n",
      "Epoch 890: : Loss: T_72.167 V_43.142 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  41.59688949584961\n",
      "Epoch 900: : Loss: T_69.084 V_41.597 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.4427490234375\n",
      "Epoch 910: : Loss: T_60.022 V_40.443 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.0797004699707\n",
      "Epoch 920: : Loss: T_54.583 V_40.080 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.30445861816406\n",
      "Epoch 930: : Loss: T_52.694 V_39.304 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  36.48213577270508\n",
      "Epoch 940: : Loss: T_63.011 V_36.482 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.06218719482422\n",
      "Epoch 950: : Loss: T_62.000 V_35.062 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.51585006713867\n",
      "Epoch 960: : Loss: T_59.515 V_34.516 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.781654357910156\n",
      "Epoch 970: : Loss: T_52.082 V_33.782 | Acc: T_0.000) V_0.000\n",
      "Epoch 980: : Loss: T_49.640 V_33.885 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.62596893310547\n",
      "Epoch 990: : Loss: T_55.592 V_33.626 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.07272720336914\n",
      "Epoch 1000: : Loss: T_50.927 V_33.073 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.622318267822266\n",
      "Epoch 1010: : Loss: T_57.713 V_32.622 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.714277267456055\n",
      "Epoch 1020: : Loss: T_73.185 V_31.714 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.20545768737793\n",
      "Epoch 1030: : Loss: T_47.819 V_31.205 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.124387741088867\n",
      "Epoch 1040: : Loss: T_42.907 V_31.124 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.272497177124023\n",
      "Epoch 1050: : Loss: T_49.016 V_30.272 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.80866241455078\n",
      "Epoch 1060: : Loss: T_56.053 V_29.809 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.600046157836914\n",
      "Epoch 1070: : Loss: T_50.255 V_29.600 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.19273567199707\n",
      "Epoch 1080: : Loss: T_54.635 V_29.193 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.418930053710938\n",
      "Epoch 1090: : Loss: T_58.060 V_28.419 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.900482177734375\n",
      "Epoch 1100: : Loss: T_50.494 V_27.900 | Acc: T_0.000) V_0.000\n",
      "Epoch 1110: : Loss: T_45.765 V_28.506 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_48.995 V_28.569 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_53.922 V_28.502 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_47.950 V_28.568 | Acc: T_0.000) V_0.000\n",
      "Epoch 1150: : Loss: T_51.473 V_28.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 1160: : Loss: T_54.866 V_28.140 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.549795150756836\n",
      "Epoch 1170: : Loss: T_51.165 V_27.550 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.208375930786133\n",
      "Epoch 1180: : Loss: T_56.566 V_27.208 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_50.998 V_27.695 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_47.928 V_28.070 | Acc: T_0.000) V_0.000\n",
      "Epoch 1210: : Loss: T_53.720 V_27.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 1220: : Loss: T_55.632 V_27.606 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_47.817 V_27.295 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_49.609 V_27.621 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_49.648 V_28.042 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_45.239 V_27.510 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.322742462158203\n",
      "Epoch 1270: : Loss: T_49.996 V_26.323 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.412580490112305\n",
      "Epoch 1280: : Loss: T_45.544 V_25.413 | Acc: T_0.000) V_0.000\n",
      "Epoch 1290: : Loss: T_52.202 V_26.496 | Acc: T_0.000) V_0.000\n",
      "Epoch 1300: : Loss: T_51.430 V_27.194 | Acc: T_0.000) V_0.000\n",
      "Epoch 1310: : Loss: T_47.454 V_26.813 | Acc: T_0.000) V_0.000\n",
      "Epoch 1320: : Loss: T_47.764 V_26.623 | Acc: T_0.000) V_0.000\n",
      "Epoch 1330: : Loss: T_44.525 V_25.910 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_51.742 V_25.902 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_54.998 V_26.456 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_45.907 V_26.473 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_51.067 V_26.377 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_39.221 V_26.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_45.082 V_26.262 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_48.363 V_25.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_41.240 V_25.657 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_50.275 V_25.741 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_50.131 V_25.851 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_42.288 V_26.051 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_47.575 V_25.911 | Acc: T_0.000) V_0.000\n",
      "Epoch 1460: : Loss: T_47.918 V_26.271 | Acc: T_0.000) V_0.000\n",
      "Epoch 1470: : Loss: T_43.799 V_26.356 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_48.119 V_26.285 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_41.545 V_25.647 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_44.593 V_25.968 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_56.808 V_25.698 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_45.612 V_26.217 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_48.846 V_26.076 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_45.436 V_25.871 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.36441993713379\n",
      "Epoch 1550: : Loss: T_48.575 V_25.364 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_50.487 V_25.880 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_47.001 V_26.214 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_45.597 V_26.051 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_49.968 V_25.723 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.310443878173828\n",
      "Epoch 1600: : Loss: T_45.836 V_25.310 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.238208770751953\n",
      "Epoch 1610: : Loss: T_41.680 V_25.238 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_44.121 V_25.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_47.020 V_25.505 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_49.051 V_25.813 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_47.974 V_25.992 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_53.729 V_26.125 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_44.483 V_26.049 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_43.326 V_26.055 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_49.722 V_25.791 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_42.337 V_26.155 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_50.718 V_26.774 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_51.084 V_26.721 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_48.535 V_25.879 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_46.146 V_25.649 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_44.340 V_25.911 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_46.117 V_26.569 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_46.302 V_26.805 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_38.569 V_26.761 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_43.946 V_26.340 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_52.485 V_26.554 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_42.022 V_26.683 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_53.075 V_26.571 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_47.974 V_26.774 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_40.171 V_27.089 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_48.451 V_27.109 | Acc: T_0.000) V_0.000\n",
      "Epoch 1860: : Loss: T_44.261 V_27.126 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_41.829 V_26.527 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_49.350 V_26.752 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_47.366 V_26.630 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_44.833 V_26.474 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_45.836 V_26.803 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_45.891 V_26.948 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_43.478 V_26.493 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_47.832 V_26.100 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_46.820 V_25.941 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_48.096 V_25.784 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_52.407 V_26.014 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_52.904 V_25.628 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_44.725 V_25.651 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_42.098 V_25.641 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_50.183 V_25.370 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.037368774414062\n",
      "Epoch 2020: : Loss: T_53.532 V_25.037 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_43.511 V_25.232 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_42.294 V_25.854 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_49.992 V_25.955 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_53.995 V_25.945 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_36.999 V_26.005 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_45.775 V_26.111 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_45.820 V_26.594 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_41.658 V_26.755 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_48.545 V_26.071 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_47.326 V_26.013 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_43.167 V_25.879 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_42.787 V_25.972 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_43.150 V_25.934 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_41.935 V_25.687 | Acc: T_0.000) V_0.000\n",
      "Epoch 2170: : Loss: T_51.398 V_25.549 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_47.474 V_25.352 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_41.157 V_25.501 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_43.697 V_25.664 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_50.572 V_25.838 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_54.091 V_25.802 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_42.499 V_25.773 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_39.512 V_25.173 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_46.298 V_25.249 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_50.133 V_25.601 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_47.807 V_25.898 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_49.272 V_26.253 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_40.722 V_26.047 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_39.271 V_25.825 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_47.409 V_25.697 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_45.608 V_26.332 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_39.803 V_25.922 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_55.207 V_26.024 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_39.440 V_26.152 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_40.058 V_25.869 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_46.877 V_25.790 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_47.049 V_25.581 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_42.764 V_25.533 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_38.151 V_26.248 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_44.907 V_26.148 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_44.809 V_25.888 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_45.693 V_25.466 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_42.399 V_25.967 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_44.129 V_26.048 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_42.168 V_25.100 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.7529239654541\n",
      "Epoch 2470: : Loss: T_48.791 V_24.753 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_42.985 V_24.955 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_50.334 V_25.241 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_39.533 V_25.776 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_38.882 V_25.738 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_40.515 V_25.188 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_47.199 V_25.666 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_44.349 V_25.650 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_38.778 V_25.236 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_40.318 V_25.568 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_49.643 V_25.444 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_39.223 V_25.631 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_44.250 V_25.359 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_47.871 V_25.253 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_39.424 V_25.495 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_38.171 V_25.363 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_34.594 V_25.809 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_38.230 V_26.239 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_42.902 V_26.347 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_43.844 V_26.110 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_40.385 V_25.994 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_40.335 V_26.220 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_40.886 V_26.614 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_45.077 V_26.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_42.055 V_26.248 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_53.128 V_26.346 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_45.201 V_27.072 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_40.785 V_26.497 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_44.428 V_26.582 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_40.807 V_26.594 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_43.720 V_26.735 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_41.251 V_26.393 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_45.064 V_25.828 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_34.533 V_25.677 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_45.003 V_25.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_51.347 V_25.456 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_43.488 V_26.284 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_43.360 V_25.779 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_37.789 V_26.171 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_47.293 V_26.031 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_44.019 V_25.565 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_42.888 V_25.685 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_38.318 V_26.460 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_51.666 V_26.442 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_47.146 V_26.381 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_46.039 V_26.738 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_52.750 V_26.859 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_39.947 V_26.099 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_44.667 V_25.543 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_45.661 V_25.276 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_49.627 V_25.523 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_42.382 V_25.448 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_35.901 V_25.360 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_45.289 V_25.438 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_43.689 V_25.515 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_39.302 V_25.753 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_56.753 V_25.653 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_38.517 V_24.884 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.748790740966797\n",
      "Epoch 3050: : Loss: T_41.157 V_24.749 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_47.580 V_24.817 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_45.025 V_24.998 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_40.383 V_25.933 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_42.955 V_26.504 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_43.484 V_25.814 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_38.049 V_25.422 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_43.176 V_25.864 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_38.149 V_25.720 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_41.978 V_26.037 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_44.633 V_26.101 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_45.153 V_26.292 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_45.505 V_26.117 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_35.193 V_26.013 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_36.635 V_25.995 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_46.352 V_26.471 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_44.137 V_26.648 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_53.198 V_26.495 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_44.228 V_26.420 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_52.116 V_26.270 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_43.425 V_26.365 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_39.559 V_26.045 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_46.051 V_25.487 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_43.360 V_25.716 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_40.642 V_26.464 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_49.001 V_26.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_41.917 V_26.377 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_43.366 V_26.424 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_44.584 V_26.575 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_34.900 V_26.030 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_40.380 V_25.766 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_46.131 V_26.191 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_35.703 V_26.630 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_40.387 V_26.715 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_40.537 V_26.678 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_49.463 V_26.577 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_40.308 V_26.614 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_39.195 V_26.583 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_40.589 V_26.276 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_41.911 V_26.198 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_45.889 V_26.387 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_41.028 V_25.997 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_40.351 V_26.103 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_40.826 V_25.988 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_44.084 V_26.312 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_36.955 V_25.628 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  602.8251953125\n",
      "Epoch 010: : Loss: T_593.308 V_602.825 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  596.2520751953125\n",
      "Epoch 020: : Loss: T_589.255 V_596.252 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  591.9368286132812\n",
      "Epoch 030: : Loss: T_581.078 V_591.937 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  587.4427490234375\n",
      "Epoch 040: : Loss: T_578.800 V_587.443 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  582.7428588867188\n",
      "Epoch 050: : Loss: T_576.014 V_582.743 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  576.6386108398438\n",
      "Epoch 060: : Loss: T_571.309 V_576.639 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  571.4064331054688\n",
      "Epoch 070: : Loss: T_569.430 V_571.406 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  568.6729125976562\n",
      "Epoch 080: : Loss: T_560.231 V_568.673 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  560.5718383789062\n",
      "Epoch 090: : Loss: T_555.953 V_560.572 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  552.0929565429688\n",
      "Epoch 100: : Loss: T_554.163 V_552.093 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  545.9642944335938\n",
      "Epoch 110: : Loss: T_546.825 V_545.964 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  542.3567504882812\n",
      "Epoch 120: : Loss: T_537.973 V_542.357 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  537.6407470703125\n",
      "Epoch 130: : Loss: T_533.001 V_537.641 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  533.3345947265625\n",
      "Epoch 140: : Loss: T_528.489 V_533.335 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  522.01904296875\n",
      "Epoch 150: : Loss: T_521.053 V_522.019 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  516.38134765625\n",
      "Epoch 160: : Loss: T_515.029 V_516.381 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  511.8974914550781\n",
      "Epoch 170: : Loss: T_502.393 V_511.897 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  506.53271484375\n",
      "Epoch 180: : Loss: T_497.065 V_506.533 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  502.015625\n",
      "Epoch 190: : Loss: T_488.615 V_502.016 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  488.7791748046875\n",
      "Epoch 200: : Loss: T_493.583 V_488.779 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  480.82794189453125\n",
      "Epoch 210: : Loss: T_480.910 V_480.828 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  468.98583984375\n",
      "Epoch 220: : Loss: T_464.434 V_468.986 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  461.38818359375\n",
      "Epoch 230: : Loss: T_455.036 V_461.388 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  452.6617126464844\n",
      "Epoch 240: : Loss: T_459.339 V_452.662 | Acc: T_0.000) V_0.000\n",
      "Epoch 250: : Loss: T_456.228 V_453.453 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  442.3730163574219\n",
      "Epoch 260: : Loss: T_444.312 V_442.373 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  431.69390869140625\n",
      "Epoch 270: : Loss: T_433.833 V_431.694 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  419.2590637207031\n",
      "Epoch 280: : Loss: T_428.146 V_419.259 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  410.23638916015625\n",
      "Epoch 290: : Loss: T_409.975 V_410.236 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  401.820556640625\n",
      "Epoch 300: : Loss: T_419.119 V_401.821 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  395.404052734375\n",
      "Epoch 310: : Loss: T_405.797 V_395.404 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  388.74615478515625\n",
      "Epoch 320: : Loss: T_387.267 V_388.746 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  373.2337341308594\n",
      "Epoch 330: : Loss: T_380.356 V_373.234 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  364.5080871582031\n",
      "Epoch 340: : Loss: T_374.421 V_364.508 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  359.3302001953125\n",
      "Epoch 350: : Loss: T_370.854 V_359.330 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  355.5823059082031\n",
      "Epoch 360: : Loss: T_368.124 V_355.582 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  342.00030517578125\n",
      "Epoch 370: : Loss: T_368.205 V_342.000 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  326.48760986328125\n",
      "Epoch 380: : Loss: T_347.669 V_326.488 | Acc: T_0.000) V_0.000\n",
      "Epoch 390: : Loss: T_341.845 V_328.054 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  320.4636535644531\n",
      "Epoch 400: : Loss: T_327.198 V_320.464 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  301.8183898925781\n",
      "Epoch 410: : Loss: T_323.782 V_301.818 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  299.8099670410156\n",
      "Epoch 420: : Loss: T_330.684 V_299.810 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  288.7521057128906\n",
      "Epoch 430: : Loss: T_302.044 V_288.752 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  284.3254089355469\n",
      "Epoch 440: : Loss: T_304.087 V_284.325 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  277.9715881347656\n",
      "Epoch 450: : Loss: T_295.368 V_277.972 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  271.543212890625\n",
      "Epoch 460: : Loss: T_289.466 V_271.543 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  255.30191040039062\n",
      "Epoch 470: : Loss: T_272.705 V_255.302 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  247.81150817871094\n",
      "Epoch 480: : Loss: T_263.502 V_247.812 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  240.9394989013672\n",
      "Epoch 490: : Loss: T_269.956 V_240.939 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  236.9884033203125\n",
      "Epoch 500: : Loss: T_257.753 V_236.988 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  226.88426208496094\n",
      "Epoch 510: : Loss: T_254.352 V_226.884 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  212.6072540283203\n",
      "Epoch 520: : Loss: T_245.733 V_212.607 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  203.00352478027344\n",
      "Epoch 530: : Loss: T_225.641 V_203.004 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  198.9003143310547\n",
      "Epoch 540: : Loss: T_232.394 V_198.900 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  193.55850219726562\n",
      "Epoch 550: : Loss: T_209.509 V_193.559 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  179.95008850097656\n",
      "Epoch 560: : Loss: T_213.251 V_179.950 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  175.6002197265625\n",
      "Epoch 570: : Loss: T_200.529 V_175.600 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  166.49044799804688\n",
      "Epoch 580: : Loss: T_186.137 V_166.490 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  161.2891387939453\n",
      "Epoch 590: : Loss: T_188.914 V_161.289 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  155.097412109375\n",
      "Epoch 600: : Loss: T_175.938 V_155.097 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  149.148681640625\n",
      "Epoch 610: : Loss: T_172.114 V_149.149 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  140.974609375\n",
      "Epoch 620: : Loss: T_174.189 V_140.975 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  132.41529846191406\n",
      "Epoch 630: : Loss: T_152.202 V_132.415 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  127.39010620117188\n",
      "Epoch 640: : Loss: T_161.265 V_127.390 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  124.0910873413086\n",
      "Epoch 650: : Loss: T_152.222 V_124.091 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  115.58731079101562\n",
      "Epoch 660: : Loss: T_144.801 V_115.587 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  108.3741226196289\n",
      "Epoch 670: : Loss: T_134.319 V_108.374 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  103.70083618164062\n",
      "Epoch 680: : Loss: T_130.416 V_103.701 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  100.5116958618164\n",
      "Epoch 690: : Loss: T_133.610 V_100.512 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  94.96416473388672\n",
      "Epoch 700: : Loss: T_121.253 V_94.964 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  90.79585266113281\n",
      "Epoch 710: : Loss: T_111.741 V_90.796 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  85.61441040039062\n",
      "Epoch 720: : Loss: T_126.996 V_85.614 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  82.93889617919922\n",
      "Epoch 730: : Loss: T_111.636 V_82.939 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  82.65204620361328\n",
      "Epoch 740: : Loss: T_100.998 V_82.652 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  78.94489288330078\n",
      "Epoch 750: : Loss: T_93.786 V_78.945 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  72.84920501708984\n",
      "Epoch 760: : Loss: T_95.048 V_72.849 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  68.55429077148438\n",
      "Epoch 770: : Loss: T_100.535 V_68.554 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  65.60881042480469\n",
      "Epoch 780: : Loss: T_96.088 V_65.609 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  61.272212982177734\n",
      "Epoch 790: : Loss: T_79.368 V_61.272 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  59.0146369934082\n",
      "Epoch 800: : Loss: T_85.863 V_59.015 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  58.08672332763672\n",
      "Epoch 810: : Loss: T_82.323 V_58.087 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  56.045867919921875\n",
      "Epoch 820: : Loss: T_83.150 V_56.046 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  54.34331512451172\n",
      "Epoch 830: : Loss: T_71.264 V_54.343 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  52.75706481933594\n",
      "Epoch 840: : Loss: T_76.864 V_52.757 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  49.07253646850586\n",
      "Epoch 850: : Loss: T_72.458 V_49.073 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  47.93437194824219\n",
      "Epoch 860: : Loss: T_68.986 V_47.934 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  46.27341842651367\n",
      "Epoch 870: : Loss: T_68.606 V_46.273 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  43.72492218017578\n",
      "Epoch 880: : Loss: T_80.598 V_43.725 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.7824821472168\n",
      "Epoch 890: : Loss: T_67.887 V_40.782 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  40.74324417114258\n",
      "Epoch 900: : Loss: T_70.970 V_40.743 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  39.293643951416016\n",
      "Epoch 910: : Loss: T_56.880 V_39.294 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  38.58283233642578\n",
      "Epoch 920: : Loss: T_62.955 V_38.583 | Acc: T_0.000) V_0.000\n",
      "Epoch 930: : Loss: T_55.867 V_38.721 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  37.09809112548828\n",
      "Epoch 940: : Loss: T_58.924 V_37.098 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  35.60326385498047\n",
      "Epoch 950: : Loss: T_59.373 V_35.603 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  34.20303726196289\n",
      "Epoch 960: : Loss: T_53.267 V_34.203 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  33.3740119934082\n",
      "Epoch 970: : Loss: T_56.763 V_33.374 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.99726486206055\n",
      "Epoch 980: : Loss: T_51.977 V_32.997 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  32.02021408081055\n",
      "Epoch 990: : Loss: T_54.809 V_32.020 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  31.51302146911621\n",
      "Epoch 1000: : Loss: T_51.618 V_31.513 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.676742553710938\n",
      "Epoch 1010: : Loss: T_53.091 V_30.677 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.620241165161133\n",
      "Epoch 1020: : Loss: T_49.291 V_30.620 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  30.608184814453125\n",
      "Epoch 1030: : Loss: T_54.577 V_30.608 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.796724319458008\n",
      "Epoch 1040: : Loss: T_51.991 V_29.797 | Acc: T_0.000) V_0.000\n",
      "Epoch 1050: : Loss: T_51.756 V_29.825 | Acc: T_0.000) V_0.000\n",
      "Epoch 1060: : Loss: T_58.850 V_29.887 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  29.48990249633789\n",
      "Epoch 1070: : Loss: T_56.107 V_29.490 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  28.542699813842773\n",
      "Epoch 1080: : Loss: T_48.851 V_28.543 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  27.553434371948242\n",
      "Epoch 1090: : Loss: T_57.232 V_27.553 | Acc: T_0.000) V_0.000\n",
      "Epoch 1100: : Loss: T_53.785 V_27.989 | Acc: T_0.000) V_0.000\n",
      "Epoch 1110: : Loss: T_55.870 V_28.223 | Acc: T_0.000) V_0.000\n",
      "Epoch 1120: : Loss: T_46.524 V_28.242 | Acc: T_0.000) V_0.000\n",
      "Epoch 1130: : Loss: T_45.888 V_27.645 | Acc: T_0.000) V_0.000\n",
      "Epoch 1140: : Loss: T_48.429 V_27.590 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.970760345458984\n",
      "Epoch 1150: : Loss: T_51.931 V_26.971 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.750837326049805\n",
      "Epoch 1160: : Loss: T_45.764 V_26.751 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.53927993774414\n",
      "Epoch 1170: : Loss: T_50.690 V_26.539 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.07999038696289\n",
      "Epoch 1180: : Loss: T_54.697 V_26.080 | Acc: T_0.000) V_0.000\n",
      "Epoch 1190: : Loss: T_48.082 V_26.378 | Acc: T_0.000) V_0.000\n",
      "Epoch 1200: : Loss: T_45.668 V_26.644 | Acc: T_0.000) V_0.000\n",
      "Epoch 1210: : Loss: T_59.120 V_26.836 | Acc: T_0.000) V_0.000\n",
      "Epoch 1220: : Loss: T_48.815 V_26.849 | Acc: T_0.000) V_0.000\n",
      "Epoch 1230: : Loss: T_50.213 V_26.871 | Acc: T_0.000) V_0.000\n",
      "Epoch 1240: : Loss: T_45.930 V_27.094 | Acc: T_0.000) V_0.000\n",
      "Epoch 1250: : Loss: T_45.156 V_26.588 | Acc: T_0.000) V_0.000\n",
      "Epoch 1260: : Loss: T_46.990 V_26.380 | Acc: T_0.000) V_0.000\n",
      "Epoch 1270: : Loss: T_49.733 V_26.227 | Acc: T_0.000) V_0.000\n",
      "Epoch 1280: : Loss: T_48.743 V_26.115 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  26.033851623535156\n",
      "Epoch 1290: : Loss: T_51.472 V_26.034 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.880727767944336\n",
      "Epoch 1300: : Loss: T_39.444 V_25.881 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.81646156311035\n",
      "Epoch 1310: : Loss: T_48.933 V_25.816 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  25.5346736907959\n",
      "Epoch 1320: : Loss: T_38.643 V_25.535 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.925386428833008\n",
      "Epoch 1330: : Loss: T_43.577 V_24.925 | Acc: T_0.000) V_0.000\n",
      "Epoch 1340: : Loss: T_43.482 V_25.098 | Acc: T_0.000) V_0.000\n",
      "Epoch 1350: : Loss: T_52.086 V_25.422 | Acc: T_0.000) V_0.000\n",
      "Epoch 1360: : Loss: T_42.774 V_25.581 | Acc: T_0.000) V_0.000\n",
      "Epoch 1370: : Loss: T_53.485 V_25.545 | Acc: T_0.000) V_0.000\n",
      "Epoch 1380: : Loss: T_39.567 V_25.597 | Acc: T_0.000) V_0.000\n",
      "Epoch 1390: : Loss: T_56.262 V_25.898 | Acc: T_0.000) V_0.000\n",
      "Epoch 1400: : Loss: T_51.178 V_26.170 | Acc: T_0.000) V_0.000\n",
      "Epoch 1410: : Loss: T_40.350 V_26.011 | Acc: T_0.000) V_0.000\n",
      "Epoch 1420: : Loss: T_48.584 V_25.562 | Acc: T_0.000) V_0.000\n",
      "Epoch 1430: : Loss: T_45.297 V_25.212 | Acc: T_0.000) V_0.000\n",
      "Epoch 1440: : Loss: T_50.079 V_25.389 | Acc: T_0.000) V_0.000\n",
      "Epoch 1450: : Loss: T_48.692 V_24.996 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.851537704467773\n",
      "Epoch 1460: : Loss: T_49.183 V_24.852 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.691892623901367\n",
      "Epoch 1470: : Loss: T_51.567 V_24.692 | Acc: T_0.000) V_0.000\n",
      "Epoch 1480: : Loss: T_48.565 V_25.001 | Acc: T_0.000) V_0.000\n",
      "Epoch 1490: : Loss: T_40.155 V_25.179 | Acc: T_0.000) V_0.000\n",
      "Epoch 1500: : Loss: T_57.051 V_24.984 | Acc: T_0.000) V_0.000\n",
      "Epoch 1510: : Loss: T_45.395 V_25.211 | Acc: T_0.000) V_0.000\n",
      "Epoch 1520: : Loss: T_45.231 V_25.524 | Acc: T_0.000) V_0.000\n",
      "Epoch 1530: : Loss: T_48.306 V_25.292 | Acc: T_0.000) V_0.000\n",
      "Epoch 1540: : Loss: T_40.405 V_25.193 | Acc: T_0.000) V_0.000\n",
      "Epoch 1550: : Loss: T_43.879 V_25.368 | Acc: T_0.000) V_0.000\n",
      "Epoch 1560: : Loss: T_44.981 V_25.510 | Acc: T_0.000) V_0.000\n",
      "Epoch 1570: : Loss: T_50.456 V_25.335 | Acc: T_0.000) V_0.000\n",
      "Epoch 1580: : Loss: T_48.396 V_25.282 | Acc: T_0.000) V_0.000\n",
      "Epoch 1590: : Loss: T_51.214 V_25.193 | Acc: T_0.000) V_0.000\n",
      "Epoch 1600: : Loss: T_47.865 V_25.310 | Acc: T_0.000) V_0.000\n",
      "Epoch 1610: : Loss: T_47.294 V_25.410 | Acc: T_0.000) V_0.000\n",
      "Epoch 1620: : Loss: T_41.939 V_25.074 | Acc: T_0.000) V_0.000\n",
      "Epoch 1630: : Loss: T_46.900 V_24.746 | Acc: T_0.000) V_0.000\n",
      "Epoch 1640: : Loss: T_48.656 V_25.035 | Acc: T_0.000) V_0.000\n",
      "Epoch 1650: : Loss: T_51.030 V_25.391 | Acc: T_0.000) V_0.000\n",
      "Epoch 1660: : Loss: T_39.021 V_25.007 | Acc: T_0.000) V_0.000\n",
      "Epoch 1670: : Loss: T_42.743 V_25.080 | Acc: T_0.000) V_0.000\n",
      "Epoch 1680: : Loss: T_44.308 V_24.882 | Acc: T_0.000) V_0.000\n",
      "Epoch 1690: : Loss: T_49.383 V_24.932 | Acc: T_0.000) V_0.000\n",
      "Epoch 1700: : Loss: T_52.034 V_25.291 | Acc: T_0.000) V_0.000\n",
      "Epoch 1710: : Loss: T_44.004 V_25.687 | Acc: T_0.000) V_0.000\n",
      "Epoch 1720: : Loss: T_42.108 V_25.254 | Acc: T_0.000) V_0.000\n",
      "Epoch 1730: : Loss: T_45.610 V_24.912 | Acc: T_0.000) V_0.000\n",
      "Epoch 1740: : Loss: T_53.162 V_24.930 | Acc: T_0.000) V_0.000\n",
      "Epoch 1750: : Loss: T_43.239 V_25.552 | Acc: T_0.000) V_0.000\n",
      "Epoch 1760: : Loss: T_38.876 V_26.003 | Acc: T_0.000) V_0.000\n",
      "Epoch 1770: : Loss: T_46.918 V_25.491 | Acc: T_0.000) V_0.000\n",
      "Epoch 1780: : Loss: T_36.404 V_25.037 | Acc: T_0.000) V_0.000\n",
      "Epoch 1790: : Loss: T_48.715 V_25.036 | Acc: T_0.000) V_0.000\n",
      "Epoch 1800: : Loss: T_49.954 V_25.217 | Acc: T_0.000) V_0.000\n",
      "Epoch 1810: : Loss: T_43.571 V_25.258 | Acc: T_0.000) V_0.000\n",
      "Epoch 1820: : Loss: T_52.499 V_24.754 | Acc: T_0.000) V_0.000\n",
      "Epoch 1830: : Loss: T_52.713 V_24.827 | Acc: T_0.000) V_0.000\n",
      "Epoch 1840: : Loss: T_43.801 V_25.043 | Acc: T_0.000) V_0.000\n",
      "Epoch 1850: : Loss: T_51.039 V_25.039 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.672948837280273\n",
      "Epoch 1860: : Loss: T_42.418 V_24.673 | Acc: T_0.000) V_0.000\n",
      "Epoch 1870: : Loss: T_51.209 V_24.897 | Acc: T_0.000) V_0.000\n",
      "Epoch 1880: : Loss: T_49.332 V_25.437 | Acc: T_0.000) V_0.000\n",
      "Epoch 1890: : Loss: T_48.624 V_25.929 | Acc: T_0.000) V_0.000\n",
      "Epoch 1900: : Loss: T_51.219 V_26.170 | Acc: T_0.000) V_0.000\n",
      "Epoch 1910: : Loss: T_50.509 V_26.091 | Acc: T_0.000) V_0.000\n",
      "Epoch 1920: : Loss: T_41.806 V_25.951 | Acc: T_0.000) V_0.000\n",
      "Epoch 1930: : Loss: T_48.256 V_25.834 | Acc: T_0.000) V_0.000\n",
      "Epoch 1940: : Loss: T_52.566 V_25.944 | Acc: T_0.000) V_0.000\n",
      "Epoch 1950: : Loss: T_50.988 V_26.176 | Acc: T_0.000) V_0.000\n",
      "Epoch 1960: : Loss: T_48.888 V_26.217 | Acc: T_0.000) V_0.000\n",
      "Epoch 1970: : Loss: T_44.657 V_26.035 | Acc: T_0.000) V_0.000\n",
      "Epoch 1980: : Loss: T_43.614 V_25.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 1990: : Loss: T_47.051 V_25.381 | Acc: T_0.000) V_0.000\n",
      "Epoch 2000: : Loss: T_54.914 V_25.767 | Acc: T_0.000) V_0.000\n",
      "Epoch 2010: : Loss: T_40.375 V_26.408 | Acc: T_0.000) V_0.000\n",
      "Epoch 2020: : Loss: T_47.128 V_25.861 | Acc: T_0.000) V_0.000\n",
      "Epoch 2030: : Loss: T_52.193 V_25.562 | Acc: T_0.000) V_0.000\n",
      "Epoch 2040: : Loss: T_45.041 V_25.239 | Acc: T_0.000) V_0.000\n",
      "Epoch 2050: : Loss: T_49.422 V_25.341 | Acc: T_0.000) V_0.000\n",
      "Epoch 2060: : Loss: T_52.665 V_25.106 | Acc: T_0.000) V_0.000\n",
      "Epoch 2070: : Loss: T_46.582 V_25.292 | Acc: T_0.000) V_0.000\n",
      "Epoch 2080: : Loss: T_46.128 V_25.576 | Acc: T_0.000) V_0.000\n",
      "Epoch 2090: : Loss: T_44.879 V_25.152 | Acc: T_0.000) V_0.000\n",
      "Epoch 2100: : Loss: T_46.085 V_24.699 | Acc: T_0.000) V_0.000\n",
      "Epoch 2110: : Loss: T_47.912 V_24.969 | Acc: T_0.000) V_0.000\n",
      "Epoch 2120: : Loss: T_45.954 V_25.281 | Acc: T_0.000) V_0.000\n",
      "Epoch 2130: : Loss: T_58.214 V_25.173 | Acc: T_0.000) V_0.000\n",
      "Epoch 2140: : Loss: T_50.497 V_24.926 | Acc: T_0.000) V_0.000\n",
      "Epoch 2150: : Loss: T_39.739 V_24.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 2160: : Loss: T_49.041 V_24.800 | Acc: T_0.000) V_0.000\n",
      "Best Model is copied - Best Loss :  24.40091896057129\n",
      "Epoch 2170: : Loss: T_46.612 V_24.401 | Acc: T_0.000) V_0.000\n",
      "Epoch 2180: : Loss: T_51.992 V_25.067 | Acc: T_0.000) V_0.000\n",
      "Epoch 2190: : Loss: T_47.011 V_25.310 | Acc: T_0.000) V_0.000\n",
      "Epoch 2200: : Loss: T_42.862 V_25.800 | Acc: T_0.000) V_0.000\n",
      "Epoch 2210: : Loss: T_47.253 V_25.474 | Acc: T_0.000) V_0.000\n",
      "Epoch 2220: : Loss: T_40.628 V_25.291 | Acc: T_0.000) V_0.000\n",
      "Epoch 2230: : Loss: T_45.688 V_25.652 | Acc: T_0.000) V_0.000\n",
      "Epoch 2240: : Loss: T_47.314 V_26.059 | Acc: T_0.000) V_0.000\n",
      "Epoch 2250: : Loss: T_43.106 V_25.937 | Acc: T_0.000) V_0.000\n",
      "Epoch 2260: : Loss: T_44.169 V_25.682 | Acc: T_0.000) V_0.000\n",
      "Epoch 2270: : Loss: T_44.865 V_25.436 | Acc: T_0.000) V_0.000\n",
      "Epoch 2280: : Loss: T_38.904 V_24.951 | Acc: T_0.000) V_0.000\n",
      "Epoch 2290: : Loss: T_46.474 V_24.925 | Acc: T_0.000) V_0.000\n",
      "Epoch 2300: : Loss: T_46.368 V_24.871 | Acc: T_0.000) V_0.000\n",
      "Epoch 2310: : Loss: T_44.387 V_24.911 | Acc: T_0.000) V_0.000\n",
      "Epoch 2320: : Loss: T_49.583 V_25.020 | Acc: T_0.000) V_0.000\n",
      "Epoch 2330: : Loss: T_47.017 V_24.954 | Acc: T_0.000) V_0.000\n",
      "Epoch 2340: : Loss: T_45.569 V_25.210 | Acc: T_0.000) V_0.000\n",
      "Epoch 2350: : Loss: T_46.114 V_25.578 | Acc: T_0.000) V_0.000\n",
      "Epoch 2360: : Loss: T_46.475 V_25.637 | Acc: T_0.000) V_0.000\n",
      "Epoch 2370: : Loss: T_45.296 V_25.858 | Acc: T_0.000) V_0.000\n",
      "Epoch 2380: : Loss: T_49.845 V_25.815 | Acc: T_0.000) V_0.000\n",
      "Epoch 2390: : Loss: T_46.556 V_26.120 | Acc: T_0.000) V_0.000\n",
      "Epoch 2400: : Loss: T_53.154 V_26.088 | Acc: T_0.000) V_0.000\n",
      "Epoch 2410: : Loss: T_40.613 V_25.693 | Acc: T_0.000) V_0.000\n",
      "Epoch 2420: : Loss: T_44.479 V_25.743 | Acc: T_0.000) V_0.000\n",
      "Epoch 2430: : Loss: T_59.850 V_26.019 | Acc: T_0.000) V_0.000\n",
      "Epoch 2440: : Loss: T_40.533 V_26.399 | Acc: T_0.000) V_0.000\n",
      "Epoch 2450: : Loss: T_46.534 V_25.537 | Acc: T_0.000) V_0.000\n",
      "Epoch 2460: : Loss: T_46.379 V_25.482 | Acc: T_0.000) V_0.000\n",
      "Epoch 2470: : Loss: T_38.138 V_25.555 | Acc: T_0.000) V_0.000\n",
      "Epoch 2480: : Loss: T_46.395 V_25.671 | Acc: T_0.000) V_0.000\n",
      "Epoch 2490: : Loss: T_43.888 V_26.347 | Acc: T_0.000) V_0.000\n",
      "Epoch 2500: : Loss: T_40.880 V_26.412 | Acc: T_0.000) V_0.000\n",
      "Epoch 2510: : Loss: T_48.817 V_26.571 | Acc: T_0.000) V_0.000\n",
      "Epoch 2520: : Loss: T_45.231 V_26.483 | Acc: T_0.000) V_0.000\n",
      "Epoch 2530: : Loss: T_37.743 V_26.703 | Acc: T_0.000) V_0.000\n",
      "Epoch 2540: : Loss: T_39.520 V_26.330 | Acc: T_0.000) V_0.000\n",
      "Epoch 2550: : Loss: T_45.219 V_26.219 | Acc: T_0.000) V_0.000\n",
      "Epoch 2560: : Loss: T_43.862 V_26.346 | Acc: T_0.000) V_0.000\n",
      "Epoch 2570: : Loss: T_42.666 V_26.540 | Acc: T_0.000) V_0.000\n",
      "Epoch 2580: : Loss: T_44.447 V_26.132 | Acc: T_0.000) V_0.000\n",
      "Epoch 2590: : Loss: T_44.125 V_25.902 | Acc: T_0.000) V_0.000\n",
      "Epoch 2600: : Loss: T_42.647 V_25.522 | Acc: T_0.000) V_0.000\n",
      "Epoch 2610: : Loss: T_44.948 V_25.680 | Acc: T_0.000) V_0.000\n",
      "Epoch 2620: : Loss: T_49.239 V_25.888 | Acc: T_0.000) V_0.000\n",
      "Epoch 2630: : Loss: T_46.518 V_25.569 | Acc: T_0.000) V_0.000\n",
      "Epoch 2640: : Loss: T_40.287 V_25.477 | Acc: T_0.000) V_0.000\n",
      "Epoch 2650: : Loss: T_40.824 V_25.612 | Acc: T_0.000) V_0.000\n",
      "Epoch 2660: : Loss: T_41.954 V_25.496 | Acc: T_0.000) V_0.000\n",
      "Epoch 2670: : Loss: T_43.991 V_25.892 | Acc: T_0.000) V_0.000\n",
      "Epoch 2680: : Loss: T_42.338 V_25.948 | Acc: T_0.000) V_0.000\n",
      "Epoch 2690: : Loss: T_43.526 V_25.810 | Acc: T_0.000) V_0.000\n",
      "Epoch 2700: : Loss: T_44.873 V_25.944 | Acc: T_0.000) V_0.000\n",
      "Epoch 2710: : Loss: T_45.525 V_25.656 | Acc: T_0.000) V_0.000\n",
      "Epoch 2720: : Loss: T_42.485 V_26.008 | Acc: T_0.000) V_0.000\n",
      "Epoch 2730: : Loss: T_43.037 V_26.415 | Acc: T_0.000) V_0.000\n",
      "Epoch 2740: : Loss: T_48.361 V_26.385 | Acc: T_0.000) V_0.000\n",
      "Epoch 2750: : Loss: T_50.654 V_26.375 | Acc: T_0.000) V_0.000\n",
      "Epoch 2760: : Loss: T_45.214 V_25.795 | Acc: T_0.000) V_0.000\n",
      "Epoch 2770: : Loss: T_47.043 V_25.686 | Acc: T_0.000) V_0.000\n",
      "Epoch 2780: : Loss: T_49.133 V_25.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 2790: : Loss: T_46.063 V_25.808 | Acc: T_0.000) V_0.000\n",
      "Epoch 2800: : Loss: T_54.125 V_25.973 | Acc: T_0.000) V_0.000\n",
      "Epoch 2810: : Loss: T_46.822 V_26.128 | Acc: T_0.000) V_0.000\n",
      "Epoch 2820: : Loss: T_43.742 V_25.583 | Acc: T_0.000) V_0.000\n",
      "Epoch 2830: : Loss: T_40.854 V_25.582 | Acc: T_0.000) V_0.000\n",
      "Epoch 2840: : Loss: T_48.862 V_25.391 | Acc: T_0.000) V_0.000\n",
      "Epoch 2850: : Loss: T_39.366 V_25.030 | Acc: T_0.000) V_0.000\n",
      "Epoch 2860: : Loss: T_44.112 V_25.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 2870: : Loss: T_43.409 V_25.802 | Acc: T_0.000) V_0.000\n",
      "Epoch 2880: : Loss: T_51.983 V_25.692 | Acc: T_0.000) V_0.000\n",
      "Epoch 2890: : Loss: T_40.436 V_26.140 | Acc: T_0.000) V_0.000\n",
      "Epoch 2900: : Loss: T_45.950 V_26.168 | Acc: T_0.000) V_0.000\n",
      "Epoch 2910: : Loss: T_46.591 V_25.981 | Acc: T_0.000) V_0.000\n",
      "Epoch 2920: : Loss: T_42.183 V_25.837 | Acc: T_0.000) V_0.000\n",
      "Epoch 2930: : Loss: T_41.617 V_26.598 | Acc: T_0.000) V_0.000\n",
      "Epoch 2940: : Loss: T_44.757 V_26.205 | Acc: T_0.000) V_0.000\n",
      "Epoch 2950: : Loss: T_42.091 V_26.426 | Acc: T_0.000) V_0.000\n",
      "Epoch 2960: : Loss: T_54.829 V_26.158 | Acc: T_0.000) V_0.000\n",
      "Epoch 2970: : Loss: T_47.573 V_26.442 | Acc: T_0.000) V_0.000\n",
      "Epoch 2980: : Loss: T_50.836 V_26.762 | Acc: T_0.000) V_0.000\n",
      "Epoch 2990: : Loss: T_43.902 V_27.140 | Acc: T_0.000) V_0.000\n",
      "Epoch 3000: : Loss: T_40.238 V_27.025 | Acc: T_0.000) V_0.000\n",
      "Epoch 3010: : Loss: T_45.064 V_26.364 | Acc: T_0.000) V_0.000\n",
      "Epoch 3020: : Loss: T_51.195 V_26.260 | Acc: T_0.000) V_0.000\n",
      "Epoch 3030: : Loss: T_47.697 V_26.202 | Acc: T_0.000) V_0.000\n",
      "Epoch 3040: : Loss: T_46.509 V_26.116 | Acc: T_0.000) V_0.000\n",
      "Epoch 3050: : Loss: T_39.267 V_25.972 | Acc: T_0.000) V_0.000\n",
      "Epoch 3060: : Loss: T_44.865 V_25.454 | Acc: T_0.000) V_0.000\n",
      "Epoch 3070: : Loss: T_52.190 V_25.804 | Acc: T_0.000) V_0.000\n",
      "Epoch 3080: : Loss: T_42.004 V_26.190 | Acc: T_0.000) V_0.000\n",
      "Epoch 3090: : Loss: T_51.294 V_25.843 | Acc: T_0.000) V_0.000\n",
      "Epoch 3100: : Loss: T_46.143 V_26.256 | Acc: T_0.000) V_0.000\n",
      "Epoch 3110: : Loss: T_47.175 V_26.395 | Acc: T_0.000) V_0.000\n",
      "Epoch 3120: : Loss: T_51.716 V_26.786 | Acc: T_0.000) V_0.000\n",
      "Epoch 3130: : Loss: T_50.227 V_26.757 | Acc: T_0.000) V_0.000\n",
      "Epoch 3140: : Loss: T_44.702 V_26.458 | Acc: T_0.000) V_0.000\n",
      "Epoch 3150: : Loss: T_41.798 V_26.631 | Acc: T_0.000) V_0.000\n",
      "Epoch 3160: : Loss: T_45.955 V_26.238 | Acc: T_0.000) V_0.000\n",
      "Epoch 3170: : Loss: T_42.363 V_26.303 | Acc: T_0.000) V_0.000\n",
      "Epoch 3180: : Loss: T_45.231 V_26.500 | Acc: T_0.000) V_0.000\n",
      "Epoch 3190: : Loss: T_52.984 V_26.461 | Acc: T_0.000) V_0.000\n",
      "Epoch 3200: : Loss: T_46.410 V_26.552 | Acc: T_0.000) V_0.000\n",
      "Epoch 3210: : Loss: T_44.290 V_26.294 | Acc: T_0.000) V_0.000\n",
      "Epoch 3220: : Loss: T_43.765 V_26.414 | Acc: T_0.000) V_0.000\n",
      "Epoch 3230: : Loss: T_53.584 V_26.944 | Acc: T_0.000) V_0.000\n",
      "Epoch 3240: : Loss: T_52.143 V_26.806 | Acc: T_0.000) V_0.000\n",
      "Epoch 3250: : Loss: T_41.913 V_26.176 | Acc: T_0.000) V_0.000\n",
      "Epoch 3260: : Loss: T_47.612 V_25.843 | Acc: T_0.000) V_0.000\n",
      "Epoch 3270: : Loss: T_46.107 V_26.109 | Acc: T_0.000) V_0.000\n",
      "Epoch 3280: : Loss: T_47.176 V_26.218 | Acc: T_0.000) V_0.000\n",
      "Epoch 3290: : Loss: T_48.564 V_26.371 | Acc: T_0.000) V_0.000\n",
      "Epoch 3300: : Loss: T_52.363 V_26.342 | Acc: T_0.000) V_0.000\n",
      "Epoch 3310: : Loss: T_42.498 V_27.032 | Acc: T_0.000) V_0.000\n",
      "Epoch 3320: : Loss: T_41.138 V_27.559 | Acc: T_0.000) V_0.000\n",
      "Epoch 3330: : Loss: T_42.603 V_26.944 | Acc: T_0.000) V_0.000\n",
      "Epoch 3340: : Loss: T_47.691 V_26.724 | Acc: T_0.000) V_0.000\n",
      "Epoch 3350: : Loss: T_52.201 V_26.401 | Acc: T_0.000) V_0.000\n",
      "Epoch 3360: : Loss: T_49.229 V_26.248 | Acc: T_0.000) V_0.000\n",
      "Epoch 3370: : Loss: T_36.825 V_26.022 | Acc: T_0.000) V_0.000\n",
      "Epoch 3380: : Loss: T_49.890 V_26.194 | Acc: T_0.000) V_0.000\n",
      "Epoch 3390: : Loss: T_49.030 V_26.243 | Acc: T_0.000) V_0.000\n",
      "Epoch 3400: : Loss: T_43.645 V_26.010 | Acc: T_0.000) V_0.000\n",
      "Epoch 3410: : Loss: T_39.068 V_26.284 | Acc: T_0.000) V_0.000\n",
      "Epoch 3420: : Loss: T_40.698 V_26.650 | Acc: T_0.000) V_0.000\n",
      "Epoch 3430: : Loss: T_34.844 V_26.863 | Acc: T_0.000) V_0.000\n",
      "Epoch 3440: : Loss: T_49.512 V_26.983 | Acc: T_0.000) V_0.000\n",
      "Epoch 3450: : Loss: T_40.701 V_26.539 | Acc: T_0.000) V_0.000\n",
      "Epoch 3460: : Loss: T_42.228 V_26.706 | Acc: T_0.000) V_0.000\n",
      "Epoch 3470: : Loss: T_50.236 V_26.778 | Acc: T_0.000) V_0.000\n",
      "Epoch 3480: : Loss: T_47.674 V_26.824 | Acc: T_0.000) V_0.000\n",
      "Epoch 3490: : Loss: T_43.357 V_26.312 | Acc: T_0.000) V_0.000\n",
      "Epoch 3500: : Loss: T_44.580 V_26.681 | Acc: T_0.000) V_0.000\n"
     ]
    }
   ],
   "source": [
    "best_models = []\n",
    "for i in range(NUM_ENSEMBLE_MODELS):\n",
    "    model = BasicRegressor()\n",
    "    model.to(device)\n",
    "\n",
    "    # criterion = nn.L1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    bagg_indices = np.random.choice(range(len(x_train)), len(x_train), replace=True)\n",
    "    # x_train_bagg = x_train[bagg_indices, :]\n",
    "    # y_train_bagg = y_train[bagg_indices, :]\n",
    "\n",
    "    rare_indicies = np.where(y_train>threshold_rare)[0]\n",
    "    normal_indicies = np.where(y_train<=threshold_rare)[0]\n",
    "\n",
    "    ov_rare_indicies = np.random.choice(range(len(rare_indicies)), len(normal_indicies), replace=True)\n",
    "\n",
    "    x_train_normal_bagg = x_train[normal_indicies, :]\n",
    "    y_train_normal_bagg = y_train[normal_indicies, :]\n",
    "\n",
    "\n",
    "    x_train_rare_bagg = x_train[rare_indicies, :]\n",
    "    y_train_rare_bagg = y_train[rare_indicies, :]\n",
    "\n",
    "\n",
    "    x_train_total_bagg = np.append(x_train_normal_bagg, x_train_rare_bagg, axis=0)\n",
    "    y_train_total_bagg = np.append(y_train_normal_bagg, y_train_rare_bagg, axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # train_data = TrainData(torch.FloatTensor(x_train), torch.FloatTensor(y_train))\n",
    "    train_data = TrainData(torch.FloatTensor(x_train_total_bagg), torch.FloatTensor(y_train_total_bagg))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=2048, shuffle=True)\n",
    "\n",
    "\n",
    "    num_train_data = len(train_loader)\n",
    "    num_eval_data = len(valid_loader)\n",
    "\n",
    "\n",
    "    elapsed_time_basic_ann = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    best_model = train_model(num_train_data, num_eval_data)\n",
    "\n",
    "    best_models.append(best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "sum_output = np.zeros(y_test.shape)\n",
    "\n",
    "for best_model in best_models:\n",
    "    best_model.eval()\n",
    "    output = best_model(data)\n",
    "    sum_output += output.cpu().detach().numpy()\n",
    "\n",
    "avg_output = sum_output / len(best_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(best_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Loss  7.119301442746765\n",
      "Normal Loss  1.9681644721697733\n",
      "Total Loss  2.422676557808919\n"
     ]
    }
   ],
   "source": [
    "rare_loss, normal_loss, total_loss = calc_l1_loss_by_shots(avg_output, answer.cpu().detach().numpy())\n",
    "print(\"Rare Loss \", rare_loss)\n",
    "print(\"Normal Loss \", normal_loss)\n",
    "print(\"Total Loss \", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.1000], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.31996314])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_output[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv_python_3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78d04299e464758119aa473303693f33db2a1bc5c94011f00bbd9c1618e77f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
