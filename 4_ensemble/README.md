# ğŸ¤”Ensembleì€ Imablanced Dataì—ì„œë„ íš¨ê³¼ë¥¼ ë³´ì¼ê¹Œ?

## The effect of ensemble learning in imbalanced regression datasets

![image-20221130231343857](./attachments/image-20221130231343857.png)



ğŸ”¥ì´ë²ˆ Tutorialì—ì„œëŠ” **Imbalanced Regression Datasetì—ì„œ Ensemble Learningì´ íš¨ê³¼**ê°€ ìˆì„ì§€ì— ëŒ€í•´ì„œ Tutorialì„ ì§„í–‰í•˜ê³ ì í•œë‹¤. íŠ¹íˆ Ensemble Learningê¸°ë²• ì¤‘ **Baggingì„ ì‚¬ìš©í•˜ì—¬ Deep Learning ëª¨ë¸**ì—ì„œ íš¨ê³¼ì„±ì„ ë³´ê³ ì í•œë‹¤.



- ìµœê·¼ì˜ ëŒ€ë¶€ë¶„ì˜ SoTA ê¸°ë²•ë“¤ì€ Deep Learningê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì–´ ì§€ê³  ìˆë‹¤. Tabular DataëŠ” ì•„ì§ ì •ë³µí•˜ì§€ ëª»í•œ ê°ì´ ìˆìœ¼ë‚˜, ê·¸ê²ƒë„ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì ì°¨ì ìœ¼ë¡œ í•´ê²°ë˜ê³  ìˆë‹¤.

  - [[1908.07442\] TabNet: Attentive Interpretable Tabular Learning (arxiv.org)](https://arxiv.org/abs/1908.07442)
  - [[2106.11189\] Well-tuned Simple Nets Excel on Tabular Datasets (arxiv.org)](https://arxiv.org/abs/2106.11189)

- ê·¸ë¦¬ê³  Real-Worldì˜ Dataë“¤ì€ ì–´ë–»ê²Œë“  Imbalanced í•œ Dataë¥¼ ê°–ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì´ë‹¤.

- ê·¸ëŸ¬ë‚˜ ì•„ì§ Imbalanced Data ì¤‘ Regression Taskì— ëŒ€í•œ ì—°êµ¬ëŠ” ë§ì´ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìœ¼ë©°, ìµœê·¼ 2021ë…„ë¶€í„° ê´€ë ¨ëœ Deep Learningê¸°ë°˜ Imbalanced Regression Taskë¥¼ í’€ë ¤í•˜ëŠ” ì‹œë„ê°€ ì´ë£¨ì–´ì§€ê³  ìˆë‹¤.

- ì•„ì§ Ensemble Learningì„ í†µí•´ í•´ë‹¹ Imbalanced Regression Taskë¥¼ í’€ë ¤ëŠ” ì‹œë„ëŠ” ê·¹ì†Œìˆ˜ë§Œ ì¡´ì¬í•˜ë©°, ë”°ë¼ì„œ ì´ë²ˆ Tutorialì—ì„œëŠ” í•´ë‹¹ Taskì— Niaveí•˜ê²Œ Ensemble Learningì„ ì ìš©í•´ ë³´ë©° ê·¸ íš¨ê³¼ì„±ì— ëŒ€í•´ì„œ ìƒê° í•´ ë³´ê³ ì í•œë‹¤.

  - [REBAGG: REsampled BAGGing for Imbalanced Regression (mlr.press)](https://proceedings.mlr.press/v94/branco18a.html)

  



# Table of Contents

- [Background of Anomaly Detection](#Background-of-Ensemble-Learning)

  - [1. Basic Concept](#1-Basic-Concept)
  - [2. Bagging](#2-Bagging)
  - [3. Boosting](#3-Boosting)

- [Tutorial. Ensemble learning in imbalanced regression task](#Tutorial-Ensemble-learning-in-imbalanced-regression-task)

  - [1. Tutorial Notebook](#1-Tutorial-Notebook)
  - [2. Setting](#2-Setting)
  - [3. Usage Code](#3-Usage-Code)
  - [4. Result (Accuracy)](#4-Result_Accuracy)

- [Final Insights](#Final-Insights)

- [Conclusion](#Conclusion)

- [References](#References)

  

-------

# Background of Ensemble Learning

## 1. Basic Concept

![image-20221130225055439](./attachments/image-20221130225055439.png)



ë‹¨ì§€ í•˜ë‚˜ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ëª¨ë“  Datasetì— ëŒ€í•´ì„œ, ëª¨ë‘ ì˜ ë™ì‘ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.(No Free Lunch) ì™œëƒí•˜ë©´ ëŒ€ë¶€ë¶„ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ì–´ë– í•œ í˜„ì‹¤ì— ëŒ€í•œ ê°„ëµí™” í˜¹ì€ ê°€ì •(Assumtion)ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ”ë°, ì´ëŸ¬í•œ ê°€ì •ì€ íŠ¹ì • Caseì—ëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.



ì‹¤ì œì ìœ¼ë¡œ ì•„ë˜ì˜ ë…¼ë¬¸ì—ì„œëŠ” ìˆ˜ë°±ê°œì˜ Classifierë“¤ì„ í™œìš©í•˜ì—¬ ìš°ë¦¬ê°€ ì›í•´ ë§ˆì§€ì•ŠëŠ”, Master Algorithmì´ ì‹¤ì œë¡œëŠ” ì¡´ì¬í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ Emiricalí•œ Testë¡œ ë°í˜€ ë‚´ì—ˆë‹¤.

- [Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? (jmlr.org)](https://jmlr.org/papers/v15/delgado14a.html)



> ì™œ ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë¸ì€ True Real Modelë¡œì¨ ì™„ë²½íˆ ì˜ í•™ìŠµë˜ì§€ ì•Šì„ê¹Œ?

ì™œëƒí•˜ë©´ ì‹¤ì œ Dataì—ëŠ” í•­ìƒ Noiseê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— Machine Learning ëª¨ë¸ì´ í•­ìƒ ì •í™•í•œ ì¶”ì •ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë˜í•œ Populationì„ ì„¤ëª…í•  ë§Œí¼ì˜ Data Sampleì´ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ê³ , Training Setê³¼ Testing Setì˜ ë¶„í¬ê°€ ë‹¤ë¥¸ ë“±ì˜ ë¬¸ì œê°€ í•­ìƒ ì¡´ì¬í•œë‹¤.

íŠ¹íˆë‚˜ Dataìƒì— Noiseê°€ ì¡´ì¬í•˜ëŠ” ë¬¸ì œ ìƒí™©ì—ì„œ, Machine Learningëª¨ë¸ì€ Modelì˜ Complexityì— ë”°ë¼, ë‹¨ìˆœí•œ(Simple) ëª¨ë¸ì€ Bias Errorê°€ ì»¤ì§€ê³ , ë³µì¡í•œ(Complex) ëª¨ë¸ì€ Variance Errorê°€ ì»¤ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤. 



![image-20221130225915539](./attachments/image-20221130225915539.png)

- Bias (í¸í–¥) Error
  - ë°˜ë³µ ëª¨ë¸ í•™ìŠµ ì‹œ, **í‰ê· ì **ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì •í™•í•œ ì¶”ì •ì´ ê°€ëŠ¥í•œì§€ ì¸¡ì •
  - **ëª¨ë¸ì´ ë‹¨ìˆœ**í•˜ë©´, Biasê°€ ì»¤ì§€ëŠ” ê²½í–¥

- Vriance (ë¶„ì‚°) Error
  - ë°˜ë³µ ëª¨ë¸ í•™ìŠµ ì‹œ, **ê°œë³„ ì¶”ì •**ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œ ì¶”ì •ì´ ê°€ëŠ¥í•œì§€ ì¸¡ì •
  - **ëª¨ë¸ì´ ë³µì¡**í•˜ë©´, Varianceê°€ ì»¤ì§€ëŠ” ê²½í–¥



ì´ëŸ¬í•œ ëª¨ë¸ì˜ Complexityì— ëŒ€ì‘í•˜ê³ , í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ëª¨ë“  Real-Worldì˜ Caseë¥¼ ëŒ€ì‘í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œì ì„ í•´ê²°í•˜ê³ ì, ë‹¤ì–‘í•œ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” Ensemble Learning ê¸°ë²•ì´ ë“±ì¥í•˜ì˜€ë‹¤. Ensemble Learningì˜ í•µì‹¬ì€ ê°œë³„ ëª¨ë¸ë“¤ì„ í•©ì³ì„œ ì¶©ë¶„í•œ **ëª¨ë¸ì˜ ë‹¤ì–‘ì„±(Diversity)ë¥¼ í™•ë³´**í•˜ê³ , ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ê°œë³„ ëª¨ë¸ì„ **ì–´ë–»ê²Œ ì˜ ê²°í•© í• ì§€**ì— ëŒ€í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì œì‹œí•œë‹¤.



ê·¸ë¦¬ê³  ìœ„ì—ì„œ ë§í•œ Complexityì— ë”°ë¥¸ Biasê³¼ Varianceì˜ Errorë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œì¨, ì•„ë˜ì˜ 3ê°€ì§€ ë¶„ë¥˜ë¡œ Ensemble Learningì€ ë¶„íŒŒë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ì´ ì¤‘ì— ìš°ë¦¬ëŠ” ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” Baggingê³¼ Boostingì— ëŒ€í•´ ì•Œì•„ë³´ê² ë‹¤.

![image-20221130230410829](./attachments/image-20221130230410829.png)



## 2. Bagging

ë¶„ì‚°(Variance) Errorê°€ ë†’ê³  í¸í–¥(Bias) Errorê°€ ë‚®ì€ ëª¨ë¸ì— ì í•©í•œ, ì¦‰ **Complexityê°€ ë†’ì€ ëª¨ë¸**(ex. ANN, SVM ë“±)ì— ì í•©í•œ ë°©ë²•ì¸ Baggingì„ ê°„ë‹¨íˆ ì•Œì•„ë³´ì. Baggingì„ ê°„ë‹¨íˆ ìš”ì•½í•˜ìë©´, ë°ì´í„°ë¥¼ Splití•˜ì—¬ ëª¨ë¸ì˜ ì°¨ë³„í™”ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê¸°ë²•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

![image-20221130230845487](./attachments/image-20221130230845487.png)



Baggingì€ ì‚¬ì‹¤ ë‹¨ìˆœí•œ ê¸°ë²•ì´ë‹¤. ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë‹¨ì§€ Dataë¥¼ Samplingí• ë•Œ Dataì˜ Sampleì— ëŒ€í•œ ë°©ì‹ì„ ë‹¤ì–‘í•˜ê²Œ ì£¼ì–´ì„œ, ëª¨ë¸ì˜ Diversityë¥¼ ë†’ì´ëŠ” ë°©ë²•ì´ ì „ë¶€ì´ê¸° ë•Œë¬¸ì´ë‹¤.(ì¦‰ Resamplingê¸°ë²•ì´ë‹¤.) ì‚¬ì‹¤ Implicití•˜ê²Œ Ensemble Modelì„ ë§Œë“œëŠ” ë°©ì‹ì¸ Data Resampling Ensembleì€ í¬ê²Œ 2ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤. í•˜ë‚˜ëŠ” ë¹„ë³µì› ì¶”ì¶œë°©ì‹ì¸ **Pasting**ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë³µì›ì¶”ì¶œ ë°©ì‹ì¸ **Bagging**ì´ë‹¤. ì´ë²ˆì—ëŠ” ìš°ë¦¬ëŠ” Baggingì— ì§‘ì¤‘í•´ì„œ ìŠ¤í„°ë”” í•´ë³´ê² ë‹¤.

![image-20221130231528610](./attachments/image-20221130231528610.png)



Baggingê¸°ë²•ì€ ëª¨ë¸ê³¼ ìƒê´€ì—†ì´ ì§„í–‰ë  ìˆ˜ ìˆëŠ” ê¸°ë²•ì´ë©°, ì¦‰ ì–´ë– í•œ Complexityê°€ ë†’ì€ ëª¨ë¸ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ Deep Learning ëª¨ë¸ì—ì„œ í™œìš©í•˜ê¸° ìš©ì´í•˜ë©°, ì´ë²ˆ Tutorialì—ì„œë„ Deep Learningëª¨ë¸ì„ í™œìš©í•˜ë¯€ë¡œ, Bagging ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•  ì˜ˆì •ì´ë‹¤.



> ì´ë¦„ì€ ë“¤ì–´ë´¤ê² ì§€? ëŒ€í‘œì  Bagging ì•Œê³ ë¦¬ì¦˜ì¸ Random Forests

ìœ ëª…í•œ Baggingê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” Random Forestsê°€ ìˆë‹¤. ì•„ë˜ì™€ ê°™ì´ Depthê°€ ìˆëŠ”(ì¦‰, Complexityë¥¼ ë†’ì¸) Treeë“¤ì„ ì—¬ëŸ¬ê°œ ì‚¬ìš©í•˜ì—¬ Dataë¥¼ Baggingì„ í†µí•´ ê° ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê¸°ë²•ì´ë‹¤.

![image-20221130231851344](./attachments/image-20221130231851344.png)



ì¶”ê°€ì ìœ¼ë¡œëŠ” ì•„ë˜ ê·¸ë¦¼ì˜ ìš°ì¸¡ê³¼ê°™ì´, Treeë¥¼ ë¶„ê¸°í•  ë•Œ Random Feature Selectionì„ í†µí•´ ëª¨ë¸ì˜ ë‹¤ì–‘ì„±ì„ ì¶”ê°€ì ìœ¼ë¡œ ê°€ì ¸ê°€ê²Œ ëœë‹¤. ê°œë…ì ìœ¼ë¡œëŠ” ë‹¨ìˆœí•œë°, Tabular Datasetì— ëŒ€í•˜ì—¬ ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì ¸ê°ˆ ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.

![image-20221130231840378](./attachments/image-20221130231840378.png)



ë˜í•œ Baggingì˜ ì¥ì ì€, ë‹¨ì§€ Data Samplingì— ëŒ€í•œ ë¬¸ì œì´ê¸° ë•Œë¬¸ì—, ê°œë³„ ëª¨ë¸ë“¤ì„ Parallelí•˜ê²Œ ë³‘ë ¬ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. ì¦‰ Multi-Processingí™˜ê²½ì—ì„œëŠ” ë‹¤ë¥¸ Ensemble ê¸°ë²•ë³´ë‹¤ ìš°ì›”í•œ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë‹¤.



## 3. Boosting

ë‘ë²ˆì§¸ë¡œëŠ” ë¶„ì‚°(Variance) Errorê°€ ë‚®ê³  í¸í–¥(Bias) Errorê°€ ë†’ì€ ëª¨ë¸ì— ì í•©í•œ, ì¦‰ **Complexityê°€ ë‚®ì€ ëª¨ë¸**(ex. Decision Stumpë“±)ì— ì í•©í•œ ë°©ë²•ì¸ Boostingì„ ê°„ë‹¨íˆ ì•Œì•„ë³´ì. Boostingì„ ê°„ë‹¨íˆ ìš”ì•½í•˜ìë©´, Sequentialí•˜ê²Œ ê°ê°ì˜ ëª¨ë¸ë“¤ì„ í•˜ë‚˜ì”© í•™ìŠµí•´ ë‚˜ê°€ë©°, ì´ì „ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ì´ ì˜ ì•ˆë˜ëŠ” ê²ƒì„ ë‹¤ìŒ ëª¨ë¸ì´ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµí•´ ë‚˜ê°€ëŠ” ë°©ë²•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. Boostingë„ ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, Baggingê³¼ëŠ” ì¡°ê¸ˆ ë‹¬ë¦¬ íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì„ ê°€ë¦¬í‚¤ëŠ” ê²½ìš°ê°€ ë§ì´ ìˆë‹¤. (ex. Adaboost, XGBoost, LightGBM ë“±)

![image-20221130232443992](./attachments/image-20221130232443992.png)





### AdaBoost (Adaptive Boosting)

![0_z6ulJBvzBXYWLZwn](./attachments/0_z6ulJBvzBXYWLZwn.gif)

ì˜ë¯¸ìˆëŠ” ê²°ê³¼ë¥¼ ë‚´ë†“ì€ ê±°ì˜ ìµœì´ˆì˜ Boosting ê¸°ë²•ì´ë‹¤. Viola-Jones Real-Time Object Detectorì— ì‚¬ìš©ë˜ê³  ìˆê³ , ë§ ê·¸ëŒ€ë¡œ Real-Timeìœ¼ë¡œ ì‘ë™ë  ì •ë„ë¡œ ì†ë„ê°€ ë¹ ë¥¸ ê¸°ë²•ì´ë‹¤. ë³´í†µ Decision Treeë¥¼ ê°„ë‹¨íˆ ë§Œë“  Decision Stumpë¡œ ë§Œë“¤ê³ , Random ForestsëŠ” ì¡°ê¸ˆ í° Decision Treeë¥¼ ì‚¬ìš©í•œë‹¤ë©´, AdaBoostëŠ” ì•„ì£¼ ë‹¨ìˆœí•œ 1-depthì˜ Decision Stumpë¥¼ ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©í•œë‹¤. 

![image-20221201134843474](./attachments/image-20221201134843474.png)



AdaBoostëŠ” Sequentialí•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œì¨, ì´ì „ ë¶„ë¥˜ê¸°ê°€ ì˜ ëª» ë¶„ë¥˜í•œ ê²ƒë“¤ì— Weightë¥¼ ë” ì£¼ì–´, ê·¸ê²ƒë“¤ì— ì§‘ì¤‘í•˜ì—¬ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•ì´ë‹¤. ì•„ë˜ì˜ Sequenceì— ë”°ë¼ ì•Œê³ ë¦¬ì¦˜ì´ ë™ì‘í•˜ë©°, Step 1~3ì„ ë°˜ë³µí•˜ë‹¤ê°€ Convergenceë˜ë©´ ìµœì¢… ì„ í˜• Weighted Sumìœ¼ë¡œ ê²°ê³¼ë¥¼ Aggregatingí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.

- Step 1. í˜„ì¬ Datasetì— ëŒ€í•˜ì—¬ ë‹¨ìˆœí•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ í•™ìŠµ
- Step 2. Training Errorê°€ í° Data ê°ì²´ì˜ ì„ íƒ í™•ë¥ ì„ ì¦ê°€, Errorê°€ ì‘ì€ ê°œì²´ì˜ ì„ íƒí™•ë¥  ê°ì†Œ
- Step 3. â€˜Step 2â€™ì˜ ê³„ì‚°ëœ í™•ë¥ ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ì˜ Dataset êµ¬ì„±
- ìµœì¢… Aggregationì€ ê° ëª¨ë¸ì˜ ì„±ëŠ¥ ì§€í‘œë¥¼ Weightë¡œ ì‚¬ìš©í•˜ì—¬ ê²°í•©

ì•Œê³ ë¦¬ì¦˜ì˜ í•™ìŠµ ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤. ì˜¤ë¶„ë¥˜ëœ ê²°ê³¼ëŠ” ì¢€ ë” í° Weight $\alpha$ë¥¼ ê°–ê²Œ ëœë‹¤.

![image-20221201135033740](./attachments/image-20221201135033740.png)



### GBM (Gradient Boosting Machine)

GBMì€ ì´ì „ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡í•œ ê°’ê³¼ ì •ë‹µê°’ ì‚¬ì´ì˜ ì˜¤ì°¨ì¸ Gradient(í˜¹ì€ Residualë¡œ í‘œí˜„)ë¥¼ ë‹¤ìŒ ëª¨ë¸ì´ í•™ìŠµí•´ ê°€ëŠ” Sequentialí•œ Boostingì•Œê³ ë¦¬ì¦˜ì´ë‹¤. XGBoost, LightGBM ë“±ì˜ ê·¼ê°„ì´ ë˜ëŠ” ê¸°ë³¸ì ì¸ ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.



ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´, ì²«ë²ˆì§¸ Treeëª¨ë¸ì´ Ground Truthì— ëŒ€í•œ í•™ìŠµì„ í•˜ê³ , ì •ë‹µê°’ê³¼ì˜ Residual(Gradient)ë¥¼ ê³„ì‚°í•˜ì—¬, ê·¸ Residualë§Œí¼ Tree 2ê°€ í•™ìŠµí•œë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ Tree 3ë„ Tree 2ì˜ Residualì„ ê³„ì‚°í•˜ì—¬ í•™ìŠµí•´ ë‚˜ê°„ë‹¤. ì´ë¥¼ ë°˜ë³µí•´ì„œ ë§Œë“  ëª¨ë¸ì´ ë°”ë¡œ Gradient Boosting Machineì´ë‹¤.

![image-20221201135302585](./attachments/image-20221201135302585.png)

ì•„ì£¼ ë‹¨ìˆœí•˜ì§€ë§Œ ê°•ë ¥í•œ ëª¨ë¸ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¨ì ìœ¼ë¡œëŠ” ì•„ë¬´ë˜ë„ ì‘ì€ ëª¨ë“  Residualì„ ê³„ì‚°í•˜ì—¬ í•™ìŠµí•˜ë‹¤ ë³´ë‹ˆ, Noiseê¹Œì§€ í•™ìŠµë˜ëŠ” ê²½í–¥ì´ ìˆì–´ Overfittingì— ì·¨ì•½í•˜ë‹¤ê³  í•  ìˆ˜ ìˆê² ë‹¤.



### XGBoost (Extreme Gradient Boosting Machine)

![image-20221201142525989](./attachments/image-20221201142525989.png)

Gradient Boostingì€ ìœ„ì˜ GBMì˜ **Overfittingì„ ë°©ì§€í•˜ëŠ” ëª‡ê°€ì§€ Regularization ê¸°ë²•**ì„ ì‚¬ìš©í•˜ë„ë¡ ê°œë°œëœ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ë˜í•œ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ Cache Hit Optimizationì´ë‚˜ Data Split Finding Algorithmë“±ì„ í™œìš©í•˜ì—¬ ê³ ì†ì˜ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë„ë¡ í•œë‹¤.

ì¼ë‹¨ ìš°ë¦¬ëŠ” **ì•Œê³ ë¦¬ì¦˜ì˜ ì†ë„ ì ì¸ ì¸¡ë©´ë³´ë‹¤ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì ì¸ ì¸¡ë©´**ì—ì„œ Overfittingì„ ë§‰ëŠ” ê¸°ë²•ì´ í•™ë¬¸ì ìœ¼ë¡œ ë” ì¤‘ìš”í•˜ë¯€ë¡œ ê·¸ ë¶€ë¶„ì„ ê°„ë‹¨íˆ ì‚´í´ ë³´ê³ ì í•œë‹¤.

í¬ê²Œ ì•„ë˜ì™€ ê°™ì´ 2ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ Ovefittingì„ ë°©ì§€í•˜ë ¤ í•˜ê³  ìˆë‹¤.



> 1. Regularized Learning Objective 

ì¼ë°˜ GBMì€ ë‹¨ìˆœì´ MSEë¥¼ Loss Functionìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”ë°, XGBoostëŠ” ì•„ë˜ì™€ ê°™ì´ REgularized Termì„ ì‚¬ìš©í•˜ì—¬ Overfittingì„ ë°©ì§€í•œë‹¤. ì—¬ê¸°ì„œ Î© ëŠ” leafì˜ ê°œìˆ˜ì¸ Tê°€ ì ê³ , ||w||^2ê°€ ì‘ë„ë¡(leafì˜ L2 normì´ ì‘ì€) í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ìœ ë„í•´ ì¤€ë‹¤.

![image-20221201141434544](./attachments/image-20221201141434544.png)



>  2. Shrinkage and Column Subsampling 

**Shrinkage** : ë¶€ìŠ¤íŒ… íŠ¸ë¦¬ì˜ ê° ë‹¨ê³„ ì´í›„ ë§ˆë‹¤ ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ê°€ì¤‘ì¹˜ Î·ë¡œ Scaling í•¨. Stochastic ìµœì í™”ì˜ Learning rateì™€ ìœ ì‚¬í•˜ê²Œ ê°œë³„ íŠ¸ë¦¬ì˜ ì˜í–¥ë„ë¥¼ ê°ì†Œí•˜ê³  ë¯¸ë˜ íŠ¸ë¦¬ ê³µê°„ì„ ë‚¨ê²¨ ë†“ìŒ 

**Column (Feature) Subsampling** :ëª¨ë“  Featureë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ì¼ë¶€ Featureë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘ì„±ì„ ë¶€ì—¬í•˜ê³  Overfittingì„ ë°©ì§€





----

# Tutorial. Ensemble learning in imbalanced regression task

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Ensemble Learning ê¸°ë²• ì¤‘ Baggingì„ ì‚¬ìš©í•˜ì—¬ Complexityê°€ ë†’ì€ ëª¨ë¸ì¸ DNNì„ ê¸°ë°˜í•˜ì—¬ Imbalanced Regression Taskì— ëŒ€í•œ í•™ìŠµì„ ìˆ˜í–‰í•˜ë ¤í•œë‹¤. ì´ë•Œ Few Shotê³¼ Many Shotì— ëŒ€í•˜ì—¬ Accuracyë¥¼ ë¹„êµí•˜ì—¬ ê³¼ì—° Few Shotì— ëŒ€í•œ Imbalanced Dataì— Regression ì„±ëŠ¥ì´ Ensembleë¡œ ë†’ì•„ì§€ëŠ”ì§€ í™•ì¸í•´ ë³´ë ¤ í•œë‹¤. ë˜í•œ ì¼ë°˜ì ì¸ Baggingì— ì¶”ê°€ì ìœ¼ë¡œ Imbalanced Regressionê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ REBAGG(Resampling Bagging) ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì˜ ë³€í™”ë¥¼ í™•ì¸í•´ ë³´ê³ ì í•œë‹¤.



## 1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/3_anomaly_detection/Tutorials/tutorial_anomaly_detection_from_R_task.ipynb)



## 2. Setting

### Datasets

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì´ 3ê°œì˜ ìœ ëª…í•œ Tabular í˜•íƒœì˜ Regression Datasetì„ ì‚¬ìš©í•œë‹¤. ì „ì²´ ë°ì´í„° ì¤‘ Training Setì€ 64%, Validation Setì€ 16%, Test Setì€ 20%ì˜ Dataë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.

|      | Datasets                            | Description                                                  | Num Instances | Num Inputs (Xs) | Num Outputs (Ys) |
| ---- | ----------------------------------- | ------------------------------------------------------------ | ------------- | --------------- | ---------------- |
| 1    | Diabetes (Regression)               | ë‹¹ë‡¨ë³‘ í™˜ì ë°ì´í„° (1ë…„ í›„ ë‹¹ë‡¨ì˜ ì§„í–‰ì •ë„ë¥¼ Targetê°’ìœ¼ë¡œ í•¨) | 442           | 10              | 1                |
| 2    | Boston House Price (Regression)     | Bostonì˜ ì§‘ê°’ì— ëŒ€í•œ Data                                    | 506           | 13              | 1                |
| 3    | California House Price (Regression) | California ì§‘ê°’ì— ëŒ€í•œ Data                                  | 20,640        | 8               | 1                |

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê²Œ ëœë‹¤.

```python
# dataset_name = 'diabetes'
dataset_name = 'california_house'
# dataset_name = 'boston_house'

if dataset_name == 'diabetes':
    x, y= datasets.load_diabetes(return_X_y=True)
    threshold_rare = 270
    EPOCHS = 3500
    TRAIN_BATCH = 2048
elif dataset_name == 'california_house':
    data = datasets.fetch_california_housing()
    x = data.data
    y = data.target
    threshold_rare = 3.5
    EPOCHS = 800
    TRAIN_BATCH = 4096 
elif dataset_name == 'boston_house':
    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
    x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    y = raw_df.values[1::2, 2]

    threshold_rare = 35
    EPOCHS = 3500
    TRAIN_BATCH = 2048


```

ë¶ˆëŸ¬ì§„ 3ê°œì˜ Datasetì— ëŒ€í•œ Yê°’ì˜ Sampling ë¶„í¬ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. íŠ¹ë³„íˆ Imbalanced Datasetì„ ê³ ë¥¸ ê²ƒë„ ì•„ë‹ˆì§€ë§Œ. ëª¨ë“  ë°ì´í„°ê°€ ì™¼ìª½ìœ¼ë¡œ Skewê°€ ëœ, Right-Side Long-tailed Regression Problemì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![image-20221201180832257](./attachments/image-20221201180832257.png)



ê° Regression Taskì—ì„œ Imbalanced Regressionì˜ ì •í™•ë„ë¥¼ êµ¬í•˜ê¸° ìœ„í•˜ì—¬, Many shotê³¼ Few Shotìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ë‚˜ëˆ„ì–´ ê³„ì‚°í•˜ë ¤ í•œë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì€ Thresholdê°’ì„ í†µí•´ ë°ì´í„°ë¥¼ 2ê°€ì§€ í˜•íƒœë¡œ êµ¬ë¶„í•˜ê³ , ê°ê°ì˜ êµ¬ë¶„ëœ Many shotê³¼ Few shotì˜ ì •í™•ë„ë¥¼ L1 Lossë¡œ êµ¬í•˜ê²Œ ëœë‹¤.

- **Diabetes : 270** 
- **Boston House Price : 35**
- **California House Price : 3.5**

ì´ëŸ¬í•œ ìˆ˜ì¹˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ SMOTE, SMOGN ë“±ì˜ ê¸°ë²•ë“¤ì„ êµ¬í˜„í•˜ ì €ìë“¤ì€ Relevance Functionì„ êµ¬í•˜ì—¬ ì •í•˜ê²Œ ë˜ëŠ”ë°, ì‚¬ì‹¤ íŠ¹ë³„í•œ ì°¨ì´ëŠ” ì—†ê¸° ë–„ë¬¸ì—, ê°„ë‹¨íˆ Constant Thresholdë¡œ Many Shotê³¼ Few Shotìœ¼ë¡œ êµ¬ë¶„ í•˜ì˜€ë‹¤. ìµœê·¼ì˜ Imbalanced Regression Task ë…¼ë¬¸ë“¤ì—ì„œë„ ìœ„ì™€ ìœ ì‚¬í•˜ê²Œ ì§„í–‰í•œë‹¤.



### Algorithms

ì•„ë˜ì™€ ê°™ì€ 3ê°€ì§€ ì¢…ë¥˜ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•˜ì˜€ë‹¤.

|      | Algorithm                                      | Target     | Description                                                  |
| ---- | ---------------------------------------------- | ---------- | ------------------------------------------------------------ |
| 1    | MLP                                            | Regression | 2ê°œì˜ Hidden Layerë¡œ êµ¬ì„±ëœ MLP Layer                        |
| 2    | Ensemble MLP                                   | Regression | ìœ„ì˜ 1ë²ˆ ëª¨ë¸ê³¼ ì™„ì „íˆ ë™ì¼í•œ MLP Layerë¥¼ 6ê°œ Ensembleí•œ ëª¨ë¸ |
| 3    | Ensemble MLP with REBAGG (Random Oversampling) | Regression | 2ë²ˆì˜ Ensemble MLPì— ê° Modelë³„ Random Oversamplingì„ ì ìš©í•œ ê¸°ë²• |



## 3. Usage Code

### MLP

2ê°œì˜ Hidden Layerì™€ Input, Output Layerë¥¼ ê°€ì§„ ê°„ë‹¨í•œ MLPêµ¬ì¡°ë¥¼ Main Modelë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ Dropoutê³¼ Batchnormalization ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ Regularizationì„ í•˜ì˜€ìœ¼ë©°, Overfittingì „ì— Validation Setìœ¼ë¡œ ê²°ì •ëœ Best Modelì„ ì¤‘ê°„ì¤‘ê°„ ì €ì¥í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ í•´ë‹¹ MLPëª¨ë¸ì€ Variance Errorë¥¼ ë§ì´ ì¤„ì—¬ë‘” ìƒíƒœë¼ê³ ë„ ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹íˆë‚˜ Dropoutê°™ì€ ê²½ìš° 0.5 ì •ë„ë¡œ í¬ê²Œ ê±¸ì–´ì£¼ì—ˆê¸° ë•Œë¬¸ì—, Dropoutì´ Ensembleê³¼ ìœ ì‚¬í•œ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆì´ë¯€ë¡œ, í•œë²ˆ ì´ ìƒíƒœì—ì„œ Ensembleì˜ íš¨ê³¼ê°€ ê³¼ì—° ì¶”ê°€ì ìœ¼ë¡œ ìˆì„ì§€ ë³´ë„ë¡ í•˜ì.



> Model Code

```python
BATCH_SIZE = 2048 
LEARNING_RATE = 0.001

NUM_INPUT = x_train.shape[1]
NUM_OUTPUT = 1 
NUM_1ST_HIDDEN = 32 
NUM_2ND_HIDDEN = 16 
NUM_1ST_DROPOUT = 0.6
NUM_2ND_DROPOUT = 0.5

class BasicRegressor(nn.Module):
    def __init__(self) -> None:
        super(BasicRegressor, self).__init__()

        self.layer_1 = nn.Linear(NUM_INPUT, NUM_1ST_HIDDEN)
        self.layer_2 = nn.Linear(NUM_1ST_HIDDEN, NUM_2ND_HIDDEN)
        self.layer_out = nn.Linear(NUM_2ND_HIDDEN, NUM_OUTPUT)

        # self.actvation = nn.ReLU()
        self.actvation_1 = nn.ReLU()
        self.actvation_2 = nn.ReLU()
        self.dropout_1 = nn.Dropout(p=NUM_1ST_DROPOUT)
        self.dropout_2 = nn.Dropout(p=NUM_2ND_DROPOUT)
        self.batchnorm_1 = nn.BatchNorm1d(NUM_1ST_HIDDEN)
        self.batchnorm_2 = nn.BatchNorm1d(NUM_2ND_HIDDEN)
    
    def forward(self, inputs):
        x = self.actvation_1(self.layer_1(inputs))
        x = self.batchnorm_1(x)
        x = self.dropout_1(x)
        x = self.actvation_2(self.layer_2(x))
        x = self.batchnorm_2(x)
        x = self.dropout_2(x)
        x = self.layer_out(x)

        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

```



> Training Code

í•™ìŠµì€ êµ‰ì¥íˆ ë‹¨ìˆœí•˜ë‹¤. í•˜ë‚˜ì˜ ëª¨ë¸ì„ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ëë‚œë‹¤. :) 

```python
num_train_data = len(train_loader)
num_eval_data = len(valid_loader)


elapsed_time_basic_ann = []

start_time = datetime.now()


best_model = train_model(num_train_data, num_eval_data)


elapsed_time_basic_ann.append((datetime.now()-start_time).total_seconds())
```



> Inference Code

Inferenceë„ êµ‰ì¥íˆ ë‹¨ìˆœí•˜ë‹¤. ì €ì¥ëœ Best Model 1ê°œë¡œ Test Datasetì— ëŒ€í•´ Evaluationí•˜ê³ , ê·¸ì— ëŒ€í•œ ê°œë³„ Lossë¥¼ êµ¬í•œë‹¤. (Few Shotê³¼ Many Shotì— ëŒ€í•œ ê°œë³„ì  L1 Lossë¥¼ ê³„ì‚°í•¨)

```python
best_model.eval()
data = torch.from_numpy(x_test).float().to(device)
answer = torch.from_numpy(y_test).float().to(device)

start_time = datetime.now()
output = best_model(data)
loss_basic_ann = calc_loss(output, answer)
elapsed_time_basic_ann.append((datetime.now()-start_time).total_seconds())

print('elapsed time ', elapsed_time_basic_ann)
```





### Ensemble MLP

Ensemble MLPì˜ ê²½ìš°ëŠ” ìœ„ì˜ MLP ëª¨ë¸ê³¼ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì ¸ê°„ë‹¤. ë‹¨ì§€ í•™ìŠµí•  ì‹œì—, NUM_ENSEMBLE_MODELSì— ë“¤ì–´ìˆëŠ” Integer Valueì— ë”°ë¼ì„œ, Baggingì˜ ê°œìˆ˜ë¥¼ ì •í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ì˜ ì˜ˆì œëŠ” 6ê°œì˜ Ensemble Learningì„ ì§„í–‰í•˜ë©°, ê°ê°ì˜ Best Modelì„ ì €ì¥í•˜ëŠ” Training Codeì´ë‹¤. ì €ì¥ëœ ëª¨ë¸ë“¤ì€ best_modelsë¼ëŠ” listì— ì €ì¥ëœë‹¤.

> Training Code

```python
NUM_ENSEMBLE_MODELS = 6 # or 3

best_models = []
for i in range(NUM_ENSEMBLE_MODELS):
    model = BasicRegressor()
    model.to(device)

    # criterion = nn.L1Loss()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    bagg_indices = np.random.choice(range(len(x_train)), len(x_train), replace=True)

    x_train_bagg = x_train[bagg_indices, :]
    y_train_bagg = y_train[bagg_indices, :]
    # train_data = TrainData(torch.FloatTensor(x_train), torch.FloatTensor(y_train))
    train_data = TrainData(torch.FloatTensor(x_train_bagg), torch.FloatTensor(y_train_bagg))
    train_loader = DataLoader(dataset=train_data, batch_size=2048, shuffle=True)


    num_train_data = len(train_loader)
    num_eval_data = len(valid_loader)


    elapsed_time_basic_ann = []
    start_time = datetime.now()

    best_model = train_model(num_train_data, num_eval_data)

    best_models.append(best_model)


```



> Inference Code

ì•„ë˜ì˜ ì½”ë“œëŠ” Ensembleì„ Aggregationí•˜ëŠ” ì½”ë“œì´ë‹¤. ë‹¨ìˆœíˆ ëª¨ë¸ë“¤ì˜ Predictionì„ Averageí•˜ì—¬ Regression Outputê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤. Baggingì´ë¯€ë¡œ ì´ëŸ¬í•œ ë°©ì‹ì´ í•©ë¦¬ì ì´ë¼ê³  ìƒê°í•œë‹¤.

```python
# inference
sum_output = np.zeros(y_test.shape)

for best_model in best_models:
    best_model.eval()
    output = best_model(data)
    sum_output += output.cpu().detach().numpy()

avg_output = sum_output / len(best_models)
```





### Ensemble MLP with REBAGG

í•´ë‹¹ ë°©ë²•ì€ Ensembleì„ í•´ì„œ í•™ìŠµí•  ë•Œ, Dataë¥¼ ë‹¨ìˆœíˆ Baggingí•˜ì—¬ Replacement Samplingë§Œ ì ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, Rare(Few Shot)ì™€ Normal(Many Shot)ì— ëŒ€í•œ Thresholdë¥¼ ê¸°ë°˜ìœ¼ë¡œ, Rare Labelì„ ë” ë§ì´ Random Oversamplingì„ í•˜ë©´ì„œ ê° Ensembleì˜ Moduleì„ í•™ìŠµí•˜ëŠ” ê¸°ë²•ì´ë‹¤. êµ‰ì¥íˆ ë‹¨ìˆœí•œ ê¸°ë²•ì´ë©°, Random Oversamplingì™¸ì— Undersampling, Gaussian Noise Adding, SMOGN ë“±ì˜ ì—¬ëŸ¬ ë°ì´í„° Over/Under Samplingì„ ê²°í•©í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. (ì‚¬ì‹¤ ê°œì¸ì ìœ¼ë¡œ SMOTEê³„ì—´ ë°©ë²•ë“¤ì€ ì„ í˜¸í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.)



> Training Code

```python
rare_indicies = np.where(y_train>threshold_rare)[0]
normal_indicies = np.where(y_train<=threshold_rare)[0]

ov_rare_indicies = np.random.choice(range(len(rare_indicies)), len(normal_indicies), replace=True)

x_train_normal_bagg = x_train[normal_indicies, :]
y_train_normal_bagg = y_train[normal_indicies, :]


x_train_rare_bagg = x_train[ov_rare_indicies, :]
y_train_rare_bagg = y_train[ov_rare_indicies, :]


best_models = []
for i in range(NUM_ENSEMBLE_MODELS):
    model = BasicRegressor()
    model.to(device)

    # criterion = nn.L1Loss()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    bagg_indices = np.random.choice(range(len(x_train)), len(x_train), replace=True)
    # x_train_bagg = x_train[bagg_indices, :]
    # y_train_bagg = y_train[bagg_indices, :]

    rare_indicies = np.where(y_train>threshold_rare)[0]
    normal_indicies = np.where(y_train<=threshold_rare)[0]

    ov_rare_indicies = np.random.choice(range(len(rare_indicies)), len(normal_indicies), replace=True)

    x_train_normal_bagg = x_train[normal_indicies, :]
    y_train_normal_bagg = y_train[normal_indicies, :]


    x_train_rare_bagg = x_train[rare_indicies, :]
    y_train_rare_bagg = y_train[rare_indicies, :]


    x_train_total_bagg = np.append(x_train_normal_bagg, x_train_rare_bagg, axis=0)
    y_train_total_bagg = np.append(y_train_normal_bagg, y_train_rare_bagg, axis=0)

    

    
    # train_data = TrainData(torch.FloatTensor(x_train), torch.FloatTensor(y_train))
    train_data = TrainData(torch.FloatTensor(x_train_total_bagg), torch.FloatTensor(y_train_total_bagg))
    train_loader = DataLoader(dataset=train_data, batch_size=2048, shuffle=True)


    num_train_data = len(train_loader)
    num_eval_data = len(valid_loader)


    elapsed_time_basic_ann = []
    start_time = datetime.now()

    best_model = train_model(num_train_data, num_eval_data)

    best_models.append(best_model)


```



> Inference Code

Trainingì—ë§Œ ë‹¤ë¥´ì§€, InferenceëŠ” ë‹¨ìˆœ Ensemble Learningê³¼ ë™ì¼í•˜ë‹¤. best_modelsì— ì €ì¥ëœ ëª¨ë¸ë“¤ì„ ì—¬ëŸ¬ Ensembleë¡œ ì˜ˆì¸¡í•´ ì£¼ê³  Outputì„ Averageí•´ ì¤€ë‹¤.

```python
# inference
sum_output = np.zeros(y_test.shape)

for best_model in best_models:
    best_model.eval()
    output = best_model(data)
    sum_output += output.cpu().detach().numpy()

avg_output = sum_output / len(best_models)
```





## 4. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : MAE (Mean Absolute Error)
- Datasetì€ Testset 20%, Training 64%, Validation 16%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨
- 3ê°œì˜ Datasetì— ëŒ€í•œ ê°ê°ì˜ LossëŠ” 3ê°€ì§€ë¡œ êµ¬ë¶„ëœë‹¤.
  - ì „ì²´ì˜ Average(Avg)
  - Normal Distribution(Many Shot)
  - Rare Distribution(Few Shot)


|      | Algorithm                     | Diabetes (Avg) | Diabetes (Many Shot) | Diabetes (Few Shot) | Boston House (Avg) | Boston House (Many Shot) | Boston House (Few Shot) | California House (Avg) | California House (Many Shot) | California House (Few Shot) |
| ---- | ----------------------------- | -------------- | -------------------- | ------------------- | ------------------ | ------------------------ | ----------------------- | ---------------------- | ---------------------------- | --------------------------- |
| 1    | MLP                           | 46.90          | 39.84                | 92.18               | 2.60               | 2.07                     | 8.09                    | 0.44                   | **0.36**                     | 1.00                        |
| 2    | Ensemble MLP (x3)             | **43.24**      | **36.47**            | **86.55**           | **2.41**           | 1.99                     | **6.71**                | 0.44                   | 0.37                         | **0.93**                    |
| 3    | Ensemble MLP with REBAGG (x3) | 44.35          | 37.38                | 89.11               | 2.59               | 2.10                     | 7.64                    | 0.44                   | **0.36**                     | 1.00                        |
| 4    | Ensemble MLP (x6)             | 43.88          | 36.60                | 90.60               | 2.50               | 2.06                     | 7.07                    | 0.44                   | 0.37                         | 0.94                        |
| 5    | Ensemble MLP with REBAGG (x6) | 44.70          | 37.66                | 89.92               | 2.42               | **1.96**                 | 7.11                    | 0.44                   | 0.37                         | 0.95                        |



----


# Final Insights





# Conclusion

- Anoamaly Detectionì€ ê·¸ í•œê³„ì„±ë„ ë¶„ëª…íˆ ìˆìœ¼ë¯€ë¡œ, ë¬´ì§€ì„±ìœ¼ë¡œ ì‰½ê²Œ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ê° ë¬¸ì œê°€ ê°–ê³  ìˆëŠ” ê·¼ë³¸ì ì¸ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ì í•©í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ ì ìš©ì„ í•´ì•¼ í•œë‹¤.



-----

# References

-  ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ê°•í•„ì„± êµìˆ˜ë‹˜ Business Analytics ê°•ì˜ ìë£Œ
- https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422
- [ZhiningLiu1998/imbalanced-ensemble: Class-imbalanced / Long-tailed ensemble learning in Python. Modular, flexible, and extensible. | æ¨¡å—åŒ–ã€çµæ´»ã€æ˜“æ‰©å±•çš„ç±»åˆ«ä¸å¹³è¡¡/é•¿å°¾æœºå™¨å­¦ä¹ åº“ (github.com)](https://github.com/ZhiningLiu1998/imbalanced-ensemble)
- [Imbalanced Classification | Handling Imbalanced Data using Python (analyticsvidhya.com)](https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/)
- [Ensemble Methods - Overview, Categories, Main Types (corporatefinanceinstitute.com)](https://corporatefinanceinstitute.com/resources/data-science/ensemble-methods/)
- [Random Forest vs Xgboost | MLJAR](https://mljar.com/machine-learning/random-forest-vs-xgboost/)
