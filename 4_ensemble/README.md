# ğŸ¤”Ensembleì€ Imablanced Dataì—ì„œë„ íš¨ê³¼ë¥¼ ë³´ì¼ê¹Œ?

## The effect of ensemble learning in imbalanced regression datasets

![image-20221130231343857](./attachments/image-20221130231343857.png)



ğŸ”¥ì´ë²ˆ Tutorialì—ì„œëŠ” **Imbalanced Regression Datasetì—ì„œ Ensemble Learningì´ íš¨ê³¼**ê°€ ìˆì„ì§€ì— ëŒ€í•´ì„œ Tutorialì„ ì§„í–‰í•˜ê³ ì í•œë‹¤. íŠ¹íˆ Ensemble Learningê¸°ë²• ì¤‘ **Baggingì„ ì‚¬ìš©í•˜ì—¬ Deep Learning ëª¨ë¸**ì—ì„œ íš¨ê³¼ì„±ì„ ë³´ê³ ì í•œë‹¤.



- ìµœê·¼ì˜ ëŒ€ë¶€ë¶„ì˜ SoTA ê¸°ë²•ë“¤ì€ Deep Learningê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì–´ ì§€ê³  ìˆë‹¤. Tabular DataëŠ” ì•„ì§ ì •ë³µí•˜ì§€ ëª»í•œ ê°ì´ ìˆìœ¼ë‚˜, ê·¸ê²ƒë„ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì ì°¨ì ìœ¼ë¡œ í•´ê²°ë˜ê³  ìˆë‹¤.

  - [[1908.07442\] TabNet: Attentive Interpretable Tabular Learning (arxiv.org)](https://arxiv.org/abs/1908.07442)
  - [[2106.11189\] Well-tuned Simple Nets Excel on Tabular Datasets (arxiv.org)](https://arxiv.org/abs/2106.11189)

- ê·¸ë¦¬ê³  Real-Worldì˜ Dataë“¤ì€ ì–´ë–»ê²Œë“  Imbalanced í•œ Dataë¥¼ ê°–ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì´ë‹¤.

- ê·¸ëŸ¬ë‚˜ ì•„ì§ Imbalanced Data ì¤‘ Regression Taskì— ëŒ€í•œ ì—°êµ¬ëŠ” ë§ì´ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìœ¼ë©°, ìµœê·¼ 2021ë…„ë¶€í„° ê´€ë ¨ëœ Deep Learningê¸°ë°˜ Imbalanced Regression Taskë¥¼ í’€ë ¤í•˜ëŠ” ì‹œë„ê°€ ì´ë£¨ì–´ì§€ê³  ìˆë‹¤.

- ì•„ì§ Ensemble Learningì„ í†µí•´ í•´ë‹¹ Imbalanced Regression Taskë¥¼ í’€ë ¤ëŠ” ì‹œë„ëŠ” ê·¹ì†Œìˆ˜ë§Œ ì¡´ì¬í•˜ë©°, ë”°ë¼ì„œ ì´ë²ˆ Tutorialì—ì„œëŠ” í•´ë‹¹ Taskì— Niaveí•˜ê²Œ Ensemble Learningì„ ì ìš©í•´ ë³´ë©° ê·¸ íš¨ê³¼ì„±ì— ëŒ€í•´ì„œ ìƒê° í•´ ë³´ê³ ì í•œë‹¤.

  - [REBAGG: REsampled BAGGing for Imbalanced Regression (mlr.press)](https://proceedings.mlr.press/v94/branco18a.html)

  



# Table of Contents

- [Background of Anomaly Detection](#Background-of-Anomaly-Detection)

  - [1. Basic Concept](#1-Basic-Concept)
  - [2. One-Class SVM](#2-One-Class-SVM)
  - [3. Isolation Forest](#3-Isolation-Forest)
  - [4. Auto-Encoder for Anomaly Detection](#4-Auto-Encoder-for-Anomaly-Detection)
  - [5. Mixture of Gaussian](#5-Mixture-of-Gaussian)

- [Tutorial 1. Regression To Anomaly Detection](#Tutorial-1-Regression-To-Anomaly-Detection)

  - [1-1. Tutorial Notebook](#1-1-Tutorial-Notebook)
  - [1-2. Setting](#1-2-Setting)
  - [1-3. Usage Code](#1-3-Usage-Code)
  - [1-4. Result (Accuracy)](#1-4-Result_Accuracy)

- [Tutorial 2. Classification To Anomaly Detection](#Tutorial-2-Classification-To-Anomaly-Detection)

  - [2-1. Tutorial Notebook](#2-1-Tutorial-Notebook)
  - [2-2. Setting](#2-2-Setting)
  - [2-3. Usage Code](#2-3-Usage-Code)
  - [2-4. Result (Accuracy)](#2-4-Result_Accuracy)

- [Final Insights](#Final-Insights)

  - [1. Regression To Anomaly Detection](#1-Regression-To-Anomaly-Detection)
  - [2. Classification To Anomaly Detection](#2-Classification-To-Anomaly-Detection)
  - [3. Conclusion](#3-Conclusion)

- [References](#References)

  

-------

# Background of Ensemble Learning

## 1. Basic Concept

![image-20221130225055439](./attachments/image-20221130225055439.png)



ë‹¨ì§€ í•˜ë‚˜ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ëª¨ë“  Datasetì— ëŒ€í•´ì„œ, ëª¨ë‘ ì˜ ë™ì‘ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.(No Free Lunch) ì™œëƒí•˜ë©´ ëŒ€ë¶€ë¶„ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ì–´ë– í•œ í˜„ì‹¤ì— ëŒ€í•œ ê°„ëµí™” í˜¹ì€ ê°€ì •(Assumtion)ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ”ë°, ì´ëŸ¬í•œ ê°€ì •ì€ íŠ¹ì • Caseì—ëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.



ì‹¤ì œì ìœ¼ë¡œ ì•„ë˜ì˜ ë…¼ë¬¸ì—ì„œëŠ” ìˆ˜ë°±ê°œì˜ Classifierë“¤ì„ í™œìš©í•˜ì—¬ ìš°ë¦¬ê°€ ì›í•´ ë§ˆì§€ì•ŠëŠ”, Master Algorithmì´ ì‹¤ì œë¡œëŠ” ì¡´ì¬í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ Emiricalí•œ Testë¡œ ë°í˜€ ë‚´ì—ˆë‹¤.

- [Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? (jmlr.org)](https://jmlr.org/papers/v15/delgado14a.html)



> ì™œ ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë¸ì€ True Real Modelë¡œì¨ ì™„ë²½íˆ ì˜ í•™ìŠµë˜ì§€ ì•Šì„ê¹Œ?

ì™œëƒí•˜ë©´ ì‹¤ì œ Dataì—ëŠ” í•­ìƒ Noiseê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— Machine Learning ëª¨ë¸ì´ í•­ìƒ ì •í™•í•œ ì¶”ì •ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë˜í•œ Populationì„ ì„¤ëª…í•  ë§Œí¼ì˜ Data Sampleì´ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ê³ , Training Setê³¼ Testing Setì˜ ë¶„í¬ê°€ ë‹¤ë¥¸ ë“±ì˜ ë¬¸ì œê°€ í•­ìƒ ì¡´ì¬í•œë‹¤.

íŠ¹íˆë‚˜ Dataìƒì— Noiseê°€ ì¡´ì¬í•˜ëŠ” ë¬¸ì œ ìƒí™©ì—ì„œ, Machine Learningëª¨ë¸ì€ Modelì˜ Complexityì— ë”°ë¼, ë‹¨ìˆœí•œ(Simple) ëª¨ë¸ì€ Bias Errorê°€ ì»¤ì§€ê³ , ë³µì¡í•œ(Complex) ëª¨ë¸ì€ Variance Errorê°€ ì»¤ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤. 



![image-20221130225915539](./attachments/image-20221130225915539.png)

- Bias (í¸í–¥) Error
  - ë°˜ë³µ ëª¨ë¸ í•™ìŠµ ì‹œ, **í‰ê· ì **ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì •í™•í•œ ì¶”ì •ì´ ê°€ëŠ¥í•œì§€ ì¸¡ì •
  - **ëª¨ë¸ì´ ë‹¨ìˆœ**í•˜ë©´, Biasê°€ ì»¤ì§€ëŠ” ê²½í–¥

- Vriance (ë¶„ì‚°) Error
  - ë°˜ë³µ ëª¨ë¸ í•™ìŠµ ì‹œ, **ê°œë³„ ì¶”ì •**ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œ ì¶”ì •ì´ ê°€ëŠ¥í•œì§€ ì¸¡ì •
  - **ëª¨ë¸ì´ ë³µì¡**í•˜ë©´, Varianceê°€ ì»¤ì§€ëŠ” ê²½í–¥



ì´ëŸ¬í•œ ëª¨ë¸ì˜ Complexityì— ëŒ€ì‘í•˜ê³ , í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ëª¨ë“  Real-Worldì˜ Caseë¥¼ ëŒ€ì‘í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œì ì„ í•´ê²°í•˜ê³ ì, ë‹¤ì–‘í•œ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” Ensemble Learning ê¸°ë²•ì´ ë“±ì¥í•˜ì˜€ë‹¤. Ensemble Learningì˜ í•µì‹¬ì€ ê°œë³„ ëª¨ë¸ë“¤ì„ í•©ì³ì„œ ì¶©ë¶„í•œ **ëª¨ë¸ì˜ ë‹¤ì–‘ì„±(Diversity)ë¥¼ í™•ë³´**í•˜ê³ , ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ê°œë³„ ëª¨ë¸ì„ **ì–´ë–»ê²Œ ì˜ ê²°í•© í• ì§€**ì— ëŒ€í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì œì‹œí•œë‹¤.



ê·¸ë¦¬ê³  ìœ„ì—ì„œ ë§í•œ Complexityì— ë”°ë¥¸ Biasê³¼ Varianceì˜ Errorë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œì¨, ì•„ë˜ì˜ 3ê°€ì§€ ë¶„ë¥˜ë¡œ Ensemble Learningì€ ë¶„íŒŒë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ì´ ì¤‘ì— ìš°ë¦¬ëŠ” ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” Baggingê³¼ Boostingì— ëŒ€í•´ ì•Œì•„ë³´ê² ë‹¤.

![image-20221130230410829](./attachments/image-20221130230410829.png)



## 2. Bagging

ë¶„ì‚°(Variance) Errorê°€ ë†’ê³  í¸í–¥(Bias) Errorê°€ ë‚®ì€ ëª¨ë¸ì— ì í•©í•œ, ì¦‰ **Complexityê°€ ë†’ì€ ëª¨ë¸**(ex. ANN, SVM ë“±)ì— ì í•©í•œ ë°©ë²•ì¸ Baggingì„ ê°„ë‹¨íˆ ì•Œì•„ë³´ì. Baggingì„ ê°„ë‹¨íˆ ìš”ì•½í•˜ìë©´, ë°ì´í„°ë¥¼ Splití•˜ì—¬ ëª¨ë¸ì˜ ì°¨ë³„í™”ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê¸°ë²•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

![image-20221130230845487](./attachments/image-20221130230845487.png)



Baggingì€ ì‚¬ì‹¤ ë‹¨ìˆœí•œ ê¸°ë²•ì´ë‹¤. ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë‹¨ì§€ Dataë¥¼ Samplingí• ë•Œ Dataì˜ Sampleì— ëŒ€í•œ ë°©ì‹ì„ ë‹¤ì–‘í•˜ê²Œ ì£¼ì–´ì„œ, ëª¨ë¸ì˜ Diversityë¥¼ ë†’ì´ëŠ” ë°©ë²•ì´ ì „ë¶€ì´ê¸° ë•Œë¬¸ì´ë‹¤.(ì¦‰ Resamplingê¸°ë²•ì´ë‹¤.) ì‚¬ì‹¤ Implicití•˜ê²Œ Ensemble Modelì„ ë§Œë“œëŠ” ë°©ì‹ì¸ Data Resampling Ensembleì€ í¬ê²Œ 2ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤. í•˜ë‚˜ëŠ” ë¹„ë³µì› ì¶”ì¶œë°©ì‹ì¸ **Pasting**ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë³µì›ì¶”ì¶œ ë°©ì‹ì¸ **Bagging**ì´ë‹¤. ì´ë²ˆì—ëŠ” ìš°ë¦¬ëŠ” Baggingì— ì§‘ì¤‘í•´ì„œ ìŠ¤í„°ë”” í•´ë³´ê² ë‹¤.

![image-20221130231528610](./attachments/image-20221130231528610.png)



Baggingê¸°ë²•ì€ ëª¨ë¸ê³¼ ìƒê´€ì—†ì´ ì§„í–‰ë  ìˆ˜ ìˆëŠ” ê¸°ë²•ì´ë©°, ì¦‰ ì–´ë– í•œ Complexityê°€ ë†’ì€ ëª¨ë¸ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ Deep Learning ëª¨ë¸ì—ì„œ í™œìš©í•˜ê¸° ìš©ì´í•˜ë©°, ì´ë²ˆ Tutorialì—ì„œë„ Deep Learningëª¨ë¸ì„ í™œìš©í•˜ë¯€ë¡œ, Bagging ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•  ì˜ˆì •ì´ë‹¤.



> ì´ë¦„ì€ ë“¤ì–´ë´¤ê² ì§€? ëŒ€í‘œì  Bagging ì•Œê³ ë¦¬ì¦˜ì¸ Random Forests

ìœ ëª…í•œ Baggingê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” Random Forestsê°€ ìˆë‹¤. ì•„ë˜ì™€ ê°™ì´ Depthê°€ ìˆëŠ”(ì¦‰, Complexityë¥¼ ë†’ì¸) Treeë“¤ì„ ì—¬ëŸ¬ê°œ ì‚¬ìš©í•˜ì—¬ Dataë¥¼ Baggingì„ í†µí•´ ê° ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê¸°ë²•ì´ë‹¤.

![image-20221130231851344](./attachments/image-20221130231851344.png)



ì¶”ê°€ì ìœ¼ë¡œëŠ” ì•„ë˜ ê·¸ë¦¼ì˜ ìš°ì¸¡ê³¼ê°™ì´, Treeë¥¼ ë¶„ê¸°í•  ë•Œ Random Feature Selectionì„ í†µí•´ ëª¨ë¸ì˜ ë‹¤ì–‘ì„±ì„ ì¶”ê°€ì ìœ¼ë¡œ ê°€ì ¸ê°€ê²Œ ëœë‹¤. ê°œë…ì ìœ¼ë¡œëŠ” ë‹¨ìˆœí•œë°, Tabular Datasetì— ëŒ€í•˜ì—¬ ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì ¸ê°ˆ ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.

![image-20221130231840378](./attachments/image-20221130231840378.png)



ë˜í•œ Baggingì˜ ì¥ì ì€, ë‹¨ì§€ Data Samplingì— ëŒ€í•œ ë¬¸ì œì´ê¸° ë•Œë¬¸ì—, ê°œë³„ ëª¨ë¸ë“¤ì„ Parallelí•˜ê²Œ ë³‘ë ¬ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. ì¦‰ Multi-Processingí™˜ê²½ì—ì„œëŠ” ë‹¤ë¥¸ Ensemble ê¸°ë²•ë³´ë‹¤ ìš°ì›”í•œ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë‹¤.



## 3. Boosting

ë‘ë²ˆì§¸ë¡œëŠ” ë¶„ì‚°(Variance) Errorê°€ ë‚®ê³  í¸í–¥(Bias) Errorê°€ ë†’ì€ ëª¨ë¸ì— ì í•©í•œ, ì¦‰ **Complexityê°€ ë‚®ì€ ëª¨ë¸**(ex. Decision Stumpë“±)ì— ì í•©í•œ ë°©ë²•ì¸ Boostingì„ ê°„ë‹¨íˆ ì•Œì•„ë³´ì. Boostingì„ ê°„ë‹¨íˆ ìš”ì•½í•˜ìë©´, Sequentialí•˜ê²Œ ê°ê°ì˜ ëª¨ë¸ë“¤ì„ í•˜ë‚˜ì”© í•™ìŠµí•´ ë‚˜ê°€ë©°, ì´ì „ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ì´ ì˜ ì•ˆë˜ëŠ” ê²ƒì„ ë‹¤ìŒ ëª¨ë¸ì´ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµí•´ ë‚˜ê°€ëŠ” ë°©ë²•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. Boostingë„ ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, Baggingê³¼ëŠ” ì¡°ê¸ˆ ë‹¬ë¦¬ íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì„ ê°€ë¦¬í‚¤ëŠ” ê²½ìš°ê°€ ë§ì´ ìˆë‹¤. (ex. Adaboost, XGBoost, LightGBM ë“±)

![image-20221130232443992](./attachments/image-20221130232443992.png)





----

# Tutorial. Ensemble learning in imbalanced regression task

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì•ì„œ ì„¤ëª…í•œ ê²ƒê³¼ ê°™ì´ ê·¼ë³¸ì ìœ¼ë¡œ Regressionì¸ Taskë¥¼ Thresholdë¥¼ í†µí•´ Anomaly Detection (ì¼ì¢…ì˜ One-Class Binary Classification)ì´ ê°€ëŠ¥í• ì§€ ì•Œì•„ë³´ëŠ” ì‹¤í—˜ì´ë‹¤. í•´ë‹¹ ì‹¤í—˜ì„ ìœ„í•´ ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ Regression(SVR)ê³¼ ì—¬ëŸ¬ Anomaly Detection ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ë¹„êµ í•˜ê³ ì í•œë‹¤.

![image-20221117010800581](./attachments/image-20221117010800581.png)

ìœ„ì™€ ê°™ì€ Logistic Regressionì´ ì•„ë§ˆ ìœ ì‚¬í•œ ê°œë…ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. Regression ê²°ê³¼(Logit)ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•˜ì—¬(Logistic), 0.5ë¼ëŠ” Thresholdë¡œ ë‚˜ëˆ ì„œ Classificationì„ í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•œ ê°œë…ìœ¼ë¡œ Regressionì„ ì‚¬ìš©í•´ Thresholdí•˜ì—¬ Classificationì„ í•˜ëŠ” ì•„ì£¼ ì§ê´€ì ì¸ ë°©ë²•ê³¼ Anomaly Detectionì˜ ë¹„êµë¼ê³  ì´í•´í•˜ë©´ ë˜ê² ë‹¤.



## 1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/3_anomaly_detection/Tutorials/tutorial_anomaly_detection_from_R_task.ipynb)



## 2. Setting

### Datasets

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì´ 2ê°œì˜ ìœ ëª…í•œ Tabular í˜•íƒœì˜ Regression Datasetì„ ì‚¬ìš©í•œë‹¤. ë‘ê°œì˜ Datasetëª¨ë‘ Regression Targetì´ë¯€ë¡œ Thresholdingì„ í†µí•´ ëª©ì ì— ë§ê²Œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•œë‹¤. ì „ì²´ ë°ì´í„° ì¤‘ Training Setì€ 64%, Validation Setì€ 16%, Test Setì€ 20%ì˜ Dataë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.

|      | Datasets                        | Description                                                  | Num Instances | Num Inputs (Xs) | Num Outputs (Ys) |
| ---- | ------------------------------- | ------------------------------------------------------------ | ------------- | --------------- | ---------------- |
| 1    | Diabetes (Regression)           | ë‹¹ë‡¨ë³‘ í™˜ì ë°ì´í„° (1ë…„ í›„ ë‹¹ë‡¨ì˜ ì§„í–‰ì •ë„ë¥¼ Targetê°’ìœ¼ë¡œ í•¨) | 442           | 10              | 1                |
| 2    | Boston House Price (Regression) | Bostonì˜ ì§‘ê°’ì— ëŒ€í•œ Data                                    | 506           | 13              | 1                |

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê²Œ ëœë‹¤.

```python
if dataset_name == 'diabetes_r':
    x, y= datasets.load_diabetes(return_x_y=true)
elif dataset_name == 'boston_r':
    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=none)
    x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    y = raw_df.values[1::2, 2]
else:
    pass
```

ê° Datasetì€ Regression Targetì´ë¯€ë¡œ, ê° Datasetì„ Anomalyì— ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ ì‚¬ìš©ë˜ëŠ” Thresholdê°’ì€ ì•„ë˜ì™€ ê°™ë‹¤. ê° ê°’ì€ ì „ì²´ ë°ì´í„°ì˜ Median ê°’ì´ë‹¤. Regression Taskì— Imbalancedì— ì˜í•œ ì˜í–¥ì„ ì¤„ì´ê¸° ìœ„í•´ ì¤‘ì•™ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì–‘ë¶ˆ Dataì˜ Balanceë¥¼ ë§ì¶”ì—ˆë‹¤.

- **Diabetes : 140** 
- **Boston House Price : 21**





### Algorithms

ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ Regression ì•Œê³ ë¦¬ì¦˜ê³¼ Anomaly Detectionì„ ì„œë¡œ ë¹„êµí•œë‹¤.

- Regerssion 
  - SVRì„ ì‚¬ìš©í•˜ì—¬ Regression Taskì—ì„œ Regression Algorithmì„ ì‚¬ìš©í•˜ê³  ì˜ˆì¸¡í•œ ê°’ì„ íŠ¹ì • Thresholdë¡œ Classificationí•˜ì—¬ ì–‘ë¶ˆì„ íŒì •í•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤.
- Anomaly Detection
  - 4ê°€ì§€ì˜ ì•Œê³ ë¦¬ì¦˜(One-Class SVM, Isolation Forest, Autoencoder Anomaly Detection, Mixture Of Gaussian)ì„ ì‚¬ìš©í•˜ì—¬, ë°ì´í„°ë¥¼ ì–‘ë¶ˆë¡œ Binary Classificationë¬¸ì œë¡œ ì „ì²˜ë¦¬ í›„, ì–‘í’ˆ ë°ì´í„°ë§Œì„ í•™ìŠµí•˜ì—¬ Anomalyë¥¼ íƒì§€í•œë‹¤.

|      | Algorithm                              | Target            | Description                                                  |
| ---- | -------------------------------------- | ----------------- | ------------------------------------------------------------ |
| 1    | Linear SVR                             | Regression        | ì„ í˜• SVR                                                     |
| 2    | Kernel SVR                             | Regression        | ì„ í˜• SVR + Kernel Trick(using rbf kernel)                    |
| 3    | One-Class SVM                          | Anomaly Detection | ì–‘í’ˆ Sampleë§Œìœ¼ë¡œ í•™ìŠµí•˜ì—¬ Anomaly Detectionì„ ìˆ˜í–‰í•˜ëŠ” SVMì˜ ë³€í˜• ë²„ì „(Nu-SVM). ì–‘í’ˆ Sample Dataê°€ ì›ì ì—ì„œ ê°€ì¥ ë©€ì–´ì§€ê²Œ í•˜ëŠ” Hyper Planeì„ ì°¾ëŠ”ë‹¤. |
| 4    | Isolation Forest                       | Anomaly Detection | ì–‘í’ˆ Sampleë§Œìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ê°„ë‹¨í•œ Decision Tree ì¡°í•©ì„ í†µí•´ Anomalyë¥¼ Detectioní•˜ëŠ” ì•Œê³ ë¦¬ì¦˜. ë¶„ë¥˜ Path Lengthê°€ ê¸¸ìˆ˜ë¡ ì–‘í’ˆì´ë‹¤. |
| 5    | Autoencoder<br />for Anomaly Detection | Anomaly Detection | ì–‘í’ˆ Sampleë§Œì„ í†µí•´ Neural Networkê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ê³ , ë™ì¼í•˜ê²Œ Reconstructioní•˜ëŠ” Taskë¥¼ ìˆ˜í–‰í•˜ì—¬, Anomaly Detectioní•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ |
| 6    | Mixture of Gaussian                    | Anomaly Detection | ì—¬ëŸ¬ê°œì˜ Gaussianì˜ ì„ í˜• ê²°í•©ì„ í†µí•´ ë¶„í¬ë¥¼ ë²—ì–´ë‚˜ëŠ” Dataë¥¼ ì°¾ì•„ë‚´ì–´ Anomaly Detectionì„ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ |



## 3. Usage Code

### Normal Neural Network

ì„±ëŠ¥ì´ ì–´ëŠì •ë„ ê²€ì¦ëœ ê¸°ë²•ì¸ SVRì„ ì‚¬ìš©í•˜ì—¬, Regression Taskë¥¼ ì˜ˆì¸¡í•œë‹¤. ê·¸ë¦¬ê³  ì˜ˆì¸¡ëœ ê²°ê³¼ë¥¼ Thresholdë¡œ ë‚˜ëˆ„ì–´, ì–‘ë¶ˆì„ íŒì •í•œë‹¤. ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ í•™ìŠµê³¼ ì¶”ë¡ í•˜ì—¬ Regressionì„ ì˜ˆì¸¡í•œë‹¤. Linear SVRê³¼ RBF SVRì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchí•˜ì—¬ ëª¨ë¸ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'C': [1.0, 2.0, 3.0, 10., 30., 100.]},
    {'kernel': ['rbf'], 'C': [1.0, 2.0, 3.0, 5.0, 10., 30., 100.],
    'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},
]

elapsed_time_kernel_svr = []

svr_regressor = SVR(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svr_regressor, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svr_regressor = grid_search.fit(x_train, y_train)
elapsed_time_kernel_svr.append((datetime.now()-start_time).total_seconds())

start_time = datetime.now()
y_pred = best_svr_regressor.predict(x_test)
elapsed_time_kernel_svr.append((datetime.now()-start_time).total_seconds())

```



ì•„ë˜ì™€ ê°™ì´ ì˜ˆì¸¡í•œ ê°’ì„ ìœ„ì—ì„œ ì„¤ì •í•œ thresholdê°’ìœ¼ë¡œ ì–‘ë¶ˆ(ì–‘í’ˆ +1, ë¶ˆëŸ‰ -1) Labelingì„ í•´ ì¤€ë‹¤. ì´ë¥¼ í†µí•´ì„œ Answer Yê°’ì˜ Classificationëœ ê°’ ê³¼ì˜ ë¹„êµë¥¼ í†µí•´ Accuracyë¥¼ ê³„ì‚°í•œë‹¤.

```python
y_pred_c = y_pred.copy()
y_pred_c[y_pred > threshold_anomaly] = -1
y_pred_c[y_pred <= threshold_anomaly] = 1

acc_svr_kernel = accuracy_score(y_test_c, y_pred_c)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_c))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svr_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svr)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ê²°ê³¼ëŠ” Regressionì„ ìˆ˜í–‰í•˜ê³  Thresholdingì„ í†µí•´ Classification ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•œ ê²°ê³¼ì´ë‹¤. íŠ¹ì •í•œ Thresholdë³´ë‹¤ í´ ê²½ìš° ë¶ˆëŸ‰ìœ¼ë¡œ ì²˜ë¦¬í•˜ì˜€ë‹¤. (-1 class)

|                                                           | Diabetes               | Boston                  |
| --------------------------------------------------------- | ---------------------- | ----------------------- |
| Confusion Matrix                                          | [[34 11]<br/> [11 33]] | [[49  6] <br />[ 6 41]] |
| Classification Accuracy<br />(by Regression Thresholding) | 75.28%                 | 88.23%                  |



### Ensemble Neural Network

One-Class SVMì€ Scikit-Learnì— êµ¬í˜„ëœ Nu-SVMì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì•„ë˜ì™€ê°™ì€ param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchingí•˜ì—¬ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ìœ¼ë©° X_Trainê°’ ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤. í•™ìŠµì€ Training_Only setì„ í†µí•´ Classê°€ 1ì¸ ì–‘í’ˆ ë°ì´í„°ë§Œ í•™ìŠµ í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7]},
    {'kernel': ['rbf'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7],
    'gamma': [0.01, 0.03, 0.1, 0.3, 0.05, 1.0]},
]

elapsed_time_kernel_svm = []

svm_classifier = OneClassSVM(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svm_classifier, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svm_classifier = grid_search.fit(x_train_only)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())


```



Inference ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•˜ì˜€ë‹¤. ë‹¨ìˆœí•œ Classificationê³¼ ìœ ì‚¬í•˜ê²Œ Anomaly Detectionì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

```python
start_time = datetime.now()
y_pred = best_svm_classifier.predict(x_test)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())

acc_svm_kernel = accuracy_score(y_test_c, y_pred)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svm_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svm)
# Isolation Forest 
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. Regressionê³¼ ë¹„êµí–ˆì„ë•Œ ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. íŠ¹íˆ Confusion Matrixë¥¼ ë³´ë©´ False Negativeì˜ ë¹„ìœ¨ì´ êµ‰ì¥íˆ ë†’ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

|                            | Diabetes               | Boston                 |
| -------------------------- | ---------------------- | ---------------------- |
| Confusion Matrix           | [[ 2 43] <br/>[ 3 41]] | [[15 40]<br />[ 3 44]] |
| Anomaly Detection Accuracy | 48.31%                 | 57.84%                 |



### Ensemble Neural Netowrk with REBAGG

Isolation Forestì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì–‘í’ˆ ë°ì´í„°(+1 Class)ë§Œì„ í•™ìŠµ í•˜ì˜€ë‹¤. Hyper Parameterë„ ì•„ë˜ì™€ ê°™ì´ iforest_parametersì— ì„¤ì •ëœ ê°’ì„ Grid-Search í•˜ì˜€ë‹¤.

```python
iforest_classifier = IsolationForest()

iforest_parameters = {'n_estimators': list(range(10, 200, 50)), 
              'max_samples': list(range(20, 120, 20)), 
              'contamination': [0.1, 0.2], 
              'max_features': [5,15, 20], 
              'bootstrap': [True, False], 
              }

elapsed_time_iforest = []

start_time = datetime.now()
iforest_grid_search = GridSearchCV(iforest_classifier, iforest_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_iforest_classifier = iforest_grid_search.fit(x_train_only)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())
```



InferenceëŠ” ì•„ë˜ì™€ ê°™ì´ ìˆ˜í–‰í•œë‹¤. ì—­ì‹œ Classificationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³ , Testì •ë‹µê°’ê³¼ì˜ ë¹„êµë¥¼ ìˆ˜í–‰í•œë‹¤.

```python
start_time = datetime.now()
y_pred_c = best_iforest_classifier.predict(x_test)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())


acc_iforest = accuracy_score(y_test_c, y_pred_c)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_c))
print("best parameters ", iforest_grid_search.best_params_)
print('Accuracy ', acc_iforest)
print('elapsed time ', elapsed_time_iforest)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì—­ì‹œ Regressionê³¼ ë¹„êµí–ˆì„ë•Œ ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. Isolation Forestë„ Confusion Matrixë¥¼ ë³´ë©´ False Negativeì˜ ë¹„ìœ¨ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, ëŒ€ë¶€ë¶„ ë¶ˆëŸ‰ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.

|                            | Diabetes               | Boston                 |
| -------------------------- | ---------------------- | ---------------------- |
| Confusion Matrix           | [[ 8 37] <br/>[ 2 42]] | [[25 30]<br />[ 8 39]] |
| Anomaly Detection Accuracy | 56.17%                 | 62.74%                 |



## 4. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : ì •í™•ë„ %
- Datasetì€ Testset 20%, Training 64%, Validation 16%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨

|      | Algorithm                                | Diabetes   | Boston     |
| ---- | ---------------------------------------- | ---------- | ---------- |
| 1    | SVR                                      | **75.28%** | **88.23%** |
| 2    | One-Class SVM                            | 48.31%     | 57.84%     |
| 3    | Isolation Forest                         | 56.17%     | 62.74%     |
| 4    | Auto-Encoder<br /> for Anomaly Detection | 60.67%     | 63.72%     |
| 5    | Mixture Of Gaussian                      | 60.67%     | 63.72%     |



----


# Final Insights





# Conclusion

- Anoamaly Detectionì€ ê·¸ í•œê³„ì„±ë„ ë¶„ëª…íˆ ìˆìœ¼ë¯€ë¡œ, ë¬´ì§€ì„±ìœ¼ë¡œ ì‰½ê²Œ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ê° ë¬¸ì œê°€ ê°–ê³  ìˆëŠ” ê·¼ë³¸ì ì¸ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ì í•©í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ ì ìš©ì„ í•´ì•¼ í•œë‹¤.



-----

# References

-  ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ê°•í•„ì„± êµìˆ˜ë‹˜ Business Analytics ê°•ì˜ ìë£Œ
- https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422
- [ZhiningLiu1998/imbalanced-ensemble: Class-imbalanced / Long-tailed ensemble learning in Python. Modular, flexible, and extensible. | æ¨¡å—åŒ–ã€çµæ´»ã€æ˜“æ‰©å±•çš„ç±»åˆ«ä¸å¹³è¡¡/é•¿å°¾æœºå™¨å­¦ä¹ åº“ (github.com)](https://github.com/ZhiningLiu1998/imbalanced-ensemble)
- [Imbalanced Classification | Handling Imbalanced Data using Python (analyticsvidhya.com)](https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/)
- [Ensemble Methods - Overview, Categories, Main Types (corporatefinanceinstitute.com)](https://corporatefinanceinstitute.com/resources/data-science/ensemble-methods/)
