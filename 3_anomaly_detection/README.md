# Anomaly Detectionì˜ í•œê³„ë¥¼ ì•Œì•„ë³´ì.



## The limitation of anomaly detection algorithm



ğŸ”¥ì´ë²ˆ Tutorialì—ì„œëŠ” Anomaly Detectionì´ ì‹¤ì œ Practicalí•œ ìƒí™©ì—ì„œ ì–´ë– í•œ ì œì•½ì ì´ ìˆëŠ”ì§€ íŒŒì•…í•´ ë³´ê³ , ì–´ë– í•  ë•Œ ì“°ë©´ ì•ˆë˜ëŠ”ì§€ í™•ì¸ í•´ ë³´ê³ ì í•œë‹¤. í™•ì¸í•´ë³´ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” 2ê°€ì§€ ì´ë‹¤.



### 1. Anomaly Detection for "Regression To Anomaly Detection"

- Manufacturing ê³µì • ë“±ì˜ DataëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Targetê°’ì´ Continuousí•œ ê²½ìš°ê°€ ë§ì´ ìˆë‹¤. ì´ëŸ° ê²½ìš°, íŠ¹ì • Threshold ì´ìƒì˜ ê°’ì€ 'ì´ìƒì¹˜(Anomaly)'ë¡œ, Threshold ì´í•˜ì˜ ê°’ì€ 'ì •ìƒì¹˜(Normal)'ë¡œ Target Dataë¥¼ Binary Categorizationì„ í•˜ì—¬, Anomaly Detectionì´ë‚˜ Classificationìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ë ¤ëŠ” ì‹œë„ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ìƒê°í•œë‹¤.

- ë”°ë¼ì„œ ì´ë ‡ê²Œ ê·¼ë³¸ì ìœ¼ë¡œ Regression Taskì¸ ê²ƒë“¤ì„ Thresholdingí•˜ì˜€ì„ ë•Œ, ê³¼ì—° Anomaly Detectionê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ ì˜ ë™ì‘í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ ë³´ì•˜ë‹¤. (íŠ¹íˆ í˜„ì—…ì˜ Manufacturing ê³µì •ì—ì„œëŠ” ëŒ€ë¶€ë¶„ì´ 'ì •ìƒ'ë°ì´í„°ì´ë©° 'ì´ìƒ'ë°ì´í„°ëŠ” ë§¤ìš° ì ì€ Imbalancedí•œ Dataê°€ ëŒ€ë¶€ë¶„ì˜ Caseì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë²ˆ ì‹¤í—˜ì—ì„œëŠ” Regressionì„ í†µí•œ Imbalancedí•œ ë¶€ë¶„ì„ ì—†ì• ê¸° ìœ„í•˜ì—¬ Normal Classì™€ Abnormal Classë¥¼ 5:5 ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.)

- ì´ë¥¼ í†µí•´ Anomaly Detectionì„ ê·¼ë³¸ì  Regression Taskì— ì¨ë„ ë ì§€, ê·¸ í•œê³„ë¥¼ ì•Œì•„ë³´ë ¤ê³  í•œë‹¤.

### 2. Anomaly Detection for "Classification To Anomaly Detection"

- ì¼ë°˜ì ìœ¼ë¡œ Unsupervisedê¸°ë°˜ì˜ Anomaly Detectionë³´ë‹¤ Supervised Classificationì´ ì„±ëŠ¥ì´ ë” ë†’ì€ ê²½ìš°ê°€ ë§ì´ ìˆë‹¤.
- ì‹¤ì œë¡œ ê°„ë‹¨í•œ Taskì—ì„œ Anomaly Detectionì´ Supervised Classificationë³´ë‹¤ ì–´ëŠì •ë„ì˜ ì„±ëŠ¥ì˜ ì°¨ì´ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ë ¤ í•œë‹¤.
- ì´ë¥¼ í†µí•´ Anomaly Detectionì´ Supervised Classification Taskì— ì¨ë„ ë ì§€, ê·¸ í•œê³„ë¥¼ ì•Œì•„ë³´ë ¤ í•œë‹¤.



ìœ„ì˜ 2ê°€ì§€ ë¬¸ì œë¥¼ ìš°ë¦¬ëŠ” 2ê°€ì§€ Tutorialì„ í†µí•´ ì•Œì•„ë³´ê³ ì í•œë‹¤. 



# Table of Contents

- [Background of Anomaly Detection](#Background-of-Anomaly-Detection)

  - [1. Basic Concept](#1-Basic-Concept)
  - [2. One-Class SVM](#2-One-Class-SVM)
  - [3. Isolation Forest](#3-Isolation-Forest)
  - [4. Auto-Encoder for Anomaly Detection](#4-Auto-Encoder-for-Anomaly-Detection)
  - [5. Mixture of Gaussian](#5-Mixture-of-Gaussian)

- [Tutorial 1. Regression To Anomaly Detection](#Tutorial-1-Regression-To-Anomaly-Detection)

  - [1-1. Tutorial Notebook](#1-1-Tutorial-Notebook)
  - [1-2. Setting](#1-2-Setting)
  - [1-3. Usage Code](#1-3-Usage-Code)
  - [1-4. Result (Accuracy)](#1-4-Result_Accuracy)

- [Tutorial 2. Classification To Anomaly Detection](#Tutorial-2-Classification-To-Anomaly-Detection)

  - [2-1. Tutorial Notebook](#2-1-Tutorial-Notebook)
  - [2-2. Setting](#2-2-Setting)
  - [2-3. Usage Code](#2-3-Usage-Code)
  - [2-4. Result (Accuracy)](#2-4-Result_Accuracy)

- [Final Insights](#Final-Insights)

  - [1. Regression To Anomaly Detection](#1-Regression-To-Anomaly-Detection)
  - [2. Classification To Anomaly Detection](#2-Classification-To-Anomaly-Detection)
  - [3. Conclusion](#3-Conclusion)

- [References](#References)

  

-------

# Background of Anomaly Detection

## 1. Basic Concept

Anomaly Detection(ì´ìƒíƒì§€)ë€ ë§ ê·¸ëŒ€ë¡œ Dataìƒì—ì„œ ì •ìƒ(Normal)ì—ì„œ ë²—ì–´ë‚œ ì´ìƒ(Anomaly) ë°ì´í„°ë¥¼ íƒì§€ í•´ ë‚´ëŠ” ê¸°ìˆ ì´ë‹¤. ë‹¤ì–‘í•œ ê¸°ë²•ì´ ìˆìœ¼ë‚˜, ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì •ìƒ(Normal) ë°ì´í„°ë¡œë§Œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬, ê·¸ ì •ìƒ Distributionì„ ì¼ì • ë²”ìœ„ ë²—ì–´ë‚˜ëŠ” ê²ƒë“¤ì„ Anomalyë¡œì¨ Detectioní•˜ëŠ” ê¸°ìˆ ì´ë‹¤. ì¦‰, Unsupervised Learning ê³„ì—´ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. (ì •í™•íˆëŠ” Semi-Supervised í˜¹ì€ Self-Supervised Learning)

![image-20221117131236843](./attachments/image-20221117131236843.png)



ì´ìƒíƒì§€ëŠ” ì•„ë˜ì™€ ê°™ì´ Supervised Anomaly Detection(ì‚¬ì‹¤ Classificationì„), Unsupervised Anomaly Detection, Semi-Supervised Anomaly Detectionìœ¼ë¡œ ë‚˜ë‰  ìˆ˜ ìˆë‹¤. ì •í™•íˆëŠ” ì´ë ‡ê²Œ ë‚˜ë‰˜ì§€ë§Œ í¸ì˜ìƒ ë§ì€ ì‚¬ëŒë“¤ì´ í˜¹ì€ ë³¸ Tutorialì—ì„œëŠ” Unsupervisedì™€ Semi-Supervisedë¥¼ ë”°ë¡œ êµ¬ë³„í•˜ì§€ ì•Šê³  Unsupervisedë¡œ ë¶€ë¥´ë„ë¡ í•˜ê² ë‹¤. (Semi-Supervised ì¤‘ ì •í™•íˆëŠ” One-Class Learningì´ë‹¤.)

![image-20221117131051297](./attachments/image-20221117131051297.png)



Anomaly Detectionì€ Class Labelì´ ì—†ê±°ë‚˜, ì •ìƒ Labelë§Œ ìˆì„ ê²½ìš° í˜¹ì€ ì´ìƒ Dataê°€ êµ‰ì¥íˆ ì ì€ Imbalanced Dataset ìƒí™©ì—ì„œ ì‚¬ìš©í•´ ë³¼ ìˆ˜ ìˆëŠ” ì¢‹ì€ ë°©ë²•ë¡  ì¤‘ í•˜ë‚˜ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ Tutorialì—ì„œ ë‹¤ë£¨ë ¤ëŠ” ë‚´ìš©ë„ ë§ˆì°¬ê°€ì§€ì§€ë§Œ Anomaly Detectionì€ ì •í™•í•œ Contextì— ë§ëŠ” ìƒí™©ì—ì„œ ì‚¬ìš©í•´ì•¼ í•˜ë©°, ì•„ë¬´ ìƒí™©ì—ì„œë‚˜ ì ìš©í•´ì„œ ì‰½ê²Œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì€ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ(Anomaly Detection vs. Classification)ì˜ Decision Making Processë¥¼ ë”°ë¥´ëŠ” ê²ƒì´ ì¢‹ë‹¤. (í•´ë‹¹ ì˜ì‚¬ê²°ì •ë°©ë²•ë¡ ì€ ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ê°•í•„ì„± êµìˆ˜ë‹˜ Referenceì…ë‹ˆë‹¤.)

![image-20221117131643910](./attachments/image-20221117131643910.png)

Anomaly Detectionì€ í¬ê²Œ Density Based, Model Based, Distance Basedë¡œ 3ê°€ì§€ Taxonomyë¡œ ë‚˜ëˆ„ì–´ì„œ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. í•˜ê¸° ë¶€í„°ëŠ” Model Basedì¸ One-Class SVM, Isolation Forest, Autoencoderì— ëŒ€í•´ ë‹¤ë£¨ê³ , Density Based ì¤‘ ë§¤ìš° ë§ì´ ì‚¬ìš©ë˜ëŠ” Mixture Of Gaussian(Gaussian Mixture Model) ê¸°ë²•ì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ì •ë¦¬ í•´ ë³´ê³ ì í•œë‹¤. 



ê·¸ë¦¬ê³  í•´ë‹¹ ì´ë¡ ì ì¸ ë‚´ìš©ì„ ê°„ë‹¨íˆ íŒŒì•…í•˜ê³  2ê°€ì§€ Tutorialì—ì„œ ê° ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ íŠ¹ì • Caseì—ì„œ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ì–´ëŠì •ë„ì˜ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€ ë¹„êµí•´ ë³´ë„ë¡ í•˜ê² ë‹¤.



## 2. One-Class SVM

ë¨¼ì € Model-Based Anomaly Detection ì¤‘ í•˜ë‚˜ì¸ One-Class SVMì„ ë‹¤ë¤„ ë³´ê² ë‹¤(ì •í™•í•˜ê²ŒëŠ” Nu-SVM). ì´ SVMì˜ Variationì€ ì •ìƒ(ì–‘í’ˆ) Classì— ëŒ€í•´ì„œë§Œ í•™ìŠµì„ ì§„í–‰í•˜ê³ , ê·¸ í•™ìŠµì˜ Boundaryë¥¼ ë²—ì–´ë‚˜ëŠ” ê²ƒë“¤ì„ ë¶ˆëŸ‰ìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤. ì´ëŠ” SVMì—ì„œ Support Vectorë¡œ êµ¬ì„±ë˜ëŠ” 2ê°œì˜ Marginìœ¼ë¡œ Binary Classificationì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•˜ê²Œ, One-Class SVMì€ ì›ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ìƒ Classì˜ Dataê¹Œì§€ì˜ ê±°ë¦¬(p)ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ëª©ì  í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤.



ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ìë©´ ì•„ë˜ì™€ ê°™ê³ , ì›ì  0ì—ì„œ ë¶€í„° Marginì™€ ê±°ë¦¬ pë¥¼ ìµœëŒ€í™”í•  ìˆ˜ ìˆëŠ” ê²°ì • í‰ë©´ì„ í•™ìŠµí•˜ê²Œ ëœë‹¤.

![image-20221117133803787](./attachments/image-20221117133803787.png)

ì´ì˜ ëª©ì í•¨ìˆ˜(Objective Function)ê³¼ ë¶„ë¥˜ë¥¼ ìœ„í•œ Decision Functionì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ìë©´ ì•„ë˜ì™€ ê°™ë‹¤. í•´ë‹¹ One-Class SVMë„ Dual Largrangian Problemìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìµœì í™” ì‹ì„ ë„ì¶œ í•  ìˆ˜ ìˆë‹¤.

![image-20221117133746937](./attachments/image-20221117133746937.png)



ì´ Nu-SVMì™¸ì— SVDD(Support Vector Data Description)ì´ë¼ëŠ” ë°©ë²•ë¡ ë„ ìˆëŠ”ë°(ì •ìƒ Dataë¥¼ ê°ì‹¸ëŠ” ê°€ì¥ ì‘ì€ ë°˜ì§€ë¦„ì„ ê°–ëŠ” Hypershpereë¥¼ ì°¾ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•¨), ê²°ë¡ ì ìœ¼ë¡œëŠ” ë‘ê°œì˜ ê¸°ë²•ì´ ê±°ì˜ ë™ì¼í•œ ìˆ˜ì‹(ëª¨ë“  ë°ì´í„°ê°€ Unit norm vectorë¡œ Normalized ë˜ì–´ìˆì„ ë•Œ SVDDì™€ Nu-SVMì€ Equivalentí•¨)ìœ¼ë¡œ ì „ê°œë˜ê¸° ë•Œë¬¸ì— ë”°ë¡œ ì„¤ëª…ì„ í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.



## 3. Isolation Forest

ë‘ë²ˆì§¸ë¡œ ë‹¤ë£° Model-Based Anomaly ê¸°ë²•ì€ Isolation Forestì´ë©°, Decision Treeë¥¼ ì‚¬ìš©í•œ ê¸°ë²•ì´ë‹¤. Isolation ForestëŠ” êµ‰ì¥íˆ ë‹¨ìˆœí•œ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ì´ ì„¤ê³„ ë˜ì—ˆë‹¤. ì¼ë‹¨ ë‹¹ì—°í•˜ê²Œë„ AnomalyëŠ” ì†Œìˆ˜ì˜ Dataë¼ëŠ” ì ì´ê³  ë˜í•œ ì •ìƒ ë¶„í¬ì™€ëŠ” ë‹¤ë¥¸ ë¶„í¬ì— ì†í•  ê²ƒì´ë¼ëŠ” ê°€ì •ì´ë‹¤.(ì¼ë°˜ì ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ê°€ì •ì„). ì´ë¥¼ í†µí•˜ì—¬ Isolation ForestëŠ” Randomí•œ ìˆ˜ì§/ìˆ˜í‰ì˜ ì§ì„ ìœ¼ë¡œ Dataë¥¼ ë¶„í•  í–ˆì„ ë•Œ, ê° Featureë³„ë¡œ ì˜ ë¶„í• ì´ ì•ˆë˜ëŠ” DataëŠ” ì •ìƒ(Noramal) ë°ì´í„°ë¡œ, ì‰½ê²Œ ë¶„ë¥˜ê°€ ë˜ì–´ ê³ ë¦½(Isolation)ì´ ë˜ëŠ” DataëŠ” ì´ìƒ(Anomaly) ë°ì´í„°ë¡œ íŒë‹¨í•œë‹¤.



ì•„ë˜ì˜ ì¢Œì¸¡ ì²˜ëŸ¼ ì •ìƒ Dataë“¤ì€ ì •ìƒ Dataë¡œì¨ ë‹¤ìˆ˜ê°€ ëª°ë ¤ ìˆê¸° ë•Œë¬¸ì— ê° Dataë¥¼ ê³ ë¦½ì‹œí‚¤ê°€ ì–´ë µì§€ë§Œ, ì•„ë˜ì˜ ìš°ì¸¡ì²˜ëŸ¼ ì´ìƒ DataëŠ” ëª‡ë²ˆì˜ ê³ ë¦½(ë¶„ë¦¬)ìœ¼ë¡œë„ ì‰½ê²Œ ë¶„ë¥˜ê°€ ëœë‹¤.

![image-20221117135242018](./attachments/image-20221117135242018.png)

ë”°ë¼ì„œ Isolation ForestëŠ” Decision Treeë¡œ ë¶„ë¥˜ë¥¼ í•´ ë‚˜ê°ˆë•Œ, ëª‡ë²ˆì´ë‚˜ Treeê°€ ë¶„ê¸°ë˜ì—ˆëŠëƒì— ë”°ë¼ ê·¸ Split ìˆ˜ê°€ í¬ë©´ Normal, ê·¸ ìˆ˜ê°€ ì‘ìœ¼ë©´ Anomalyë¼ê³  íŒë‹¨ë‚´ë¦¬ê²Œ ëœë‹¤.

![image-20221117135446688](./attachments/image-20221117135446688.png)

ê° ê°œë³„ ë¶„ë¥˜ë¥¼ í†µí•´ Path Lengthë¥¼ êµ¬í•˜ê³  ê·¸ë¥¼ í†µí•´ Novelty Scoreë¥¼ êµ¬í•˜ê²Œ ë˜ëŠ”ë°, ì´ëŸ°ì‹ìœ¼ë¡œ ë°˜ë³µì ì¸ ê³„ì‚°ì„ í•˜ë‹¤ë³´ë‹ˆ ìƒëŒ€ì ìœ¼ë¡œ ê³„ì‚°ëŸ‰ì´ ë§ì•„ Trainingì‹œì— ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë§ì€ ì‹œê°„ì„ ì†Œìš”í•˜ê²Œ ëœë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ Data Scoreê°€ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ê³„ì‚°ë˜ì–´ í•­ìƒ 0~1 ì‚¬ì´ì˜ ê°’ì„ Outputí•˜ê¸° ë–„ë¬¸ì— ê·¸ í•´ì„ì´ ì‰½ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. (ì •ìƒ:0, ì´ìƒ:1)

![image-20221117135703097](./attachments/image-20221117135703097.png)



## 4. Auto-Encoder for Anomaly Detection

ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹¤ë£° Model-Based Anomaly Detectionì€ Neural Networkê¸°ë°˜ì˜ Auto-Encoderë¥¼ ì‚¬ìš©í•œ Anomaly Detetionê¸°ë²•ì´ë‹¤. Auto-EncoderëŠ” ì´ì œ ëª¨ë¥´ëŠ” ì‚¬ëŒì´ ì—†ì„ ì •ë„ë¡œ ìœ ëª…í•˜ê³  êµ‰ì¥íˆ Simpeí•˜ë©° ë‹¤ì–‘í•œ Applicationì„ ê°€ì§„ ê¸°ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. Unsupervsedë¡œë„, Semi-Supervisedë¡œë„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ë²•ì´ë‹¤. ë˜í•œ êµ‰ì¥íˆ ìœ ì—°í•˜ì—¬ ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ê¸°ë²•ë“¤ì„ ë‹¤ì–‘í•˜ê²Œ ì ìš©í•´ ë³¼ ìˆ˜ ìˆëŠ” í˜„ì¬ë„ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê¸°ë²•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ ì¤‘ ìš°ë¦¬ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ Auto-Encoderë¥¼ ì´ì•¼ê¸° í•´ ë³´ê² ë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ Classificationì€ ì•„ë˜ì™€ ê°™ì´ Featureë¥¼ Extractioní•˜ëŠ” Encoderë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ì—¬ Classificationì„ ìˆ˜í–‰í•œë‹¤. Encodingëœ Featureë¥¼ ë§ˆì§€ë§‰ Classifierì—ì„œ ë¶„ë¥˜ Taskë¥¼ ìˆ˜í–‰í•œë‹¤.

![image-20221117141527803](./attachments/image-20221117141527803.png)



ë°˜ë©´ì— AutoencoderëŠ” Encoderì™€ Decoderë¡œ ì´ë£¨ì–´ì ¸, ì…ë ¥ë°ì´í„°ë¥¼ ë°›ìœ¼ë©´, ê·¸ ì…ë ¥ ë°ì´í„°ì™€ ë™ì¼í•œ ë°ì´í„°ë¥¼ ì¶œë ¥ë°ì´í„°ë¡œ ì˜ˆì¸¡í•˜ë ¤ëŠ” ëª¨ë¸ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë•Œ ì¤‘ìš”í•œ ê²ƒì€, EncoderëŠ” Featureë¥¼ Extractí•˜ê¸° ìœ„í•˜ì—¬, ë” ì ì€ Featureê°œìˆ˜ë¡œ Encodingí•´ì•¼í•˜ë©°, DecoderëŠ” Extractedëœ ë” ì ì€ Featureì—ì„œ ë‹¤ì‹œê¸ˆ Decodingì„ í†µí•´ Input Featureì™€ ë™ì¼í•œ Output Featureë¥¼ ë‚´ë³´ë‚´ë„ë¡ í•œë‹¤. 

![image-20221117150541130](./attachments/image-20221117150541130.png)

Auto-Encoderë¥¼ Anomaly Detection Taskì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´, Inputê°’ê³¼ Decodingëœ Outputê°’ì˜ ì°¨ì´ë¥¼ Reconstruction Errorë¡œ ì •ì˜í•˜ì—¬, ê·¸ Errorë¥¼ Thresholdí•˜ì—¬ ê·¸ ê°’ì´ í¬ë©´ Anomalyë¡œ, ì‘ìœ¼ë©´ Normal Classë¡œ ë¶„ë¥˜ë¥¼ í•˜ê²Œ ëœë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ Imageì˜ ê²½ìš° Pixel Levelë¡œ Anomaly Scoreë¥¼ ê³„ì‚°í•˜ê³ , Summationì„ Reconstructin Errorë¡œ ì‚¬ìš©í•´ Anomaly Detectionì„ ìˆ˜í–‰í•œë‹¤.

![image-20221117150655541](./attachments/image-20221117150655541.png)

## 5. Mixture of Gaussian

Mixture Of Gaussian (MoG)ëŠ” ì—¬ëŸ¬ê°œì˜ Gaussian Distributionì„ ì„ í˜• ê²°í•©í•˜ì—¬ Normal Data Distributionì„ í•™ìŠµí•˜ê³ , ë¹„ì •ìƒ Dataê°€ ì™”ì„ ê²½ìš° í™•ë¥  ê°’ì´ íŠ¹ì • ê°’ë³´ë‹¤ ë‚®ì„ ë•Œ Anomalyë¡œ Classificatinoí•˜ëŠ” ë°©ë²•ë¡ ì´ë‹¤.

ì•„ë˜ì˜ ì˜ˆì‹œëŠ” 4ê°œì˜ Gaussian Distributionì„ í†µí•´ Dataë¥¼ Fittingí•˜ì—¬ Linear Combinationí•œ MoGë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ê²€ì€ìƒ‰ ì ì„ ì¸ Actual Dataì— ê±°ì˜ ìœ ì‚¬í•˜ê²Œ Gaussian Mixture Model(Mixture of Gaussian)ì´ Fittingí•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![image-20221117165912161](./attachments/image-20221117165912161.png)

ìµœì í™” í•´ì•¼í•˜ëŠ” ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤. ì•„ë˜ì—ì„œ gëŠ” ê°œë³„ Gaussian Distributiomì„ ë‚˜íƒ€ë‚´ë©°, mê°œì˜ Gaussian Distributionì´ ìˆê³ , weightë¥¼ ë‚˜íƒ€ë‚´ëŠ” wë¥¼ í†µí•˜ì—¬ ê°ê°ì˜ Gaussianì˜ ê¸°ì—¬ë¥¼ Linear Combinationí•˜ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤.

![image-20221117170046524](./attachments/image-20221117170046524.png)

MoGëŠ” Latent Vectorì¸ Weightì˜ ì¡´ì¬ë¡œ ì¸í•˜ì—¬, ê°ê°ì˜ m Clusterì— ë”°ë¥¸ ìµœì ê°’ì„ Muê°’ê³¼ Sigmaê°’ê³¼ í•¨ê»˜ ë™ì‹œì— ìµœì í™” í•˜ê¸°ê°€ ì–´ë µë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ìµœì í™” í•˜ê¸° ìœ„í•˜ì—¬ Sequentialí•œ Optimizationê¸°ë²•ì¸ EM(Expectation-Maximizaiton) Algorithmì„ í†µí•´ ìµœì í™”ë¥¼ ìˆ˜í–‰í•œë‹¤. ì´ëŠ” ìµœì í™”í•  ë•Œ ê°ê°ì˜ Parameterë¥¼ ê³ ì •í•˜ë©´ì„œ í•˜ë‚˜ì”© ìµœì í™” í•´ ë‚˜ê°€ëŠ” ë°©ë²•ì´ë‹¤.

ì¼ë‹¨ ì•„ë˜ì™€ ê°™ì´ Expectationë‹¨ê³„ì—ì„œëŠ”, ì•„ë˜ì— ë„¤ëª¨ ì¹œ Weight, Mu, SIgmaë¥¼ ê³ ì •í•˜ê³ , ì–´ë– í•œ m Clusterì— ë°ì´í„°ê°€ ìƒì„±ë ì§€ì— ëŒ€í•˜ì—¬ í™•ë¥  ê°’ì„ ìµœì í™”ë¥¼ í•´ ì¤€ë‹¤.

![image-20221117170426317](./attachments/image-20221117170426317.png)

ë‹¤ìŒìœ¼ë¡œ Maximization ë‹¨ê³„ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´, **p(m|x_i, lambda)**ëŠ” ìœ„ì˜ Expectationë‹¨ê³„ì—ì„œ êµ¬í•´ì§„ ê°’ìœ¼ë¡œ ê³ ì •í•´ ë†“ê³ , ê°ê°ì˜ weightì™€ mu, sigmaë¥¼ ê°œë³„ì ìœ¼ë¡œ ìµœì  í™” í•´ ì¤€ë‹¤.

![image-20221117170558380](./attachments/image-20221117170558380.png)

ìœ„ì˜ Expectationê³¼ Maximizationì„ í•˜ë‚˜ì”© ìˆ˜í–‰í•˜ë©´ì„œ, ë”ì´ìƒ ê° Parameterê°’ì´ ë³€í•˜ì§€ ì•Šê³  ìˆ˜ë ´í•˜ê²Œ ë˜ë©´ ë©ˆì¶”ê²Œ ë˜ëŠ” ê²ƒì´ ë°”ë¡œ EMì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ì´ë¥¼ í†µí•´ í•™ìŠµì„ í•´ ë‚˜ê°€ë©´ ì•„ë˜ì™€ ê°™ì´ ë¶„í¬ë¥¼ MoGê°€ í•™ìŠµí•˜ê²Œ ëœë‹¤.

![MoG_EM](./attachments/MoG_EM.gif)



----

# Tutorial 1. Regression To Anomaly Detection

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì•ì„œ ì„¤ëª…í•œ ê²ƒê³¼ ê°™ì´ ê·¼ë³¸ì ìœ¼ë¡œ Regressionì¸ Taskë¥¼ Thresholdë¥¼ í†µí•´ Anomaly Detection (ì¼ì¢…ì˜ One-Class Binary Classification)ì´ ê°€ëŠ¥í• ì§€ ì•Œì•„ë³´ëŠ” ì‹¤í—˜ì´ë‹¤. í•´ë‹¹ ì‹¤í—˜ì„ ìœ„í•´ ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ Regression(SVR)ê³¼ ì—¬ëŸ¬ Anomaly Detection ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ë¹„êµ í•˜ê³ ì í•œë‹¤.

![image-20221117010800581](./attachments/image-20221117010800581.png)

ìœ„ì™€ ê°™ì€ Logistic Regressionì´ ì•„ë§ˆ ìœ ì‚¬í•œ ê°œë…ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. Regression ê²°ê³¼(Logit)ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•˜ì—¬(Logistic), 0.5ë¼ëŠ” Thresholdë¡œ ë‚˜ëˆ ì„œ Classificationì„ í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•œ ê°œë…ìœ¼ë¡œ Regressionì„ ì‚¬ìš©í•´ Thresholdí•˜ì—¬ Classificationì„ í•˜ëŠ” ì•„ì£¼ ì§ê´€ì ì¸ ë°©ë²•ê³¼ Anomaly Detectionì˜ ë¹„êµë¼ê³  ì´í•´í•˜ë©´ ë˜ê² ë‹¤.



## 1-1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/3_anomaly_detection/Tutorials/tutorial_anomaly_detection_from_R_task.ipynb)



## 1-2. Setting

### Datasets

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì´ 2ê°œì˜ ìœ ëª…í•œ Tabular í˜•íƒœì˜ Regression Datasetì„ ì‚¬ìš©í•œë‹¤. ë‘ê°œì˜ Datasetëª¨ë‘ Regression Targetì´ë¯€ë¡œ Thresholdingì„ í†µí•´ ëª©ì ì— ë§ê²Œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•œë‹¤. ì „ì²´ ë°ì´í„° ì¤‘ Training Setì€ 64%, Validation Setì€ 16%, Test Setì€ 20%ì˜ Dataë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.

|      | Datasets                        | Description                                                  | Num Instances | Num Inputs (Xs) | Num Outputs (Ys) |
| ---- | ------------------------------- | ------------------------------------------------------------ | ------------- | --------------- | ---------------- |
| 1    | Diabetes (Regression)           | ë‹¹ë‡¨ë³‘ í™˜ì ë°ì´í„° (1ë…„ í›„ ë‹¹ë‡¨ì˜ ì§„í–‰ì •ë„ë¥¼ Targetê°’ìœ¼ë¡œ í•¨) | 442           | 10              | 1                |
| 2    | Boston House Price (Regression) | Bostonì˜ ì§‘ê°’ì— ëŒ€í•œ Data                                    | 506           | 13              | 1                |

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê²Œ ëœë‹¤.

```python
if dataset_name == 'diabetes_r':
    x, y= datasets.load_diabetes(return_x_y=true)
elif dataset_name == 'boston_r':
    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=none)
    x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    y = raw_df.values[1::2, 2]
else:
    pass
```

ê° Datasetì€ Regression Targetì´ë¯€ë¡œ, ê° Datasetì„ Anomalyì— ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ ì‚¬ìš©ë˜ëŠ” Thresholdê°’ì€ ì•„ë˜ì™€ ê°™ë‹¤. ê° ê°’ì€ ì „ì²´ ë°ì´í„°ì˜ Median ê°’ì´ë‹¤. Regression Taskì— Imbalancedì— ì˜í•œ ì˜í–¥ì„ ì¤„ì´ê¸° ìœ„í•´ ì¤‘ì•™ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì–‘ë¶ˆ Dataì˜ Balanceë¥¼ ë§ì¶”ì—ˆë‹¤.

- **Diabetes : 140** 
- **Boston House Price : 21**





### Algorithms

ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ Regression ì•Œê³ ë¦¬ì¦˜ê³¼ Anomaly Detectionì„ ì„œë¡œ ë¹„êµí•œë‹¤.

- Regerssion 
  - SVRì„ ì‚¬ìš©í•˜ì—¬ Regression Taskì—ì„œ Regression Algorithmì„ ì‚¬ìš©í•˜ê³  ì˜ˆì¸¡í•œ ê°’ì„ íŠ¹ì • Thresholdë¡œ Classificationí•˜ì—¬ ì–‘ë¶ˆì„ íŒì •í•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤.
- Anomaly Detection
  - 4ê°€ì§€ì˜ ì•Œê³ ë¦¬ì¦˜(One-Class SVM, Isolation Forest, Autoencoder Anomaly Detection, Mixture Of Gaussian)ì„ ì‚¬ìš©í•˜ì—¬, ë°ì´í„°ë¥¼ ì–‘ë¶ˆë¡œ Binary Classificationë¬¸ì œë¡œ ì „ì²˜ë¦¬ í›„, ì–‘í’ˆ ë°ì´í„°ë§Œì„ í•™ìŠµí•˜ì—¬ Anomalyë¥¼ íƒì§€í•œë‹¤.

|      | Algorithm                              | Target            | Description                                                  |
| ---- | -------------------------------------- | ----------------- | ------------------------------------------------------------ |
| 1    | Linear SVR                             | Regression        | ì„ í˜• SVR                                                     |
| 2    | Kernel SVR                             | Regression        | ì„ í˜• SVR + Kernel Trick(using rbf kernel)                    |
| 3    | One-Class SVM                          | Anomaly Detection | ì–‘í’ˆ Sampleë§Œìœ¼ë¡œ í•™ìŠµí•˜ì—¬ Anomaly Detectionì„ ìˆ˜í–‰í•˜ëŠ” SVMì˜ ë³€í˜• ë²„ì „(Nu-SVM). ì–‘í’ˆ Sample Dataê°€ ì›ì ì—ì„œ ê°€ì¥ ë©€ì–´ì§€ê²Œ í•˜ëŠ” Hyper Planeì„ ì°¾ëŠ”ë‹¤. |
| 4    | Isolation Forest                       | Anomaly Detection | ì–‘í’ˆ Sampleë§Œìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ê°„ë‹¨í•œ Decision Tree ì¡°í•©ì„ í†µí•´ Anomalyë¥¼ Detectioní•˜ëŠ” ì•Œê³ ë¦¬ì¦˜. ë¶„ë¥˜ Path Lengthê°€ ê¸¸ìˆ˜ë¡ ì–‘í’ˆì´ë‹¤. |
| 5    | Autoencoder<br />for Anomaly Detection | Anomaly Detection | ì–‘í’ˆ Sampleë§Œì„ í†µí•´ Neural Networkê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ê³ , ë™ì¼í•˜ê²Œ Reconstructioní•˜ëŠ” Taskë¥¼ ìˆ˜í–‰í•˜ì—¬, Anomaly Detectioní•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ |
| 6    | Mixture of Gaussian                    | Anomaly Detection | ì—¬ëŸ¬ê°œì˜ Gaussianì˜ ì„ í˜• ê²°í•©ì„ í†µí•´ ë¶„í¬ë¥¼ ë²—ì–´ë‚˜ëŠ” Dataë¥¼ ì°¾ì•„ë‚´ì–´ Anomaly Detectionì„ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ |



## 1-3. Usage Code

### SVR

ì„±ëŠ¥ì´ ì–´ëŠì •ë„ ê²€ì¦ëœ ê¸°ë²•ì¸ SVRì„ ì‚¬ìš©í•˜ì—¬, Regression Taskë¥¼ ì˜ˆì¸¡í•œë‹¤. ê·¸ë¦¬ê³  ì˜ˆì¸¡ëœ ê²°ê³¼ë¥¼ Thresholdë¡œ ë‚˜ëˆ„ì–´, ì–‘ë¶ˆì„ íŒì •í•œë‹¤. ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ í•™ìŠµê³¼ ì¶”ë¡ í•˜ì—¬ Regressionì„ ì˜ˆì¸¡í•œë‹¤. Linear SVRê³¼ RBF SVRì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchí•˜ì—¬ ëª¨ë¸ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'C': [1.0, 2.0, 3.0, 10., 30., 100.]},
    {'kernel': ['rbf'], 'C': [1.0, 2.0, 3.0, 5.0, 10., 30., 100.],
    'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},
]

elapsed_time_kernel_svr = []

svr_regressor = SVR(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svr_regressor, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svr_regressor = grid_search.fit(x_train, y_train)
elapsed_time_kernel_svr.append((datetime.now()-start_time).total_seconds())

start_time = datetime.now()
y_pred = best_svr_regressor.predict(x_test)
elapsed_time_kernel_svr.append((datetime.now()-start_time).total_seconds())

```



ì•„ë˜ì™€ ê°™ì´ ì˜ˆì¸¡í•œ ê°’ì„ ìœ„ì—ì„œ ì„¤ì •í•œ thresholdê°’ìœ¼ë¡œ ì–‘ë¶ˆ(ì–‘í’ˆ +1, ë¶ˆëŸ‰ -1) Labelingì„ í•´ ì¤€ë‹¤. ì´ë¥¼ í†µí•´ì„œ Answer Yê°’ì˜ Classificationëœ ê°’ ê³¼ì˜ ë¹„êµë¥¼ í†µí•´ Accuracyë¥¼ ê³„ì‚°í•œë‹¤.

```python
y_pred_c = y_pred.copy()
y_pred_c[y_pred > threshold_anomaly] = -1
y_pred_c[y_pred <= threshold_anomaly] = 1

acc_svr_kernel = accuracy_score(y_test_c, y_pred_c)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_c))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svr_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svr)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ê²°ê³¼ëŠ” Regressionì„ ìˆ˜í–‰í•˜ê³  Thresholdingì„ í†µí•´ Classification ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•œ ê²°ê³¼ì´ë‹¤. íŠ¹ì •í•œ Thresholdë³´ë‹¤ í´ ê²½ìš° ë¶ˆëŸ‰ìœ¼ë¡œ ì²˜ë¦¬í•˜ì˜€ë‹¤. (-1 class)

|                                                           | Diabetes               | Boston                  |
| --------------------------------------------------------- | ---------------------- | ----------------------- |
| Confusion Matrix                                          | [[34 11]<br/> [11 33]] | [[49  6] <br />[ 6 41]] |
| Classification Accuracy<br />(by Regression Thresholding) | 75.28%                 | 88.23%                  |



### One-Class SVM

One-Class SVMì€ Scikit-Learnì— êµ¬í˜„ëœ Nu-SVMì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì•„ë˜ì™€ê°™ì€ param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchingí•˜ì—¬ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ìœ¼ë©° X_Trainê°’ ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤. í•™ìŠµì€ Training_Only setì„ í†µí•´ Classê°€ 1ì¸ ì–‘í’ˆ ë°ì´í„°ë§Œ í•™ìŠµ í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7]},
    {'kernel': ['rbf'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7],
    'gamma': [0.01, 0.03, 0.1, 0.3, 0.05, 1.0]},
]

elapsed_time_kernel_svm = []

svm_classifier = OneClassSVM(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svm_classifier, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svm_classifier = grid_search.fit(x_train_only)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())


```



Inference ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•˜ì˜€ë‹¤. ë‹¨ìˆœí•œ Classificationê³¼ ìœ ì‚¬í•˜ê²Œ Anomaly Detectionì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

```python
start_time = datetime.now()
y_pred = best_svm_classifier.predict(x_test)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())

acc_svm_kernel = accuracy_score(y_test_c, y_pred)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svm_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svm)
# Isolation Forest 
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. Regressionê³¼ ë¹„êµí–ˆì„ë•Œ ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. íŠ¹íˆ Confusion Matrixë¥¼ ë³´ë©´ False Negativeì˜ ë¹„ìœ¨ì´ êµ‰ì¥íˆ ë†’ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

|                            | Diabetes               | Boston                 |
| -------------------------- | ---------------------- | ---------------------- |
| Confusion Matrix           | [[ 2 43] <br/>[ 3 41]] | [[15 40]<br />[ 3 44]] |
| Anomaly Detection Accuracy | 48.31%                 | 57.84%                 |



### Isolation Forest

Isolation Forestì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì–‘í’ˆ ë°ì´í„°(+1 Class)ë§Œì„ í•™ìŠµ í•˜ì˜€ë‹¤. Hyper Parameterë„ ì•„ë˜ì™€ ê°™ì´ iforest_parametersì— ì„¤ì •ëœ ê°’ì„ Grid-Search í•˜ì˜€ë‹¤.

```python
iforest_classifier = IsolationForest()

iforest_parameters = {'n_estimators': list(range(10, 200, 50)), 
              'max_samples': list(range(20, 120, 20)), 
              'contamination': [0.1, 0.2], 
              'max_features': [5,15, 20], 
              'bootstrap': [True, False], 
              }

elapsed_time_iforest = []

start_time = datetime.now()
iforest_grid_search = GridSearchCV(iforest_classifier, iforest_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_iforest_classifier = iforest_grid_search.fit(x_train_only)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())
```



InferenceëŠ” ì•„ë˜ì™€ ê°™ì´ ìˆ˜í–‰í•œë‹¤. ì—­ì‹œ Classificationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³ , Testì •ë‹µê°’ê³¼ì˜ ë¹„êµë¥¼ ìˆ˜í–‰í•œë‹¤.

```python
start_time = datetime.now()
y_pred_c = best_iforest_classifier.predict(x_test)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())


acc_iforest = accuracy_score(y_test_c, y_pred_c)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_c))
print("best parameters ", iforest_grid_search.best_params_)
print('Accuracy ', acc_iforest)
print('elapsed time ', elapsed_time_iforest)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì—­ì‹œ Regressionê³¼ ë¹„êµí–ˆì„ë•Œ ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. Isolation Forestë„ Confusion Matrixë¥¼ ë³´ë©´ False Negativeì˜ ë¹„ìœ¨ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, ëŒ€ë¶€ë¶„ ë¶ˆëŸ‰ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.

|                            | Diabetes               | Boston                 |
| -------------------------- | ---------------------- | ---------------------- |
| Confusion Matrix           | [[ 8 37] <br/>[ 2 42]] | [[25 30]<br />[ 8 39]] |
| Anomaly Detection Accuracy | 56.17%                 | 62.74%                 |



### Auto-Encoder for Anomaly Detection

ë”¥ëŸ¬ë‹ ê³„ì—´ì˜ Auto-Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ Anomaly Detectionì„ ìˆ˜í–‰í•œë‹¤. SELU Activation Functionì„ í†µí•´ BatchNormë“±ì„ ì§€ìš°ê³ ë„ ì„±ëŠ¥ì„ ì–´ëŠì •ë„ ë„ë‹¬ í•  ìˆ˜ ìˆê²Œ ëª¨ë¸ì„ ì„¸íŒ…í•˜ì˜€ë‹¤. EncoderëŠ” 2ê°œ, Decoderë„ 2ê°œì˜ Layerë¥¼ ê°€ì§€ê³  ìˆë‹¤.

```python
class BasicAutoEncoder(nn.Module):
    def __init__(self) -> None:
        super(BasicAutoEncoder, self).__init__()

        self.layer_1 = nn.Linear(NUM_INPUT, NUM_1ST_HIDDEN)
        self.layer_2 = nn.Linear(NUM_1ST_HIDDEN, NUM_2ND_HIDDEN)
        self.layer_3 = nn.Linear(NUM_2ND_HIDDEN, NUM_1ST_HIDDEN)
        self.layer_4 = nn.Linear(NUM_1ST_HIDDEN, NUM_INPUT)

        self.actvation_1 = nn.SELU()
        self.actvation_2 = nn.SELU()
        self.actvation_3 = nn.SELU()
    
    def forward(self, inputs):
        x = self.actvation_1(self.layer_1(inputs))
        x = self.actvation_2(self.layer_2(x))
        x = self.actvation_3(self.layer_3(x))
        x = self.layer_4(x)

        return x
        
```

InferenceëŠ” ì•„ë˜ì™€ ê°™ë‹¤. Test ì…ë ¥ê°’ Xì™€ ëª¨ë¸ ì¶œë ¥ ì˜ˆì¸¡ê°’ê³¼ì˜ ì ˆëŒ€ê°’ ì°¨ì´ë¥¼ ë¹„êµí•˜ì—¬, Reconstruction Errorë¥¼ êµ¬í•˜ì—¬, ê·¸ì— ë”°ë¼ Thresholdingí•˜ì—¬ ì–‘ë¶ˆì„ íŒì •í•˜ì˜€ë‹¤.

```python
result_reconstruct = abs(x_test - output_num).sum(axis=1)

result_class = result_reconstruct.copy()
result_class[result_reconstruct > THRESHOLD_FOR_RECONSTRUCTION] = -1
result_class[result_reconstruct <=THRESHOLD_FOR_RECONSTRUCTION] = 1

# result_class
acc_ae = accuracy_score(y_test_c, result_class)

print('Confusion Matrix\n', confusion_matrix(y_test_c, result_class))
print('Accuracy ', acc_ae)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. Regressionê³¼ ë¹„êµí–ˆì„ë•Œ ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. (ê·¸ë˜ë„ One-Class SVMê³¼ Isolation Forestë³´ë‹¤ëŠ” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ê¸´ í•œë‹¤.)

|                            | Diabetes                | Boston                 |
| -------------------------- | ----------------------- | ---------------------- |
| Confusion Matrix           | [[17 28] <br />[ 7 37]] | [[33 22]<br />[15 32]] |
| Anomaly Detection Accuracy | 60.67%                  | 63.72%                 |



### Mixture Of Gaussian

MoGë¥¼ ì‚¬ìš©í•˜ì—¬ Anomaly Detectionì„ ìˆ˜í–‰í•œë‹¤. ParameterëŠ” gmm_parametersì— ìˆëŠ” ê²ƒì„ Grid-Searchí•˜ì—¬ ëª¨ë¸ì„ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤. ì—­ì‹œ í•˜ë‚˜ì˜ ì–‘í’ˆ Labelì— ëŒ€í•œ Training Setìœ¼ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•˜ì˜€ë‹¤.

```python
gmm_classifier = GaussianMixture()

gmm_parameters ={'n_components' : [1, 2, 3,4,5,6, 7] , 'max_iter': [int(1e2), int(1e3), int(1e6)]}

elapsed_time_gmm= []

start_time = datetime.now()
gmm_grid_search = GridSearchCV(gmm_classifier, gmm_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_gmm_classifier = gmm_grid_search.fit(x_train_only)
elapsed_time_gmm.append((datetime.now()-start_time).total_seconds())

```



MoGì˜ ê²½ìš° Thresholdë¥¼ ì§€ì •í•˜ê¸° ìœ„í•˜ì—¬ Percentile(ë°±ë¶„ìœ„ìˆ˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ Densityì— ëŒ€í•´ì„œ ëª‡ % ë¯¸ë§Œê¹Œì§€ Anomalyë¥¼ ì •í• ì§€ Thresholdë¥¼ ê²°ì •í•˜ê²Œ ëœë‹¤. AutoEncoderì—ì„œ Reconstruction Errorì— ëŒ€í•œ Thresholdë¥¼ ì§€ì •í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•œ Hyper-Parameterì´ë‹¤.

```python
start_time = datetime.now()
y_pred_c = best_gmm_classifier.predict(x_test)
elapsed_time_gmm.append((datetime.now()-start_time).total_seconds())


densities = best_gmm_classifier.score_samples(x_test)
density_threshold = np.percentile(densities, THRESHOLD_FOR_DENSITY)
anomalies = np.argwhere(densities < density_threshold)
print(len(anomalies))

real_anomaly = np.argwhere(y_test_c == -1)


y_pred_anomalies = y_test_c.copy()
y_pred_anomalies[densities < density_threshold] = -1
y_pred_anomalies[densities >= density_threshold] = 1

acc_gmm = accuracy_score(y_test_c, y_pred_anomalies)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_anomalies))
print("best parameters ", best_gmm_classifier.best_params_)
print('Accuracy ', acc_gmm)
print('elapsed time ', elapsed_time_gmm)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. Regressionê³¼ ë¹„êµí–ˆì„ë•Œ ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. (ê·¸ë˜ë„ One-Class SVMê³¼ Isolation Forestë³´ë‹¤ëŠ” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ê¸´ í•œë‹¤. Auto Encoderì™€ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤.)

|                            | Diabetes               | Boston                 |
| -------------------------- | ---------------------- | ---------------------- |
| Confusion Matrix           | [[24 21]<br />[14 30]] | [[31 24]<br />[13 34]] |
| Anomaly Detection Accuracy | 60.67%                 | 63.72%                 |





## 1-4. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : ì •í™•ë„ %
- Datasetì€ Testset 20%, Training 64%, Validation 16%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨

|      | Algorithm                                | Diabetes   | Boston     |
| ---- | ---------------------------------------- | ---------- | ---------- |
| 1    | SVR                                      | **75.28%** | **88.23%** |
| 2    | One-Class SVM                            | 48.31%     | 57.84%     |
| 3    | Isolation Forest                         | 56.17%     | 62.74%     |
| 4    | Auto-Encoder<br /> for Anomaly Detection | 60.67%     | 63.72%     |
| 5    | Mixture Of Gaussian                      | 60.67%     | 63.72%     |



----

# Tutorial 2. Classification To Anomaly Detection

ì´ë²ˆ Tutorialì€ ê¸°ë³¸ì ì¸ Supervised Classification Taskì— ëŒ€í•˜ì—¬, SVMê³¼ ê°™ì€ Supervised Classification í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ê³¼, ê°™ì€ Dataì— ëŒ€í•˜ì—¬ ê° Classì˜ ì–‘í’ˆ ë°ì´í„°ë§Œ í•™ìŠµí•˜ì—¬ íŒë‹¨ë‚´ë¦¬ëŠ” Anomaly Detectionì˜ ì„±ëŠ¥ì— ëŒ€í•œ ë¹„êµë¥¼ ìˆ˜í–‰í•˜ê³ ì í•œë‹¤.  

ìœ„ì˜ Anomaly Detectionì˜ [Basic Concept](#1-Basic-Concept)ì—ì„œ ì„¤ëª…í–ˆë˜ Decision Making Processë¥¼ ë‹¤ì‹œí•œë²ˆ í•´ì„í•˜ìë©´, ì™ ë§Œí•˜ë©´ Classificationìœ¼ë¡œ í•˜ê³ , ì •ë§ ë¶ˆê· í˜•ì´ ì‹¬í•˜ê³  Anomaly Classì˜ ì ˆëŒ€ì  ë°ì´í„°ëŸ‰ì´ ì‘ì€ ê²½ìš°ì—ë§Œ Anomaly Detectionì„ ì“°ë¼ëŠ” ì˜ë¯¸ë¼ê³  ì´í•´í•  ìˆ˜ ìˆê² ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê³¼ì—° Supervised Classification ì•Œê³ ë¦¬ì¦˜ì´ ê°™ì€ í™˜ê²½ Setting(Dataset ì¢…ë¥˜ ë° Data Instace ë¹„ìœ¨)ì—ì„œ Anomaly Detectionê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ì‹¤í—˜í•´ ë³´ëŠ” Tutorialì„ ì§„í–‰ í•´ ë³´ê² ë‹¤.



## 2-1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/3_anomaly_detection/Tutorials/tutorial_anomaly_detection_from_C_task.ipynb)



## 2-2. Setting

### Datasets

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì´ 3ê°œì˜ ìœ ëª…í•œ Tabular í˜•íƒœì˜ Classification Datasetì„ ì‚¬ìš©í•œë‹¤.  ì „ì²´ ë°ì´í„° ì¤‘ Training Setì€ 64%, Validation Setì€ 16%, Test Setì€ 20%ì˜ Dataë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.

|      | Datasets                      | Description                                                  | Num Instances | Num Inputs (Xs) | Num Outputs (Ys) |
| ---- | ----------------------------- | ------------------------------------------------------------ | ------------- | --------------- | ---------------- |
| 1    | Diabetes (Classification)     | ë‹¹ë‡¨ë³‘ í™˜ì ë°ì´í„° (ì–‘ì„±, ìŒì„±). ì´ 2ê°œ Class.               | 768           | 8               | 1 (0, 1)         |
| 2    | Breast Cancer(Classification) | ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„° (ì–‘ì„±, ìŒì„±). ì´ 2ê°œ Class.           | 569           | 30              | 1 (0, 1)         |
| 3    | Digits (Classification)       | 0~9ê¹Œì§€ì˜ ìˆ«ì ë°ì´í„°. Mini MNIST(8*8 image). ì´ 10ê°œ Class. | 1797          | 64              | 1 (0 ~ 9)        |

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê²Œ ëœë‹¤.

```python
if dataset_name == 'diabetes':
    df = pd.read_csv('diabetes.csv')
    X = df.iloc[:,:-1].values   
    y = df.iloc[:,-1].values    

elif dataset_name == 'breast_cancer':
    breast_cancer = datasets.load_breast_cancer()
    X = breast_cancer.data
    y = breast_cancer.target

elif dataset_name == 'digits':
    digits = datasets.load_digits()
    X = digits.data
    y = digits.target

else:
    pass
```

ê° Datasetì€ Classification Targetì´ë¯€ë¡œ, ê° Datasetì„ Anomalyì— ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ ì‚¬ìš©ë˜ëŠ” ê° ì–‘ë¶ˆ Classì˜ Labelì€ ì•„ë˜ì™€ ê°™ë‹¤. Binary Classê°€ ì•„ë‹Œ Multi-Target Classificationì˜ ê²½ìš°, í•˜ë‚˜ì˜ Labelì„ ë¶ˆëŸ‰ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ, ìì—°ìŠ¤ëŸ½ê²Œ Imbalanced Classification Problemì´ ëœë‹¤.

- Anomaly of Diabetes Dataset : 1 (ì–‘ì„±)
- Anomaly of Breast Cancer Dataset : 1 (ì–‘ì„±)
- Anomaly of Digits Dataset : 5 (ìˆ«ì 5)



ìœ„ì˜ ëª¨ë“  Datasetì€ AnomalyëŠ” -1ë¡œ, ê·¸ ì™¸ì˜ ê²ƒì€ Normalë¡œ +1ë¡œ Re-Labelingí•˜ì—¬ Binary Classificationë¬¸ì œë¡œ ë°”ê¾¼ë‹¤. SVMì€ 2ê°œì˜ Classì— ëŒ€í•œ ëª¨ë“  í•™ìŠµì„ ì§„í–‰í•˜ê³ , ë‚˜ë¨¸ì§€ Anomaly Detection ì•Œê³ ë¦¬ì¦˜ë“¤ì€ +1ì¸ Normal Dataì— ëŒ€í•´ì„œë§Œ í•™ìŠµì„ ì§„í–‰í•˜ê³  Anomalyë¥¼ ì°¾ì•„ë‚´ë„ë¡ í•™ìŠµí•œë‹¤.





### Algorithms

ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ Classification ì•Œê³ ë¦¬ì¦˜ê³¼ Anomaly Detectionì„ ì„œë¡œ ë¹„êµí•œë‹¤.

- Classification
  - SVMì„ ì‚¬ìš©í•˜ì—¬ Classification Taskì—ì„œ Supervised Classification Algorithmì„ ì‚¬ìš©í•œë‹¤. +1ê³¼ -1ì˜ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì´ë‹¤.
- Anomaly Detection
  - 4ê°€ì§€ì˜ ì•Œê³ ë¦¬ì¦˜(One-Class SVM, Isolation Forest, Autoencoder Anomaly Detection, Mixture Of Gaussian)ì„ ì‚¬ìš©í•˜ì—¬, ë°ì´í„°ë¥¼ ì–‘ë¶ˆë¡œ Binary Classificationë¬¸ì œë¡œ ì „ì²˜ë¦¬ í›„, ì–‘í’ˆ ë°ì´í„°ë§Œì„ í•™ìŠµí•˜ì—¬ Anomalyë¥¼ íƒì§€í•œë‹¤.

|      | Algorithm                              | Target            | Description                                                  |
| ---- | -------------------------------------- | ----------------- | ------------------------------------------------------------ |
| 1    | SVM (Linear, Kernel)                   | Classification    | ì´ì§„ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜. ì„ í˜•/ë¹„ì„ í˜• SVM ë‘ê°€ì§€ ëª¨ë‘ Hyper-Param Searchingì— í™œìš©í•´ ìµœì  ëª¨ë¸ ì°¾ìŒ |
| 3    | One-Class SVM                          | Anomaly Detection | ì–‘í’ˆ Sampleë§Œìœ¼ë¡œ í•™ìŠµí•˜ì—¬ Anomaly Detectionì„ ìˆ˜í–‰í•˜ëŠ” SVMì˜ ë³€í˜• ë²„ì „(Nu-SVM). ì–‘í’ˆ Sample Dataê°€ ì›ì ì—ì„œ ê°€ì¥ ë©€ì–´ì§€ê²Œ í•˜ëŠ” Hyper Planeì„ ì°¾ëŠ”ë‹¤. |
| 4    | Isolation Forest                       | Anomaly Detection | ì–‘í’ˆ Sampleë§Œìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ê°„ë‹¨í•œ Decision Tree ì¡°í•©ì„ í†µí•´ Anomalyë¥¼ Detectioní•˜ëŠ” ì•Œê³ ë¦¬ì¦˜. ë¶„ë¥˜ Path Lengthê°€ ê¸¸ìˆ˜ë¡ ì–‘í’ˆì´ë‹¤. |
| 5    | Autoencoder<br />for Anomaly Detection | Anomaly Detection | ì–‘í’ˆ Sampleë§Œì„ í†µí•´ Neural Networkê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ê³ , ë™ì¼í•˜ê²Œ Reconstructioní•˜ëŠ” Taskë¥¼ ìˆ˜í–‰í•˜ì—¬, Anomaly Detectioní•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ |
| 6    | Mixture of Gaussian                    | Anomaly Detection | ì—¬ëŸ¬ê°œì˜ Gaussianì˜ ì„ í˜• ê²°í•©ì„ í†µí•´ ë¶„í¬ë¥¼ ë²—ì–´ë‚˜ëŠ” Dataë¥¼ ì°¾ì•„ë‚´ì–´ Anomaly Detectionì„ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ |



## 2-3. Usage Code

### SVM

í•´ë‹¹ Datasetì—ì„œ ì„±ëŠ¥ì´ ì¢‹ì€ SVMì„ ì‚¬ìš©í•˜ì—¬, Classification Taskë¥¼ ì˜ˆì¸¡í•œë‹¤. ì˜ˆì¸¡ëœ ê²°ê³¼ëŠ” ìœ„ì˜ Datasetì „ì²˜ë¦¬ë¥¼ í†µí•´ ì–‘í’ˆ/ë¶ˆëŸ‰ì˜ 2-Class Classificationì„ ìˆ˜í–‰í•œë‹¤. Linear SVMê³¼ RBF SVMì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchí•˜ì—¬ ëª¨ë¸ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'C': [1.0, 2.0, 3.0, 10.]},
    {'kernel': ['rbf'], 'C': [1.0, 2.0, 3.0, 5.0, 10.],
    'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},
]

elapsed_time_kernel_svm = []

svm_classifier = SVC(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svm_classifier, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svc_classifier = grid_search.fit(x_train, y_train_a)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())
```



ì•„ë˜ì™€ ê°™ì´ ì˜ˆì¸¡í•œ ê°’ì„ ìœ„ì—ì„œ ì„¤ì •í•œ thresholdê°’ìœ¼ë¡œ ì–‘ë¶ˆ(ì–‘í’ˆ +1, ë¶ˆëŸ‰ -1) Labelingì„ í•´ ì¤€ë‹¤. ì´ë¥¼ í†µí•´ì„œ Answer Yê°’ì˜ Classificationëœ ê°’ ê³¼ì˜ ë¹„êµë¥¼ í†µí•´ Accuracyë¥¼ ê³„ì‚°í•œë‹¤.

```python
start_time = datetime.now()
y_pred = best_svc_classifier.predict(x_test)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())
acc_svm_kernel = accuracy_score(y_test_a, y_pred)

print('Confusion Matrix\n', confusion_matrix(y_test_a, y_pred))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svm_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svm)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

|                         | Diabetes               | Breast Cancer          | Digits                      |
| ----------------------- | ---------------------- | ---------------------- | --------------------------- |
| Confusion Matrix        | [[28 28]<br />[10 88]] | [[66  1]<br />[ 1 46]] | [[ 49   0] <br />[  0 311]] |
| Classification Accuracy | 75.32%                 | 98.24%                 | 100%                        |



### One-Class SVM

One-Class SVMì€ Scikit-Learnì— êµ¬í˜„ëœ Nu-SVMì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì•„ë˜ì™€ê°™ì€ param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchingí•˜ì—¬ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ìœ¼ë©° X_Trainê°’ ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7]},
    {'kernel': ['rbf'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7],
    'gamma': [0.01, 0.03, 0.1, 0.3, 0.05, 1.0]},
]

elapsed_time_kernel_svm = []

svm_classifier = OneClassSVM(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svm_classifier, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svm_classifier = grid_search.fit(x_train_only)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())


```



Inference ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•˜ì˜€ë‹¤.

```python
start_time = datetime.now()
y_pred = best_svm_classifier.predict(x_test)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())

acc_svm_kernel = accuracy_score(y_test_c, y_pred)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svm_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svm)
# Isolation Forest 
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. SVMê³¼ ë¹„êµí–ˆì„ë•Œ ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. íŠ¹íˆ Diabetesì™€ DigitsëŠ” False Negativeê°€ êµ‰ì¥íˆ ë†’ìœ¼ë©°(ê·¸ëƒ¥ ë‹¤ ë¶ˆëŸ‰ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤), Digitsê°™ì€ ê²½ìš° 20% ì´ìƒì˜ Accuracy ì°¨ì´ê°€ ë°œìƒí•œë‹¤. ë°˜ë©´ Breast CancerëŠ” SVMë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ë‚®ìœ¼ë‚˜, ê·¸ë˜ë„ 93.85%ë¼ëŠ” ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²½ìš°ë„ ì¡´ì¬í•œë‹¤.

|                            | Diabetes                | Breast Cancer           | Digits                      |
| -------------------------- | ----------------------- | ----------------------- | --------------------------- |
| Confusion Matrix           | [[ 1 55]<br /> [ 7 91]] | [[60  7]<br /> [ 0 47]] | [[  3  46]<br /> [ 27 284]] |
| Anomaly Detection Accuracy | 59.74%                  | 93.85%                  | 79.72%                      |



### Isolation Forest

Isolation Forestì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì–‘í’ˆ ë°ì´í„°(+1 Class)ë§Œì„ í•™ìŠµ í•˜ì˜€ë‹¤. Hyper Parameterë„ ì•„ë˜ì™€ ê°™ì´ iforest_parametersì— ì„¤ì •ëœ ê°’ì„ Grid-Search í•˜ì˜€ë‹¤.

```python
iforest_classifier = IsolationForest()

iforest_parameters = {'n_estimators': list(range(10, 200, 50)), 
              'max_samples': list(range(20, 120, 20)), 
              'contamination': [0.1, 0.2], 
              'max_features': [5,15, 20], 
              'bootstrap': [True, False], 
              }

elapsed_time_iforest = []

start_time = datetime.now()
iforest_grid_search = GridSearchCV(iforest_classifier, iforest_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_iforest_classifier = iforest_grid_search.fit(x_train_only)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())
```

InferenceëŠ” ì•„ë˜ì™€ ê°™ì´ ìˆ˜í–‰í•œë‹¤. ì—­ì‹œ Classificationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³ , Testì •ë‹µê°’ê³¼ì˜ ë¹„êµë¥¼ ìˆ˜í–‰í•œë‹¤.

```python
# y_pred = xgb_classifier.predict(x_test)
start_time = datetime.now()
y_pred_c = best_iforest_classifier.predict(x_test)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())


acc_iforest = accuracy_score(y_test_c, y_pred_c)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_c))
print("best parameters ", iforest_grid_search.best_params_)
print('Accuracy ', acc_iforest)
print('elapsed time ', elapsed_time_iforest)
```





ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. SVMê³¼ ë¹„êµí–ˆì„ë•Œ ëª¨ë‘ ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•˜ë‹¤. ê·¸ë˜ë„ DiabetesëŠ” SVMë³´ë‹¤ 5% ì´í•˜ì˜ ì„±ëŠ¥ ì €í•˜ê°€ ìˆì—ˆê¸° ë•Œë¬¸ì— ë‚˜ì˜ì§€ ì•Šì€ ê²°ê³¼ë¼ê³ ë„ ë³¼ ìˆ˜ ìˆê² ë‹¤. ë°˜ë©´ Breast Cancerì™€ DigitsëŠ” SVM ëŒ€ë¹„ ë§¤ìš° ì„±ëŠ¥ì´ ì €ì¡°í•˜ë‹¤.

|                            | Diabetes                | Breast Cancer           | Digits                      |
| -------------------------- | ----------------------- | ----------------------- | --------------------------- |
| Confusion Matrix           | [[23 33]<br /> [11 87]] | [[49 18]<br /> [ 5 42]] | [[ 16  33]<br /> [ 41 270]] |
| Anomaly Detection Accuracy | 71.42%                  | 79.82%                  | 79.44%                      |





### Auto-Encoder for Anomaly Detection

ë”¥ëŸ¬ë‹ ê³„ì—´ì˜ Auto-Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ Anomaly Detectionì„ ìˆ˜í–‰í•œë‹¤. SELU Activation Functionì„ í†µí•´ BatchNormë“±ì„ ì§€ìš°ê³ ë„ ì„±ëŠ¥ì„ ì–´ëŠì •ë„ ë„ë‹¬ í•  ìˆ˜ ìˆê²Œ ëª¨ë¸ì„ ì„¸íŒ…í•˜ì˜€ë‹¤. EncoderëŠ” 2ê°œ, Decoderë„ 2ê°œì˜ Layerë¥¼ ê°€ì§€ê³  ìˆë‹¤.

```python
class BasicAutoEncoder(nn.Module):
    def __init__(self) -> None:
        super(BasicAutoEncoder, self).__init__()

        self.layer_1 = nn.Linear(NUM_INPUT, NUM_1ST_HIDDEN)
        self.layer_2 = nn.Linear(NUM_1ST_HIDDEN, NUM_2ND_HIDDEN)
        self.layer_3 = nn.Linear(NUM_2ND_HIDDEN, NUM_1ST_HIDDEN)
        self.layer_4 = nn.Linear(NUM_1ST_HIDDEN, NUM_INPUT)

        self.actvation_1 = nn.SELU()
        self.actvation_2 = nn.SELU()
        self.actvation_3 = nn.SELU()
    
    def forward(self, inputs):
        x = self.actvation_1(self.layer_1(inputs))
        x = self.actvation_2(self.layer_2(x))
        x = self.actvation_3(self.layer_3(x))
        x = self.layer_4(x)

        return x
        
```

InferenceëŠ” ì•„ë˜ì™€ ê°™ë‹¤. Test ì…ë ¥ê°’ Xì™€ ëª¨ë¸ ì¶œë ¥ ì˜ˆì¸¡ê°’ê³¼ì˜ ì ˆëŒ€ê°’ ì°¨ì´ë¥¼ ë¹„êµí•˜ì—¬, Reconstruction Errorë¥¼ êµ¬í•˜ì—¬, ê·¸ì— ë”°ë¼ Thresholdingí•˜ì—¬ ì–‘ë¶ˆì„ íŒì •í•˜ì˜€ë‹¤.

```python
result_reconstruct = abs(x_test - output_num).sum(axis=1)
result_class = result_reconstruct.copy()
result_class[result_reconstruct > THRESHOLD_FOR_AUTOENCODER] = -1
result_class[result_reconstruct <= THRESHOLD_FOR_AUTOENCODER] = 1
acc_ae = accuracy_score(y_test_a, result_class)

print('Confusion Matrix\n', confusion_matrix(y_test_a, result_class))
print('Accuracy ', acc_ae)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. Supervised Classificationì¸ SVMê³¼ ë¹„êµí–ˆì„ë•Œ ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë˜ë„ ë‹¤ë¥¸ Anomaly Detection ì•Œê³ ë¦¬ì¦˜ë“¤ ë³´ë‹¤ëŠ” Digitsì—ì„œ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤.(93.33%) ì´ëŠ” ì–´ëŠì •ë„ Digits Datasetì—ì„œëŠ” AEê°€ ë‚˜ì˜ì§€ ì•Šì€ ê²°ê³¼ë¥¼ ë³´ì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

|                            | Diabetes                | Breast Cancer           | Digits                      |
| -------------------------- | ----------------------- | ----------------------- | --------------------------- |
| Confusion Matrix           | [[26 30]<br /> [20 78]] | [[45 22]<br /> [19 28]] | [[ 35  14]<br /> [ 10 301]] |
| Anomaly Detection Accuracy | 67.53%                  | 64.03%                  | 93.33%                      |





### Mixture Of Gaussian

MoGë¥¼ ì‚¬ìš©í•˜ì—¬ Anomaly Detectionì„ ìˆ˜í–‰í•œë‹¤. ParameterëŠ” gmm_parametersì— ìˆëŠ” ê²ƒì„ Grid-Searchí•˜ì—¬ ëª¨ë¸ì„ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤. ì—­ì‹œ í•˜ë‚˜ì˜ ì–‘í’ˆ Labelì— ëŒ€í•œ Training Setìœ¼ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•˜ì˜€ë‹¤.

```python
gmm_classifier = GaussianMixture()

gmm_parameters ={'n_components' : [1, 2, 3,4,5,6, 7] , 'max_iter': [int(1e2), int(1e3), int(1e6)]}

elapsed_time_gmm= []

start_time = datetime.now()
gmm_grid_search = GridSearchCV(gmm_classifier, gmm_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_gmm_classifier = gmm_grid_search.fit(x_train_only)
elapsed_time_gmm.append((datetime.now()-start_time).total_seconds())

```



MoGì˜ ê²½ìš° Thresholdë¥¼ ì§€ì •í•˜ê¸° ìœ„í•˜ì—¬ Percentile(ë°±ë¶„ìœ„ìˆ˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ Densityì— ëŒ€í•´ì„œ ëª‡ % ë¯¸ë§Œê¹Œì§€ Anomalyë¥¼ ì •í• ì§€ Thresholdë¥¼ ê²°ì •í•˜ê²Œ ëœë‹¤. AutoEncoderì—ì„œ Reconstruction Errorì— ëŒ€í•œ Thresholdë¥¼ ì§€ì •í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•œ Hyper-Parameterì´ë‹¤.

```python
start_time = datetime.now()
y_pred_c = best_gmm_classifier.predict(x_test)
elapsed_time_gmm.append((datetime.now()-start_time).total_seconds())


densities = best_gmm_classifier.score_samples(x_test)
density_threshold = np.percentile(densities, THRESHOLD_FOR_DENSITY)
anomalies = np.argwhere(densities < density_threshold)
print(len(anomalies))

real_anomaly = np.argwhere(y_test_c == -1)


y_pred_anomalies = y_test_c.copy()
y_pred_anomalies[densities < density_threshold] = -1
y_pred_anomalies[densities >= density_threshold] = 1

acc_gmm = accuracy_score(y_test_c, y_pred_anomalies)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_anomalies))
print("best parameters ", best_gmm_classifier.best_params_)
print('Accuracy ', acc_gmm)
print('elapsed time ', elapsed_time_gmm)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. MoGë„ ì—­ì‹œ SVMë³´ë‹¤ ëª¨ë‘ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤. ì „ë°˜ì ìœ¼ë¡œ AEì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì˜ ì„±í–¥ì„ ë³´ì¸ë‹¤. ëª¨ë“  ê²°ê³¼ë¥¼ ë³´ì•˜ì„ë•Œ ë”±íˆ íŠ¹ì¶œë‚˜ê²Œ ì–´ëŠ Datasetì— ì¢‹ì€ ê²°ê³¼ë¼ê³  í•´ì„í•˜ê¸°ëŠ” í˜ë“¤ ê²ƒ ê°™ë‹¤.

|                            | Diabetes                | Breast Cancer           | Digits                      |
| -------------------------- | ----------------------- | ----------------------- | --------------------------- |
| Confusion Matrix           | [[32 24]<br /> [24 74]] | [[56 11]<br /> [24 23]] | [[ 41   8]<br /> [ 42 269]] |
| Anomaly Detection Accuracy | 68.83%                  | 69.29%                  | 86.11%                      |





## 2-4. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : ì •í™•ë„ %
- Datasetì€ Testset 20%, Training 64%, Validation 16%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨

|      | Algorithm                                | Diabetes   | Breast Cancer | Digits   |
| ---- | ---------------------------------------- | ---------- | ------------- | -------- |
| 1    | SVM                                      | **75.32%** | **98.24%**    | **100%** |
| 2    | One-Class SVM                            | 59.74%     | 93.85%        | 79.72%   |
| 3    | Isolation Forest                         | 71.42%     | 79.82%        | 79.44%   |
| 4    | Auto-Encoder<br /> for Anomaly Detection | 67.53%     | 64.03%        | 93.33%   |
| 5    | Mixture Of Gaussian                      | 68.83%     | 69.29%        | 86.11%   |



----

# Final Insights

## 1. Regression To Anomaly Detection

|      | Algorithm                                | Diabetes   | Boston     |
| ---- | ---------------------------------------- | ---------- | ---------- |
| 1    | SVR                                      | **75.28%** | **88.23%** |
| 2    | One-Class SVM                            | 48.31%     | 57.84%     |
| 3    | Isolation Forest                         | 56.17%     | 62.74%     |
| 4    | Auto-Encoder<br /> for Anomaly Detection | 60.67%     | 63.72%     |
| 5    | Mixture Of Gaussian                      | 60.67%     | 63.72%     |

- ğŸ”¥**ê²°ë¡ ì ìœ¼ë¡œ Anomaly Detectionì€ ìœ„ì™€ ê°™ì€ ê·¼ë³¸ì  Regression Taskì—ì„œëŠ” ì‚¬ìš©ì„ ìì œí•˜ëŠ” ê²Œ ì¢‹ì„ ê²ƒ ê°™ë‹¤.**
- ìœ„ì˜ í‘œë¥¼ í™•ì¸í•˜ìë©´, Regression Taskì—ì„œ SVR Regressionìœ¼ë¡œ Regressioní•œ í›„ì—, ì–‘ë¶ˆì„ íŒì •í•˜ëŠ” ë°©ë²•ì´ ë‹¤ë¥¸ ëª¨ë“  Anomaly Detection ë°©ë²•ì„ ì••ë„í•œë‹¤.
- ê·¼ë³¸ì ìœ¼ë¡œ Regressionì˜ íŠ¹ì„±ì„ ì§€ë‹ˆê³  ìˆëŠ” Datasetì— ëŒ€í•˜ì—¬ Thresholdë¥¼ ë‚˜ëˆ ì„œ Classification ë¬¸ì œë¡œ ë³€í˜•í›„ Anomaly Detectionìœ¼ë¡œ í’€ ë•Œì—ëŠ” ì „ë°˜ì ìœ¼ë¡œ Anomaly Detectionì•Œê³ ë¦¬ì¦˜ì´ ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì§€ ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
- ìµœê·¼ì˜ ë”¥ëŸ¬ë‹ ì—°êµ¬ê²°ê³¼ë“¤ì„ ë³´ë©´ Targetì´ Continousí•œ Regression Taskëƒ, í˜¹ì€ Discreteí•œ Classificationì´ëƒì— ë”°ë¼ì„œ ì•Œê³ ë¦¬ì¦˜ì´ í•™ìŠµí•˜ëŠ” Feature(Representation)ì´ ì „í˜€ ë‹¤ë¥¸ ì–‘ìƒì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì´ ë°í˜€ì§€ê³  ìˆë‹¤. ê·¸ ì—°êµ¬ë“¤ì—ì„œëŠ” Regression Taskì—ì„œëŠ” Featureë“¤ ì—­ì‹œ Continousí•˜ê³  Orderingì´ ìˆëŠ” ë°©ì‹ìœ¼ë¡œ Representationëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.
- ì´ëŸ¬í•œ ì˜ë¯¸ë¡œ, Anomaly Detectionì€ íŠ¹íˆë‚˜ Imbalanced Classificationìƒí™©ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²½ìš°ê°€ ë§ì€ë°, ì—­ì‹œ Imbalancedí•œ ìƒí™©ì—ì„œ Classificationìœ¼ë¡œ í’€ë ¤ê³ í•˜ëŠ” ì ‘ê·¼ ë°©ë²•ë¡ ë„ ë§ì´ ìˆë‹¤. ê·¸ëŸ°ë° ì´ëŸ¬í•œ Regression Taskì—ì„œ Imbalanced Classification Methodë¥¼ ì‚¬ìš©í•˜ë©´ ì˜ í•™ìŠµì´ ì•ˆë˜ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì¸ë°, ì´ê²ƒë„ ì—­ì‹œ ê·¼ë³¸ì ì¸ Regression Taskê°€ Dataìƒìœ¼ë¡œ í’ˆê³  ìˆëŠ” Representationì´ Classificationê³¼ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ì–‘ìƒì„ ë„ê³  ìˆê¸° ë•Œë¬¸ì´ë¼ê³  ì´í•´ë  ìˆ˜ ìˆê² ë‹¤.
- í˜„ì—…ì—ì„œëŠ” ë¬¼ë¦¬ì ì´ë©° Continousí•œ ìˆ˜ì¹˜ ì¸¡ì •ì˜ ê²°ê³¼ê°€ ëŒ€ë¶€ë¶„ì´ë¯€ë¡œ, ì´ëŸ¬í•œ íŠ¹ì„±ì„ ë„ëŠ” ê²½ìš°ê°€ ì•„ì£¼ ë§ì´ ìˆë‹¤.(ë¬¼ë¡  ì˜ìƒ ì–‘ë¶ˆíŒì • ê°™ì€ ê²ƒì€ ê·¼ë³¸ì ìœ¼ë¡œ Classificationë¬¸ì œì¼ í™•ë¥ ì´ ë†’ë‹¤. ê·¼ë° ì´ëŸ° Vision Taskë“¤ì€ ëŒ€ë¶€ë¶„ì´ ë„ˆë¬´ë‚˜ í’€ê¸° ì‰¬ìš´ ë¬¸ì œë“¤ì´ë‹¤..)



## 2. Classification To Anomaly Detection

|      | Algorithm                                | Diabetes   | Breast Cancer | Digits   |
| ---- | ---------------------------------------- | ---------- | ------------- | -------- |
| 1    | SVM                                      | **75.32%** | **98.24%**    | **100%** |
| 2    | One-Class SVM                            | 59.74%     | 93.85%        | 79.72%   |
| 3    | Isolation Forest                         | 71.42%     | 79.82%        | 79.44%   |
| 4    | Auto-Encoder<br /> for Anomaly Detection | 67.53%     | 64.03%        | 93.33%   |
| 5    | Mixture Of Gaussian                      | 68.83%     | 69.29%        | 86.11%   |

- ğŸ”¥**ê²°ë¡ ì ìœ¼ë¡œ Anomaly Detectionì€ ìœ„ì™€ ê°™ì€ ê·¼ë³¸ì  Supervised Classification Taskì—ì„œëŠ” ì‚¬ìš©ì„ ìì œí•˜ëŠ” ê²Œ ì¢‹ì„ ê²ƒ ê°™ë‹¤.**
- ìœ„ì˜ í‘œë¥¼ í™•ì¸ìí•˜ë©´, Classificationë¬¸ì œì— ìˆì–´ì„œ Labelì„ ë‹¤ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ë©´ ì—­ì‹œ Supervised Classificationì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ë¥¸ ëª¨ë“  ê²ƒì„ ì••ë„í•˜ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë”°. (ë¬¼ë¡  ëª‡ëª‡ Datasetì—ì„œ íŠ¹ì • ë°©ë²•ë¡ ì´ SVMë³´ë‹¤ ì•„ì£¼ ì¡°ê¸ˆ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë©´ì„œ í•´ê²°í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤.)
- ìµœê·¼ ì—°êµ¬ ê²°ê³¼ë¥¼ ë³´ë©´ Representation Learningì„ ìˆ˜í–‰í•  ë•Œ, Targetê°’ì´ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ Machine Learningì•Œê³ ë¦¬ì¦˜ë“¤ì´ ì˜ í‘œí˜„ì„ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì´ ì•Œë ¤ì ¸ ìˆë‹¤.
- íŠ¹íˆë‚˜ Real-Worldì˜ ë¬¸ì œì—ì„œëŠ” ëŒ€ë¶€ë¶„ ì‹œê°„ì´ ê±¸ë¦¬ë”ë¼ë„ Labelingì„ í†µí•˜ì—¬ Supervised Learningìœ¼ë¡œ í’€ë ¤ê³  í•˜ëŠ”ë°, ì´ëŠ” ìœ„ì˜ ê²°ê³¼ì™€ ë§ˆì°¬ê°€ì§€ë¡œ Supervised Learningì´ ì¼ë°˜ì ìœ¼ë¡œ Unsupervised Learningë³´ë‹¤ ë” ì„±ëŠ¥ì´ ëŒ€ë¶€ë¶„ ì¢‹ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬í•œ Trendë•Œë¬¸ì— ìš”ì¦˜ì— Contrasitive, Self-Training ë“±ì´ ì¢€ ë” ê°ê´‘ì„ ë°›ëŠ”ê²Œ ì•„ë‹Œê°€ ì‹¶ê¸°ë„ í•˜ë‹¤.
- ì„±ëŠ¥ì„ ìœ„í•´ì„œë¼ë©´ Anomaly Detectionê³¼ ê°™ì€ ë°©ë²•ë³´ë‹¤ëŠ” Supervisedë‚˜ í˜¹ì€ Semi-Supervisedë¥¼ ì‚¬ìš©í•˜ëŠ” í¸ì´ ì¢€ ë” ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ë¦¬ë¼ ìƒê°í•œë‹¤.
- ë¬¼ë¡  Imbalancedí•œ ìƒí™©ì´ ë§¤ìš° ê·¹ë‹¨ì ì¼ ê²½ìš°ëŠ”, Supervised Classificationì„ ì•„ì–˜ ì‚¬ìš©í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ë˜í•œ ê·¼ë³¸ì ìœ¼ë¡œ Labelingì„ í•˜ê¸°ê°€ ì •ë§ë¡œ ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ Supervised Classificationì´ ë°”ë¡œ ì‚¬ìš©ë˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ì´ëŸ¬í•œ ê²½ìš°ì—ëŠ” Anomaly Detectionë„ ë¬¼ë¡  ì ìš©í•˜ì—¬ ì¢‹ì€ íš¨ê³¼ë¥¼ ë°œíœ˜í•  ìˆ˜ë„ ìˆë‹¤.
- ê·¸ëŸ¬ë‚˜ Supervised Classificationì„ ì‚¬ìš©í•˜ê¸° ì–´ë µë”ë¼ë„, ê´€ì ì— ë”°ë¼ ìœ„ì˜ Tutorial 1ì²˜ëŸ¼ Regression Taskë¡œ ë¬¸ì œë¥¼ ë³€í™˜í•  ìˆ˜ ìˆì„ ê²½ìš°, ê·¸ë¦¬ê³  ë™ì‹œì— Imbalanced Dataìƒí™©ì—ì„œëŠ” Anomaly Detectionë³´ë‹¤ëŠ” Regressionì„ ìˆ˜í–‰í•˜ê¸°ë¥¼ ì¶”ì²œí•œë‹¤.





## 3. Conclusion

- Anoamaly Detectionì€ ê·¸ í•œê³„ì„±ë„ ë¶„ëª…íˆ ìˆìœ¼ë¯€ë¡œ, ë¬´ì§€ì„±ìœ¼ë¡œ ì‰½ê²Œ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ê° ë¬¸ì œê°€ ê°–ê³  ìˆëŠ” ê·¼ë³¸ì ì¸ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ì í•©í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ ì ìš©ì„ í•´ì•¼ í•œë‹¤.



-----

# References

-  https://deepai.org/machine-learning-glossary-and-terms/anomaly-detection
- https://www.tibco.com/reference-center/what-is-anomaly-detection
- ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ê°•í•„ì„± êµìˆ˜ë‹˜ Business Analytics ê°•ì˜ ìë£Œ
