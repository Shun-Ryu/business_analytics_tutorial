# Anomaly Detectionì˜ í•œê³„ë¥¼ ì•Œì•„ë³´ì.



## The limitation of anomaly detection algorithm



ğŸ”¥ì´ë²ˆ Tutorialì—ì„œëŠ” Anomaly Detectionì´ ì‹¤ì œ Practicalí•œ ìƒí™©ì—ì„œ ì–´ë– í•œ ì œì•½ì ì´ ìˆëŠ”ì§€ íŒŒì•…í•´ ë³´ê³ , ì–´ë– í•  ë•Œ ì“°ë©´ ì•ˆë˜ëŠ”ì§€ í™•ì¸ í•´ ë³´ê³ ì í•œë‹¤. í™•ì¸í•´ë³´ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” 2ê°€ì§€ ì´ë‹¤.



### 1. Anomaly Detection for "Regression To Anomaly Detection"

- Manufacturing ê³µì • ë“±ì˜ DataëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Targetê°’ì´ Continuousí•œ ê²½ìš°ê°€ ë§ì´ ìˆë‹¤. ì´ëŸ° ê²½ìš°, íŠ¹ì • Threshold ì´ìƒì˜ ê°’ì€ 'ì´ìƒì¹˜(Anomaly)'ë¡œ, Threshold ì´í•˜ì˜ ê°’ì€ 'ì •ìƒì¹˜(Normal)'ë¡œ Target Dataë¥¼ Binary Categorizationì„ í•˜ì—¬, Anomaly Detectionì´ë‚˜ Classificationìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ë ¤ëŠ” ì‹œë„ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ìƒê°í•œë‹¤.

- ë”°ë¼ì„œ ì´ë ‡ê²Œ ê·¼ë³¸ì ìœ¼ë¡œ Regression Taskì¸ ê²ƒë“¤ì„ Thresholdingí•˜ì˜€ì„ ë•Œ, ê³¼ì—° Anomaly Detectionê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ ì˜ ë™ì‘í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ ë³´ì•˜ë‹¤. (íŠ¹íˆ í˜„ì—…ì˜ Manufacturing ê³µì •ì—ì„œëŠ” ëŒ€ë¶€ë¶„ì´ 'ì •ìƒ'ë°ì´í„°ì´ë©° 'ì´ìƒ'ë°ì´í„°ëŠ” ë§¤ìš° ì ì€ Imbalancedí•œ Dataê°€ ëŒ€ë¶€ë¶„ì˜ Caseì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë²ˆ ì‹¤í—˜ì—ì„œëŠ” Regressionì„ í†µí•œ Imbalancedí•œ ë¶€ë¶„ì„ ì—†ì• ê¸° ìœ„í•˜ì—¬ Normal Classì™€ Abnormal Classë¥¼ 5:5 ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.)

- ì´ë¥¼ í†µí•´ Anomaly Detectionì„ ê·¼ë³¸ì  Regression Taskì— ì¨ë„ ë ì§€, ê·¸ í•œê³„ë¥¼ ì•Œì•„ë³´ë ¤ê³  í•œë‹¤.

### 2. Anomaly Detection for "Classification To Anomaly Detection"

- ì¼ë°˜ì ìœ¼ë¡œ Unsupervisedê¸°ë°˜ì˜ Anomaly Detectionë³´ë‹¤ Supervised Classificationì´ ì„±ëŠ¥ì´ ë” ë†’ì€ ê²½ìš°ê°€ ë§ì´ ìˆë‹¤.
- ì‹¤ì œë¡œ ê°„ë‹¨í•œ Taskì—ì„œ Anomaly Detectionì´ Supervised Classificationë³´ë‹¤ ì–´ëŠì •ë„ì˜ ì„±ëŠ¥ì˜ ì°¨ì´ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ë ¤ í•œë‹¤.
- ì´ë¥¼ í†µí•´ Anomaly Detectionì´ Supervised Classification Taskì— ì¨ë„ ë ì§€, ê·¸ í•œê³„ë¥¼ ì•Œì•„ë³´ë ¤ í•œë‹¤.





# Table of Contents

- [Background of SVM](#Background-of-SVM)
  
  - [1. Basic Concept](#1-Basic-Concept)
  - [2. About SVM](#2-About-SVM)
  - [3. Linear SVM](#3-Linear-SVM)
  - [4. Kernel SVM](#4-Kernel-SVM)
  
- [Tutorial - Competion for tabular datasets](#Tutorial_Competion-for-tabular-datasets)
  
  - [1. Tutorial Notebook](#1-Tutorial-Notebook)
  - [2. Setting](#2-Setting)
  - [3. Result (Accuracy)](#3-Result_Accuracy)
  - [4. Result (Training Time)](#4-Result_Training-Time)
  - [5. Result (Inference Time)](#5-Result_Inference-Time)
  
- [Final Insights](#Final-Insights)
  
  - [1. Training Time ê´€ì ](#1-Training-Time-ê´€ì )
  - [2. Inference Time ê´€ì ](#2-Inference-Time-ê´€ì )
  - [3. Accuracy ê´€ì ](#3-Accuracy-ê´€ì )
  - [4. ê·¸ ì™¸ì˜ ìƒê°ë“¤](#4-ê·¸-ì™¸ì˜-ìƒê°ë“¤)
  - [5. ê²°ë¡ ](#5-ê²°ë¡ )
  
  







# Background of Anomaly Detection

## 1. Basic Concept

- - -




## 2. One-Class SVM

- 

## 3. Isolation Forest

- - 

## 4. Auto-Encoder for Anomaly Detection

- 
## 5. Mixture of Gaussian


- 



# Tutorial_Regression_2_AnomalyDetection

ìœ„ì—ì„œ ìš°ë¦¬ëŠ” SVMì— ëŒ€í•´ì„œ ìƒì„¸íˆ ì•Œì•„ë³´ì•˜ìœ¼ë‹ˆ, ê³¼ì—° SVMì´ í˜„ì¬ì—ë„ Tabular Dataì—ì„œ ì ì ˆí•œ ì„ íƒì¸ì§€ ë¹„êµë¥¼ í•´ë³´ì. ì•„ë˜ì˜ Tutorial Linkë¥¼ í†µí•´ Notebookìœ¼ë¡œ ê° Datasetì— ë”°ë¥¸ Algorithmì˜ ì†ë„ì™€ ì„±ëŠ¥ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤.



## 1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/2_kernel_based_learning/Tutorials/tutorial_svm_comparison.ipynb)



## 2. Setting

### Datasets

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì´ 2ê°œì˜ ìœ ëª…í•œ Tabular í˜•íƒœì˜ Regression Datasetì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 

|      | Datasets                        | Description                                                  | Num Instances | Num Inputs (Xs) | Num Outputs (Ys) |
| ---- | ------------------------------- | ------------------------------------------------------------ | ------------- | --------------- | ---------------- |
| 1    | Diabetes (Regression)           | ë‹¹ë‡¨ë³‘ í™˜ì ë°ì´í„° (1ë…„ í›„ ë‹¹ë‡¨ì˜ ì§„í–‰ì •ë„ë¥¼ Targetê°’ìœ¼ë¡œ í•¨) | 442           | 10              | 1                |
| 2    | Boston House Price (Regression) | Bostonì˜ ì§‘ê°’ì— ëŒ€í•œ Data                                    | 506           | 13              | 1                |

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê²Œ ë©ë‹ˆë‹¤.

```python
if dataset_name == 'diabetes_r':
    x, y= datasets.load_diabetes(return_x_y=true)
elif dataset_name == 'boston_r':
    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=none)
    x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    y = raw_df.values[1::2, 2]
else:
    pass
```

ê° Datasetì€ Regression Targetì´ë¯€ë¡œ, ê° Datasetì„ Anomalyì— ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ ì‚¬ìš©ë˜ëŠ” Thresholdê°’ì€ ì•„ë˜ì™€ ê°™ë‹¤. ê° ê°’ì€ ì „ì²´ ë°ì´í„°ì˜ Median ê°’ì´ë‹¤. Regression Taskì— Imbalancedì— ì˜í•œ ì˜í–¥ì„ ì¤„ì´ê¸° ìœ„í•´ ì¤‘ì•™ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì–‘ë¶ˆ Dataì˜ Balanceë¥¼ ë§ì¶”ì—ˆë‹¤.

- Diabetes : 140 
- Boston House Price : 21





### Algorithms

ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ Regression ì•Œê³ ë¦¬ì¦˜ê³¼ Anomaly Detectionì„ ì„œë¡œ ë¹„êµí•©ë‹ˆë‹¤.

- Regerssion 
  - SVRì„ ì‚¬ìš©í•˜ì—¬ Regression Taskì—ì„œ Regression Algorithmì„ ì‚¬ìš©í•˜ê³  ì˜ˆì¸¡í•œ ê°’ì„ íŠ¹ì • Thresholdë¡œ Classificationí•˜ì—¬ ì–‘ë¶ˆì„ íŒì •í•˜ëŠ”ë° ì‚¬ìš©í•©ë‹ˆë‹¤.
- Anomaly Detection
  - 4ê°€ì§€ì˜ ì•Œê³ ë¦¬ì¦˜(One-Class SVM, Isolation Forest, Autoencoder Anomaly Detection, Mixture Of Gaussian)ì„ ì‚¬ìš©í•˜ì—¬, ë°ì´í„°ë¥¼ ì–‘ë¶ˆë¡œ Binary Classificationë¬¸ì œë¡œ ì „ì²˜ë¦¬ í›„, ì–‘í’ˆ ë°ì´í„°ë§Œì„ í•™ìŠµí•˜ì—¬ Anomalyë¥¼ íƒì§€í•œë‹¤.

|      | Algorithm           | Target            | Description                               |
| ---- | ------------------- | ----------------- | ----------------------------------------- |
| 1    | Linear SVR          | Regression        | ì„ í˜• SVR                                  |
| 2    | Kernel SVR          | Regression        | ì„ í˜• SVR + Kernel Trick(using rbf kernel) |
| 3    | One-Class SVM       | Anomaly Detection |                                           |
| 4    | Isolation Forest    | Anomaly Detection |                                           |
| 5    | Autoencoder AD      | Anomaly Detection |                                           |
| 6    | Mixture of Gaussian | Anomaly Detection |                                           |



## 3. Usage Code

### SVR

ì„±ëŠ¥ì´ ì–´ëŠì •ë„ ê²€ì¦ëœ ê¸°ë²•ì¸ SVRì„ ì‚¬ìš©í•˜ì—¬, Regression Taskë¥¼ ì˜ˆì¸¡í•œë‹¤. ê·¸ë¦¬ê³  ì˜ˆì¸¡ëœ ê²°ê³¼ë¥¼ Thresholdë¡œ ë‚˜ëˆ„ì–´, ì–‘ë¶ˆì„ íŒì •í•œë‹¤. ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ í•™ìŠµê³¼ ì¶”ë¡ í•˜ì—¬ Regressionì„ ì˜ˆì¸¡í•œë‹¤. Linear SVRê³¼ RBF SVRì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchí•˜ì—¬ ëª¨ë¸ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'C': [1.0, 2.0, 3.0, 10., 30., 100.]},
    {'kernel': ['rbf'], 'C': [1.0, 2.0, 3.0, 5.0, 10., 30., 100.],
    'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},
]

elapsed_time_kernel_svr = []

svr_regressor = SVR(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svr_regressor, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svr_regressor = grid_search.fit(x_train, y_train)
elapsed_time_kernel_svr.append((datetime.now()-start_time).total_seconds())

start_time = datetime.now()
y_pred = best_svr_regressor.predict(x_test)
elapsed_time_kernel_svr.append((datetime.now()-start_time).total_seconds())

```



ì•„ë˜ì™€ ê°™ì´ ì˜ˆì¸¡í•œ ê°’ì„ ìœ„ì—ì„œ ì„¤ì •í•œ thresholdê°’ìœ¼ë¡œ ì–‘ë¶ˆ(ì–‘í’ˆ +1, ë¶ˆëŸ‰ -1) Labelingì„ í•´ ì¤€ë‹¤. ì´ë¥¼ í†µí•´ì„œ Answer Yê°’ì˜ Classificationëœ ê°’ ê³¼ì˜ ë¹„êµë¥¼ í†µí•´ Accuracyë¥¼ ê³„ì‚°í•œë‹¤.

```python
y_pred_c = y_pred.copy()
y_pred_c[y_pred > threshold_anomaly] = -1
y_pred_c[y_pred <= threshold_anomaly] = 1

acc_svr_kernel = accuracy_score(y_test_c, y_pred_c)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_c))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svr_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svr)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

|                                                           | Diabetes               | Boston                  |
| --------------------------------------------------------- | ---------------------- | ----------------------- |
| Confusion Matrix                                          | [[34 11]<br/> [11 33]] | [[49  6] <br />[ 6 41]] |
| Classification Accuracy<br />(by Regression Thresholding) | 75.28%                 | 88.23%                  |



### One-Class SVM

One-Class SVMì€ Scikit-Learnì— êµ¬í˜„ëœ Nu-SVMì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì•„ë˜ì™€ê°™ì€ param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchingí•˜ì—¬ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ìœ¼ë©° X_Trainê°’ ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7]},
    {'kernel': ['rbf'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7],
    'gamma': [0.01, 0.03, 0.1, 0.3, 0.05, 1.0]},
]

elapsed_time_kernel_svm = []

svm_classifier = OneClassSVM(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svm_classifier, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svm_classifier = grid_search.fit(x_train_only)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())


```



Inference ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•˜ì˜€ë‹¤.

```python
start_time = datetime.now()
y_pred = best_svm_classifier.predict(x_test)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())

acc_svm_kernel = accuracy_score(y_test_c, y_pred)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svm_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svm)
# Isolation Forest 
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•Šë‹¤. ğŸ”¥)

|                            | Diabetes               | Boston                 |
| -------------------------- | ---------------------- | ---------------------- |
| Confusion Matrix           | [[ 2 43] <br/>[ 3 41]] | [[15 40]<br />[ 3 44]] |
| Anomaly Detection Accuracy | 48.31%                 | 57.84%                 |



### Isolation Forest

```python
iforest_classifier = IsolationForest()

iforest_parameters = {'n_estimators': list(range(10, 200, 50)), 
              'max_samples': list(range(20, 120, 20)), 
              'contamination': [0.1, 0.2], 
              'max_features': [5,15, 20], 
              'bootstrap': [True, False], 
              }

elapsed_time_iforest = []

start_time = datetime.now()
iforest_grid_search = GridSearchCV(iforest_classifier, iforest_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_iforest_classifier = iforest_grid_search.fit(x_train_only)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())
```



```python
# y_pred = xgb_classifier.predict(x_test)
start_time = datetime.now()
y_pred_c = best_iforest_classifier.predict(x_test)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())


acc_iforest = accuracy_score(y_test_c, y_pred_c)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_c))
print("best parameters ", iforest_grid_search.best_params_)
print('Accuracy ', acc_iforest)
print('elapsed time ', elapsed_time_iforest)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•Šë‹¤. ğŸ”¥)

|                            | Diabetes               | Boston                 |
| -------------------------- | ---------------------- | ---------------------- |
| Confusion Matrix           | [[ 8 37] <br/>[ 2 42]] | [[25 30]<br />[ 8 39]] |
| Anomaly Detection Accuracy | 56.17%                 | 62.74%                 |



### Auto-Encoder for Anomaly Detection

```python
class BasicClassification(nn.Module):
    def __init__(self) -> None:
        super(BasicClassification, self).__init__()

        self.layer_1 = nn.Linear(NUM_INPUT, NUM_1ST_HIDDEN)
        self.layer_2 = nn.Linear(NUM_1ST_HIDDEN, NUM_2ND_HIDDEN)
        self.layer_3 = nn.Linear(NUM_2ND_HIDDEN, NUM_1ST_HIDDEN)
        self.layer_4 = nn.Linear(NUM_1ST_HIDDEN, NUM_INPUT)

        self.actvation_1 = nn.SELU()
        self.actvation_2 = nn.SELU()
        self.actvation_3 = nn.SELU()
    
    def forward(self, inputs):
        x = self.actvation_1(self.layer_1(inputs))
        x = self.actvation_2(self.layer_2(x))
        x = self.actvation_3(self.layer_3(x))
        x = self.layer_4(x)

        return x
        
```



```python
result_reconstruct = abs(x_test - output_num).sum(axis=1)

result_class = result_reconstruct.copy()
result_class[result_reconstruct > THRESHOLD_FOR_RECONSTRUCTION] = -1
result_class[result_reconstruct <=THRESHOLD_FOR_RECONSTRUCTION] = 1

# result_class
acc_ae = accuracy_score(y_test_c, result_class)

print('Confusion Matrix\n', confusion_matrix(y_test_c, result_class))
print('Accuracy ', acc_ae)
```





ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•Šë‹¤. ğŸ”¥)

|                            | Diabetes                | Boston                 |
| -------------------------- | ----------------------- | ---------------------- |
| Confusion Matrix           | [[17 28] <br />[ 7 37]] | [[33 22]<br />[15 32]] |
| Anomaly Detection Accuracy | 60.67%                  | 63.72%                 |



### Mixture Of Gaussian

```python
gmm_classifier = GaussianMixture()

gmm_parameters ={'n_components' : [1, 2, 3,4,5,6, 7] , 'max_iter': [int(1e2), int(1e3), int(1e6)]}

elapsed_time_gmm= []

start_time = datetime.now()
gmm_grid_search = GridSearchCV(gmm_classifier, gmm_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_gmm_classifier = gmm_grid_search.fit(x_train_only)
elapsed_time_gmm.append((datetime.now()-start_time).total_seconds())

```



```python
start_time = datetime.now()
y_pred_c = best_gmm_classifier.predict(x_test)
elapsed_time_gmm.append((datetime.now()-start_time).total_seconds())


densities = best_gmm_classifier.score_samples(x_test)
density_threshold = np.percentile(densities, THRESHOLD_FOR_DENSITY)
anomalies = np.argwhere(densities < density_threshold)
print(len(anomalies))

real_anomaly = np.argwhere(y_test_c == -1)


y_pred_anomalies = y_test_c.copy()
y_pred_anomalies[densities < density_threshold] = -1
y_pred_anomalies[densities >= density_threshold] = 1

acc_gmm = accuracy_score(y_test_c, y_pred_anomalies)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_anomalies))
print("best parameters ", best_gmm_classifier.best_params_)
print('Accuracy ', acc_gmm)
print('elapsed time ', elapsed_time_gmm)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•Šë‹¤. ğŸ”¥)

|                            | Diabetes               | Boston                 |
| -------------------------- | ---------------------- | ---------------------- |
| Confusion Matrix           | [[24 21]<br />[14 30]] | [[31 24]<br />[13 34]] |
| Anomaly Detection Accuracy | 60.67%                 | 63.72%                 |





## 4. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : ì •í™•ë„ %
- Datasetì€ Testset 20%, Training 72%, Validation 8%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨

|      | Algorithm                                | Diabetes   | Boston     |
| ---- | ---------------------------------------- | ---------- | ---------- |
| 1    | SVR                                      | **75.28%** | **88.23%** |
| 2    | One-Class SVM                            | 48.31%     | 57.84%     |
| 3    | Isolation Forest                         | 56.17%     | 62.74%     |
| 4    | Auto-Encoder<br /> for Anomaly Detection | 60.67%     | 63.72%     |
| 5    | Mixture Of Gaussian                      | 60.67%     | 63.72%     |





# Tutorial_Classification_2_AnomalyDetection

ìœ„ì—ì„œ ìš°ë¦¬ëŠ” SVMì— ëŒ€í•´ì„œ ìƒì„¸íˆ ì•Œì•„ë³´ì•˜ìœ¼ë‹ˆ, ê³¼ì—° SVMì´ í˜„ì¬ì—ë„ Tabular Dataì—ì„œ ì ì ˆí•œ ì„ íƒì¸ì§€ ë¹„êµë¥¼ í•´ë³´ì. ì•„ë˜ì˜ Tutorial Linkë¥¼ í†µí•´ Notebookìœ¼ë¡œ ê° Datasetì— ë”°ë¥¸ Algorithmì˜ ì†ë„ì™€ ì„±ëŠ¥ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤.





## 1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/2_kernel_based_learning/Tutorials/tutorial_svm_comparison.ipynb)



## 2. Setting

### Datasets

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì´ 2ê°œì˜ ìœ ëª…í•œ Tabular í˜•íƒœì˜ Regression Datasetì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 

|      | Datasets                      | Description        | Num Instances | Num Inputs (Xs) | Num Outputs (Ys) |
| ---- | ----------------------------- | ------------------ | ------------- | --------------- | ---------------- |
| 1    | Diabetes (Classification)     | ë‹¹ë‡¨ë³‘ í™˜ì ë°ì´í„° | 768           | 8               | 1 (0, 1)         |
| 2    | Breast Cancer(Classification) |                    | 569           | 30              | 1 (0, 1)         |
| 3    | Digits (Classification)       |                    | 1797          | 64              | 1 (0 ~ 9)        |

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê²Œ ë©ë‹ˆë‹¤.

```python
if dataset_name == 'diabetes':
    df = pd.read_csv('diabetes.csv')
    X = df.iloc[:,:-1].values   
    y = df.iloc[:,-1].values    

elif dataset_name == 'breast_cancer':
    breast_cancer = datasets.load_breast_cancer()
    X = breast_cancer.data
    y = breast_cancer.target

elif dataset_name == 'digits':
    digits = datasets.load_digits()
    X = digits.data
    y = digits.target

else:
    pass
```

ê° Datasetì€ Classification Targetì´ë¯€ë¡œ, ê° Datasetì„ Anomalyì— ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ ì‚¬ìš©ë˜ëŠ” ê° ì–‘ë¶ˆ Classì˜ Labelì€ ì•„ë˜ì™€ ê°™ë‹¤. Binary Classê°€ ì•„ë‹Œ Multi-Target Classificationì˜ ê²½ìš°, í•˜ë‚˜ì˜ Labelì„ ë¶ˆëŸ‰ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ, ìì—°ìŠ¤ëŸ½ê²Œ Imbalanced Classification Problemì´ ëœë‹¤.

- Diabetes : 1 (ì–‘ì„±)
- Breast Cancer : 1 (ì–‘ì„±)
- Digits : 5 (ìˆ«ì 5)





### Algorithms

ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ Regression ì•Œê³ ë¦¬ì¦˜ê³¼ Anomaly Detectionì„ ì„œë¡œ ë¹„êµí•©ë‹ˆë‹¤.

- Regerssion 
  - SVRì„ ì‚¬ìš©í•˜ì—¬ Regression Taskì—ì„œ Regression Algorithmì„ ì‚¬ìš©í•˜ê³  ì˜ˆì¸¡í•œ ê°’ì„ íŠ¹ì • Thresholdë¡œ Classificationí•˜ì—¬ ì–‘ë¶ˆì„ íŒì •í•˜ëŠ”ë° ì‚¬ìš©í•©ë‹ˆë‹¤.
- Anomaly Detection
  - 4ê°€ì§€ì˜ ì•Œê³ ë¦¬ì¦˜(One-Class SVM, Isolation Forest, Autoencoder Anomaly Detection, Mixture Of Gaussian)ì„ ì‚¬ìš©í•˜ì—¬, ë°ì´í„°ë¥¼ ì–‘ë¶ˆë¡œ Binary Classificationë¬¸ì œë¡œ ì „ì²˜ë¦¬ í›„, ì–‘í’ˆ ë°ì´í„°ë§Œì„ í•™ìŠµí•˜ì—¬ Anomalyë¥¼ íƒì§€í•œë‹¤.

|      | Algorithm           | Target            | Description                               |
| ---- | ------------------- | ----------------- | ----------------------------------------- |
| 1    | Linear SVR          | Regression        | ì„ í˜• SVR                                  |
| 2    | Kernel SVR          | Regression        | ì„ í˜• SVR + Kernel Trick(using rbf kernel) |
| 3    | One-Class SVM       | Anomaly Detection |                                           |
| 4    | Isolation Forest    | Anomaly Detection |                                           |
| 5    | Autoencoder AD      | Anomaly Detection |                                           |
| 6    | Mixture of Gaussian | Anomaly Detection |                                           |



## 3. Usage Code

### SVM

í•´ë‹¹ Datasetì—ì„œ ì„±ëŠ¥ì´ ì¢‹ì€ SVMì„ ì‚¬ìš©í•˜ì—¬, Classification Taskë¥¼ ì˜ˆì¸¡í•œë‹¤. ì˜ˆì¸¡ëœ ê²°ê³¼ëŠ” ìœ„ì˜ Datasetì „ì²˜ë¦¬ë¥¼ í†µí•´ ì–‘í’ˆ/ë¶ˆëŸ‰ì˜ 2-Class Classificationì„ ìˆ˜í–‰í•œë‹¤. Linear SVMê³¼ RBF SVMì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchí•˜ì—¬ ëª¨ë¸ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'C': [1.0, 2.0, 3.0, 10.]},
    {'kernel': ['rbf'], 'C': [1.0, 2.0, 3.0, 5.0, 10.],
    'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},
]

elapsed_time_kernel_svm = []

svm_classifier = SVC(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svm_classifier, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svc_classifier = grid_search.fit(x_train, y_train_a)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())
```



ì•„ë˜ì™€ ê°™ì´ ì˜ˆì¸¡í•œ ê°’ì„ ìœ„ì—ì„œ ì„¤ì •í•œ thresholdê°’ìœ¼ë¡œ ì–‘ë¶ˆ(ì–‘í’ˆ +1, ë¶ˆëŸ‰ -1) Labelingì„ í•´ ì¤€ë‹¤. ì´ë¥¼ í†µí•´ì„œ Answer Yê°’ì˜ Classificationëœ ê°’ ê³¼ì˜ ë¹„êµë¥¼ í†µí•´ Accuracyë¥¼ ê³„ì‚°í•œë‹¤.

```python
start_time = datetime.now()
y_pred = best_svc_classifier.predict(x_test)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())
acc_svm_kernel = accuracy_score(y_test_a, y_pred)

print('Confusion Matrix\n', confusion_matrix(y_test_a, y_pred))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svm_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svm)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

|                         | Diabetes               | Breast Cancer          | Digits                      |
| ----------------------- | ---------------------- | ---------------------- | --------------------------- |
| Confusion Matrix        | [[28 28]<br />[10 88]] | [[66  1]<br />[ 1 46]] | [[ 49   0] <br />[  0 311]] |
| Classification Accuracy | 75.32%                 | 98.24%                 | 100%                        |



### One-Class SVM

One-Class SVMì€ Scikit-Learnì— êµ¬í˜„ëœ Nu-SVMì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì•„ë˜ì™€ê°™ì€ param_gridì— ìˆëŠ” Hyper-parameterë¥¼ Grid Searchingí•˜ì—¬ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ìœ¼ë©° X_Trainê°’ ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.

```python
param_grid = [
    {'kernel': ['linear'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7]},
    {'kernel': ['rbf'], 'nu': [0.05, 0.1, 0.25, 0,5, 0.7],
    'gamma': [0.01, 0.03, 0.1, 0.3, 0.05, 1.0]},
]

elapsed_time_kernel_svm = []

svm_classifier = OneClassSVM(kernel='rbf')
# svm_classifier = svm_classifier.fit(x_train, y_train)

start_time = datetime.now()
grid_search = GridSearchCV(svm_classifier, param_grid, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_svm_classifier = grid_search.fit(x_train_only)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())


```



Inference ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•˜ì˜€ë‹¤.

```python
start_time = datetime.now()
y_pred = best_svm_classifier.predict(x_test)
elapsed_time_kernel_svm.append((datetime.now()-start_time).total_seconds())

acc_svm_kernel = accuracy_score(y_test_c, y_pred)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred))
print('Best Prameters ', grid_search.best_params_)
print('Accuracy ', acc_svm_kernel)
print('Elapsed Time(train, test) ', elapsed_time_kernel_svm)
# Isolation Forest 
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•Šë‹¤. ğŸ”¥)

|                            | Diabetes                | Breast Cancer           | Digits                      |
| -------------------------- | ----------------------- | ----------------------- | --------------------------- |
| Confusion Matrix           | [[ 1 55]<br /> [ 7 91]] | [[60  7]<br /> [ 0 47]] | [[  3  46]<br /> [ 27 284]] |
| Anomaly Detection Accuracy | 59.74%                  | 93.85%                  | 79.72%                      |



### Isolation Forest

```python
iforest_classifier = IsolationForest()

iforest_parameters = {'n_estimators': list(range(10, 200, 50)), 
              'max_samples': list(range(20, 120, 20)), 
              'contamination': [0.1, 0.2], 
              'max_features': [5,15, 20], 
              'bootstrap': [True, False], 
              }

elapsed_time_iforest = []

start_time = datetime.now()
iforest_grid_search = GridSearchCV(iforest_classifier, iforest_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_iforest_classifier = iforest_grid_search.fit(x_train_only)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())
```



```python
# y_pred = xgb_classifier.predict(x_test)
start_time = datetime.now()
y_pred_c = best_iforest_classifier.predict(x_test)
elapsed_time_iforest.append((datetime.now()-start_time).total_seconds())


acc_iforest = accuracy_score(y_test_c, y_pred_c)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_c))
print("best parameters ", iforest_grid_search.best_params_)
print('Accuracy ', acc_iforest)
print('elapsed time ', elapsed_time_iforest)
```





ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•Šë‹¤. ğŸ”¥)

|                            | Diabetes                | Breast Cancer           | Digits                      |
| -------------------------- | ----------------------- | ----------------------- | --------------------------- |
| Confusion Matrix           | [[23 33]<br /> [11 87]] | [[49 18]<br /> [ 5 42]] | [[ 16  33]<br /> [ 41 270]] |
| Anomaly Detection Accuracy | 71.42%                  | 79.82%                  | 79.44%                      |





### Auto-Encoder for Anomaly Detection

```python
class BasicClassification(nn.Module):
    def __init__(self) -> None:
        super(BasicClassification, self).__init__()

        self.layer_1 = nn.Linear(NUM_INPUT, NUM_1ST_HIDDEN)
        self.layer_2 = nn.Linear(NUM_1ST_HIDDEN, NUM_2ND_HIDDEN)
        self.layer_3 = nn.Linear(NUM_2ND_HIDDEN, NUM_1ST_HIDDEN)
        self.layer_4 = nn.Linear(NUM_1ST_HIDDEN, NUM_INPUT)

        self.actvation_1 = nn.SELU()
        self.actvation_2 = nn.SELU()
        self.actvation_3 = nn.SELU()
    
    def forward(self, inputs):
        x = self.actvation_1(self.layer_1(inputs))
        x = self.actvation_2(self.layer_2(x))
        x = self.actvation_3(self.layer_3(x))
        x = self.layer_4(x)

        return x
        
```



```python
result_reconstruct = abs(x_test - output_num).sum(axis=1)
result_class = result_reconstruct.copy()
result_class[result_reconstruct > THRESHOLD_FOR_AUTOENCODER] = -1
result_class[result_reconstruct <= THRESHOLD_FOR_AUTOENCODER] = 1
acc_ae = accuracy_score(y_test_a, result_class)

print('Confusion Matrix\n', confusion_matrix(y_test_a, result_class))
print('Accuracy ', acc_ae)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•Šë‹¤. ğŸ”¥)

|                            | Diabetes                | Breast Cancer           | Digits                      |
| -------------------------- | ----------------------- | ----------------------- | --------------------------- |
| Confusion Matrix           | [[26 30]<br /> [20 78]] | [[45 22]<br /> [19 28]] | [[ 35  14]<br /> [ 10 301]] |
| Anomaly Detection Accuracy | 67.53%                  | 64.03%                  | 93.33%                      |





### Mixture Of Gaussian

```python
gmm_classifier = GaussianMixture()

gmm_parameters ={'n_components' : [1, 2, 3,4,5,6, 7] , 'max_iter': [int(1e2), int(1e3), int(1e6)]}

elapsed_time_gmm= []

start_time = datetime.now()
gmm_grid_search = GridSearchCV(gmm_classifier, gmm_parameters, cv=7, scoring="neg_mean_squared_error", verbose=2)
best_gmm_classifier = gmm_grid_search.fit(x_train_only)
elapsed_time_gmm.append((datetime.now()-start_time).total_seconds())

```



```python
start_time = datetime.now()
y_pred_c = best_gmm_classifier.predict(x_test)
elapsed_time_gmm.append((datetime.now()-start_time).total_seconds())


densities = best_gmm_classifier.score_samples(x_test)
density_threshold = np.percentile(densities, THRESHOLD_FOR_DENSITY)
anomalies = np.argwhere(densities < density_threshold)
print(len(anomalies))

real_anomaly = np.argwhere(y_test_c == -1)


y_pred_anomalies = y_test_c.copy()
y_pred_anomalies[densities < density_threshold] = -1
y_pred_anomalies[densities >= density_threshold] = 1

acc_gmm = accuracy_score(y_test_c, y_pred_anomalies)

print('Confusion Matrix\n', confusion_matrix(y_test_c, y_pred_anomalies))
print("best parameters ", best_gmm_classifier.best_params_)
print('Accuracy ', acc_gmm)
print('elapsed time ', elapsed_time_gmm)
```



ê·¸ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. (ë§¤ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ì•Šë‹¤. ğŸ”¥)

|                            | Diabetes                | Breast Cancer           | Digits                      |
| -------------------------- | ----------------------- | ----------------------- | --------------------------- |
| Confusion Matrix           | [[32 24]<br /> [24 74]] | [[56 11]<br /> [24 23]] | [[ 41   8]<br /> [ 42 269]] |
| Anomaly Detection Accuracy | 68.83%                  | 69.29%                  | 86.11%                      |







## 4. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : ì •í™•ë„ %
- Datasetì€ Testset 20%, Training 72%, Validation 8%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨

|      | Algorithm                                | Diabetes   | Breast Cancer | Digits   |
| ---- | ---------------------------------------- | ---------- | ------------- | -------- |
| 1    | SVM                                      | **75.32%** | **98.24%**    | **100%** |
| 2    | One-Class SVM                            | 59.74%     | 93.85%        | 79.72%   |
| 3    | Isolation Forest                         | 71.42%     | 79.82%        | 79.44%   |
| 4    | Auto-Encoder<br /> for Anomaly Detection | 67.53%     | 64.03%        | 93.33%   |
| 5    | Mixture Of Gaussian                      | 68.83%     | 69.29%        | 86.11%   |









# Final Insights

## 1. Training Time ê´€ì 

- **SVMì€ íƒ€ ë°©ì‹ ëŒ€ë¹„ ì „ë°˜ì ìœ¼ë¡œ ì¥ì ì„ ê°–ê³  ìˆìŒ** âœ…

- SVMì€ ì „ë°˜ì ìœ¼ë¡œ Training Timeì´ ë§¤ìš° ìš°ìˆ˜í•˜ë©°, ê°™ì€ ì‹œê°„ ëŒ€ë¹„ Boosting, Bagging, NN, RVM ê³„ì—´ë“¤ ëŒ€ë¹„ ë‹¤ì–‘í•œ Hyper Parameterë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆìŒ. SVMë³´ë‹¤ ë¹ ë¥¸ ë°©ì‹ì€ í¬ê²Œ ì°¨ì´ëŠ” ë‚˜ì§€ ì•Šìœ¼ë‚˜ Diabetes Datasetì—ì„œì˜ LightGBMì •ë„ ë°–ì— ì¡´ì¬í•˜ì§€ ì•Šì•˜ìŒ (ê·¸ëŸ¬ë‚˜ Parallel Threadingì„ ì“´ë‹¤ë©´ Boosting ê³„ì—´ì´ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ)
- ë¬¼ë¡  Training Time ë“±ì˜ ì†ë„ëŠ” ì–´ë– í•œ ì–¸ì–´ë¡œ ëœ êµ¬í˜„ì²´(ex. C, Rust ë“±)ì¸ì§€ì— ë”°ë¼ ë‹¤ë¥´ê³ , Coding Trickì„ ì¼ëŠëƒì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ê·¸ë¦¬ê³  SVMì€ ë§¤ìš° Optimizationëœ Codeë¡œ Scikit-Learnì— êµ¬í˜„ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ì„ ê²ƒ ê°™ìŒ. ê·¸ëŸ¬ë‚˜ XGBoostë‚˜ LightGBMê°™ì€ ê²½ìš°ë„ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ì‹(ex. Cache Hit Optimization ë“±)ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ì™„ì „ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì´ ë¶ˆë¦¬í•˜ë‹¤ê³  ë³´ê¸°ëŠ” í˜ë“¬.
- ê·¸ëŸ¬ë‚˜ XGBoost ê°™ì€ ê²½ìš° ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ë°©ì‹ ìì²´ê°€ Thread Processingì„ ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, single threadê¸°ë°˜ì—ì„œëŠ” ì†ë„ì  ì´ë“ì„ ì–»ê¸°ê°€ í˜ë“œë¦¬ë¼ ë´„
- ë”°ë¼ì„œ ARM Cortex Aê°€ ì•„ë‹Œ, ê·¸ ì™¸ì˜ Real-Time Embedded Systemì—ì„œì˜ Trainingì—ì„œëŠ” SVMì´ íƒ€ ì•Œê³ ë¦¬ì¦˜ì„ ì••ë„í•˜ëŠ” ì†ë„ì ì¸ ì´ìµì„ ì–»ì„ ê²ƒì´ë¼ ìƒê°ë¨
- ê·¸ë¦¬ê³  Tabular Dataì—ì„œ Hyper Parmeter Searchingì„ ê°€ë¯¸í•œ Baseline ëª¨ë¸ì„ ì°¾ì•„ë‚´ëŠ”ë° SVMì´ ë§¤ìš° ì í•©í•˜ë¦¬ë¼ ìƒê°ë¨



## 2. Inference Time ê´€ì 

- **SVMì€ íƒ€ ë°©ì‹ ëŒ€ë¹„ Big Datasetì—ì„œëŠ” ë‹¨ì ì„ ê°–ê³  ìˆìŒ** âŒ
- **ê·¸ëŸ¬ë‚˜ SVMì€ íƒ€ ë°©ì‹ ëŒ€ë¹„ Small Datasetì—ì„œëŠ” ë‚˜ì˜ì§€ ì•ŠëŠ” ì†ë„ë¥¼ ê°–ê³  ìˆìŒ(Single Thread Embedded Real-Time Systemì— ì í•©)** âœ…

- DiabetesëŠ” 700ê°œ ì •ë„ì˜ Datasetì¸ë°, ì´ì •ë„ í¬ê¸° ì´í›„ë¶€í„° Inferenceì—ì„œëŠ” SVMì´ ë‹¤ë¥¸ ë°©ì‹ ëŒ€ë¹„ ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì´ì§€ëŠ” ì•ŠìŒ. ë¬¼ë¡  GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” NNê³„ì—´ë³´ë‹¤ëŠ” ë¹ ë¥¼ ìˆ˜ ìˆê² ìœ¼ë‚˜, Boosting, Random Forest, RVM ë“±ì— ëª¨ë‘ ì†ë„ê°€ ë°€ë¦¼

- ê·¸ëŸ¬ë‚˜ Small Datasetì„ í•œë²ˆì— Inferenceí•  ë•Œì—ëŠ” ì¥ì ì„ ê°–ì¶”ê³  ìˆìŒ. ì´ë¥¼ ë³´ì•˜ì„ë•Œ Single Threadì˜ Embedded Systemì—ì„œ Real-Time Inferenceë‚˜ FPGA, ASICìœ¼ë¡œ êµ¬í˜„í–ˆì„ë•Œì˜ ì†ë„ëŠ” SVMì´ ê°€ì¥ ë¹ ë¥´ë¦¬ë¼ ìƒê°ë¨ (íŠ¹íˆ Inference Timeì— Single Instanceë¥¼ ì²˜ë¦¬í•˜ëŠ” ì†ë„ê°€ ê°€ì¥ ë¹ ë¥´ë¦¬ë¼ ì˜ˆìƒ)

- ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„±ìƒ Boostingê³„ì—´ì—ì„œ ì •ë ¬ ë“±ì´ í•„ìš”í•˜ì—¬ ì´ë•Œ Boot-upë˜ëŠ” ì†ë„ê°€ SVMëŒ€ë¹„ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •ë¨

  



## 3. Accuracy ê´€ì  

- **SVMì´ íƒ€ ë°©ì‹ ëŒ€ë¹„ Accuracyê°€ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ì¢‹ì€ ê²½ìš°ë„ ìˆìŒ** âœ…
- Accuracyë„ SVMì´ ì „ì²´ì ìœ¼ë¡œ ëª¨ë“  Datasetì—ì„œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ëŒ€ë¹„ ê°€ì¥ ì¢‹ê±°ë‚˜(Diabetes, Digits), 2~3ìœ„ ìˆ˜ì¤€(Irs, Breast Cancer)ì˜ Accuracyë¥¼ ë‚˜íƒ€ëƒ„. 
- íŠ¹íˆë‚˜ Linear SVM (Soft Margin)ì´ ë‹¤ë¥¸ ëª¨ë¸ë“¤ ëŒ€ë¹„ í¬ê²Œ ë–¨ì–´ì§€ì§€ ì•ŠëŠ” Accuracyë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨, Kernelì´ í•„ìš”í•œ íŠ¹ì´í•œ Caseì˜ ì„ì˜ë¡œ ìƒì„±ëœ Datasetì´ ì•„ë‹Œí•œ, ë§¤ìš° ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ Classificationì„ í•˜ëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ì—ˆë‹¤ê³  ë³´ì—¬ì§. íŠ¹íˆë‚˜ Dimensionì´ ì»¤ì§€ë©´ì„œ Linear Modelë¡œ ë¶„ë¥˜ë˜ëŠ” Hyper Planeì„ ì°¾ê¸°ê°€ ë” ì‰½ì§€ ì•Šì„ê¹Œ ìƒê°ë¨.
- ë¬¼ë¡  Datasetì´ í¬ì§€ ì•Šê³  ë‹¨ìˆœí•˜ë©°, ì „ë°˜ì ìœ¼ë¡œ Accuracyê°€ ìœ ì‚¬í•˜ê²Œ ë†’ìœ¼ë¯€ë¡œ(ë¬¼ë¡  Use-Caseì— ë”°ë¼ì„œ 0.1% Accuracyë„ ë§¤ìš° í¬ë‹¤ê³  ë³¼ ìˆ˜ ìˆê¸´ í•˜ì§€ë§Œ) ì´ ê²°ê³¼ë¡œë§Œ ê°€ì§€ê³  SVMì´ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ í™•ì‹¤íˆ ë›°ì–´ë‚˜ë‹¤ê³  ë³¼ìˆ˜ëŠ” ì—†ìŒ.
- ê·¸ëŸ¬ë‚˜ í™•ì‹¤í•œê±´, **Training Timeê³¼ Inference TimeëŒ€ë¹„, SVMì´ ë‹¤ë¥¸ ìµœì‹ ì˜ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë–¨ì–´ì§„ë‹¤ê³  ë³´ê¸° ì–´ë ¤ìš°ë©°, ì˜¤íˆë ¤ ë” ì¢‹ì€ ê²½ìš° ìˆë‹¤ê³  ë§í•  ìˆ˜ ìˆìŒ**.  ë”°ë¼ì„œ Silver Bulletì€ ì—†ìœ¼ë¯€ë¡œ, SVMì•Œê³ ë¦¬ì¦˜ì„ ì‹¤ë¬´ì—ì„œ ê¼­ ê²€ì¦ í•´ ë³¼ í•„ìš”ëŠ” ìˆìŒ



## 4. ê·¸ ì™¸ì˜ ìƒê°ë“¤

- ê·¸ ì™¸ Tabnetê°™ì€ ê²½ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, Datasetì— ì ì€ ê²½ìš° íŠ¹íˆë‚˜ ê·¸ëŸ¬í•˜ë©°(Iris, Breast Cancer), Tabnetì€ category encodingì— ìœ ë¦¬í•œ ì¸¡ë©´ì´ ìˆë‹¤ê³  ë³´ì—¬, ì¢€ ë” ë³µì¡í•œ columnì„ ê°€ì§„ dataset, ì¢€ ë” í° datasetì—ì„œ í™œìš©ë˜ë©´ ì¢‹ì„ ê²ƒì´ë¼ ìƒê°ë¨
- ê·¸ë¦¬ê³  Basic ANNì€ Dropoutê³¼ SELUë“±ì˜ í™œìš©ìœ¼ë¡œ Tabularì—ì„œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ëª»ì§€ì•Šì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•¨
- ë˜í•œ RVMì€ íƒ€ ì•Œê³ ë¦¬ì¦˜ ëŒ€ë¹„ Training Timeì´ ìƒëŒ€ì ìœ¼ë¡œ ë§¤ìš° ê¸¸ì—ˆì§€ë§Œ, Inference Timeì€ íƒ€ ì•Œê³ ë¦¬ì¦˜ëŒ€ë¹„ ë§¤ìš° ë¹ ë¥¸ í¸ì„. ë˜í•œ ì„±ëŠ¥ë„ DIgits Dataset ì™¸ì—ëŠ” íƒ€ ì•Œê³ ë¦¬ì¦˜ ëŒ€ë¹„ ìƒëŒ€ì ìœ¼ë¡œ  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŒ. 
- ê·¸ëŸ¬ë‚˜ RVMì€ Digits ë°ì´í„°ê°€ 1700ê°œ ê°€ëŸ‰ ë°–ì— ì•ˆë˜ëŠ” ê²½ìš°ì—ë„, 35ë¶„ì´ë‚˜ ê±¸ë¦¬ëŠ” ì•„ì£¼ ê¸´ Trainingì‹œê°„ì„ ê°–ìŒ. ë”°ë¼ì„œ RVMì€ Small Datasetì—ì„œë§Œ ê¼­ ê³ ë ¤í•´ì•¼í•  ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìƒê°ë¨(Hyper Parameter Tunningë„ ìë™ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë©°, Outputì— ëŒ€í•œ Uncertaintyë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ)
- RVMì€ Gaussian Processì™€ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°ê°€ ì»¤ì§ì— ë”°ë¼ ë§¤ìš° ëŠë ¤ì§€ê³  O(N^3), Featureì˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ Accuracyê°€ ë–¨ì–´ì§€ëŠ” ê²½í–¥ì„ ë³´ì´ì§€ ì•Šë‚˜ ìƒê°ì´ ë“¬
- RVM Trainingì˜ ë¹ ë¥¸ êµ¬í˜„ Tipping, M., & Faul, A. (2003). Fast marginal likelihood maximization for sparse Bayesian modelsë„ ì¡´ì¬í•˜ë¯€ë¡œ, í•´ë‹¹ êµ¬í˜„ì„ ì‚¬ìš©í•œë‹¤ë©´ ì´ë²ˆ Tutorialì˜ êµ¬í˜„ë³´ë‹¤ëŠ” ë” ê¸ì •ì ì¸ ëŠë‚Œì„ ë°›ì•˜ìœ¼ë¦¬ë¼ ìƒê°ë¨
- Random ForestëŠ” ì „ë°˜ì ìœ¼ë¡œ Training Timeì´ë‚˜ Inference Timeì—ì„œ í° ì¥ì ì€ ì—†ì—ˆìœ¼ë©°(ë¹ ë¥¸ í¸ì´ ì•„ë‹ˆì—ˆìŒ), Accuracy ì„±ëŠ¥ ì—­ì‹œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ë³´ë‹¤ ë”±íˆ ë›°ì–´ë‚˜ë‹¤ê³  ë³´ì´ì§€ëŠ” ì•ŠìŒ(Breaset Cancerì œì™¸ Boostingë³´ë‹¤ ì „ë°˜ì ìœ¼ë¡œ ë–¨ì–´ì§). í–¥í›„ì—ëŠ” Random Forestë³´ë‹¤ëŠ”, SVM, Boosting, RVMì„ ì¢€ ë” ê³ ë ¤í•˜ì§€ ì•Šì„ê¹Œ ìƒê°ì´ ë“¬ (ë¬¼ë¡  Hyper Parameterë¥¼ ì¢€ ë” í…ŒìŠ¤íŠ¸ í•˜ë©´ ë‹¤ë¥¼ ìˆ˜ëŠ” ìˆì„ ë“¯)
- Boosting ê³„ì—´ì—ì„œ ë¹„êµí•˜ìë©´, ì†ë„ ì¸¡ë©´ì—ì„œëŠ” LightGBMì´ ë¹ ë¥¸ í¸ì´ì§€ë§Œ Accuracy ì¸¡ë©´ì—ì„œëŠ” XGBoostë‚˜ CatBoostê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ë©° ì†ë„ë„ í¬ê²Œ ëŠë¦¬ì§€ëŠ” ì•ŠìŒ. LightGBMë³´ë‹¤ XGBoostë‚˜ CatBootë¥¼ ì¢€ ë” ê³ ë ¤í•˜ëŠ”ê²Œ ì¢‹ì§€ ì•Šì„ê¹Œ ìƒê°ë¨



## 5. ê²°ë¡ 

- ê²°ë¡ ì ìœ¼ë¡œëŠ” SVMì•Œê³ ë¦¬ì¦˜ì€ ì§€ê¸ˆë„ ì“¸ë§Œí•œ ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  ë§í•  ìˆ˜ ìˆìŒ.

