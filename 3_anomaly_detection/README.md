# Anomaly Detectionì˜ í•œê³„ë¥¼ ì•Œì•„ë³´ì.



## The limitation of anomaly detection algorithm



ğŸ”¥ì´ë²ˆ Tutorialì—ì„œëŠ” Anomaly Detectionì´ ì‹¤ì œ Practicalí•œ ìƒí™©ì—ì„œ ì–´ë– í•œ ì œì•½ì ì´ ìˆëŠ”ì§€ íŒŒì•…í•´ ë³´ê³ , ì–´ë– í•  ë•Œ ì“°ë©´ ì•ˆë˜ëŠ”ì§€ í™•ì¸ í•´ ë³´ê³ ì í•œë‹¤. í™•ì¸í•´ë³´ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” 2ê°€ì§€ ì´ë‹¤.



### 1. Anomaly Detection for "From Regression to Binary Classification Task"

- Manufacturing ê³µì • ë“±ì˜ DataëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Targetê°’ì´ Continuousí•œ ê²½ìš°ê°€ ë§ì´ ìˆë‹¤. ì´ëŸ° ê²½ìš°, íŠ¹ì • Threshold ì´ìƒì˜ ê°’ì€ 'ì´ìƒì¹˜(Anomaly)'ë¡œ, Threshold ì´í•˜ì˜ ê°’ì€ 'ì •ìƒì¹˜(Normal)'ë¡œ Target Dataë¥¼ Binary Categorizationì„ í•˜ì—¬, Anomaly Detectionì´ë‚˜ Classificationìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ë ¤ëŠ” ì‹œë„ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ìƒê°í•œë‹¤.

- ë”°ë¼ì„œ ì´ë ‡ê²Œ ê·¼ë³¸ì ìœ¼ë¡œ Regression Taskì¸ ê²ƒë“¤ì„ Thresholdingí•˜ì˜€ì„ ë•Œ, ê³¼ì—° Anomaly Detectionê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ ì˜ ë™ì‘í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰ í•´ ë³´ì•˜ë‹¤. (íŠ¹íˆ í˜„ì—…ì˜ Manufacturing ê³µì •ì—ì„œëŠ” ëŒ€ë¶€ë¶„ì´ 'ì •ìƒ'ë°ì´í„°ì´ë©° 'ì´ìƒ'ë°ì´í„°ëŠ” ë§¤ìš° ì ì€ Imbalancedí•œ Dataê°€ ëŒ€ë¶€ë¶„ì˜ Caseì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë²ˆ ì‹¤í—˜ì—ì„œëŠ” Regressionì„ í†µí•œ Imbalancedí•œ ë¶€ë¶„ì„ ì—†ì• ê¸° ìœ„í•˜ì—¬ Normal Classì™€ Abnormal Classë¥¼ 5:5 ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.)

- ì´ë¥¼ í†µí•´ Anomaly Detectionì„ ê·¼ë³¸ì  Regression Taskì— ì¨ë„ ë˜ëŠ”ì§€ ì•„ë‹Œì§€ ê·¸ í•œê³„ë¥¼ ì•Œì•„ë³´ë ¤ê³  í•œë‹¤.

### 2. Anomaly Detection for "From Supervised Classification Task"

- ì¼ë°˜ì ìœ¼ë¡œ Unsupervisedê¸°ë°˜ì˜ Anomaly Detectionë³´ë‹¤ Supervised Classificationì´ ì„±ëŠ¥ì´ ë” ë†’ì€ ê²½ìš°ê°€ ë§ì´ ìˆë‹¤.
- ì‹¤ì œë¡œ ê°„ë‹¨í•œ Taskì—ì„œ Anomaly Detectionì´ Supervised Classificationë³´ë‹¤ ì–´ëŠì •ë„ì˜ ì„±ëŠ¥ì˜ ì°¨ì´ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ë ¤ í•œë‹¤.
- ì´ë¥¼ í†µí•´ Anomaly Detectionì´ Supervised Classification Taskì— ì¨ë„ ë˜ëŠ”ì§€ ì•„ëŠ”ì§€ ê·¸ í•œê³„ë¥¼ ì•Œì•„ë³´ë ¤ í•œë‹¤.





# Table of Contents

- [Background of SVM](#Background-of-SVM)
  
  - [1. Basic Concept](#1-Basic-Concept)
  - [2. About SVM](#2-About-SVM)
  - [3. Linear SVM](#3-Linear-SVM)
  - [4. Kernel SVM](#4-Kernel-SVM)
  
- [Tutorial - Competion for tabular datasets](#Tutorial_Competion-for-tabular-datasets)
  
  - [1. Tutorial Notebook](#1-Tutorial-Notebook)
  - [2. Setting](#2-Setting)
  - [3. Result (Accuracy)](#3-Result_Accuracy)
  - [4. Result (Training Time)](#4-Result_Training-Time)
  - [5. Result (Inference Time)](#5-Result_Inference-Time)
  
- [Final Insights](#Final-Insights)
  
  - [1. Training Time ê´€ì ](#1-Training-Time-ê´€ì )
  - [2. Inference Time ê´€ì ](#2-Inference-Time-ê´€ì )
  - [3. Accuracy ê´€ì ](#3-Accuracy-ê´€ì )
  - [4. ê·¸ ì™¸ì˜ ìƒê°ë“¤](#4-ê·¸-ì™¸ì˜-ìƒê°ë“¤)
  - [5. ê²°ë¡ ](#5-ê²°ë¡ )
  
  







# Background of SVM

## 1. Basic Concept

- - -




## 2. About SVM

- 

## 3. Linear SVM

- - 

## 4. Kernel SVM

- 


- 

# Tutorial_Competion for tabular datasets

ìœ„ì—ì„œ ìš°ë¦¬ëŠ” SVMì— ëŒ€í•´ì„œ ìƒì„¸íˆ ì•Œì•„ë³´ì•˜ìœ¼ë‹ˆ, ê³¼ì—° SVMì´ í˜„ì¬ì—ë„ Tabular Dataì—ì„œ ì ì ˆí•œ ì„ íƒì¸ì§€ ë¹„êµë¥¼ í•´ë³´ì. ì•„ë˜ì˜ Tutorial Linkë¥¼ í†µí•´ Notebookìœ¼ë¡œ ê° Datasetì— ë”°ë¥¸ Algorithmì˜ ì†ë„ì™€ ì„±ëŠ¥ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤.



## 1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/2_kernel_based_learning/Tutorials/tutorial_svm_comparison.ipynb)



## 2. Setting

### Datasets

- ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì´ 4ê°œì˜ ìœ ëª…í•œ Tabular í˜•íƒœì˜ Datasetì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 

|      | Datasets      | Description                                    | Num Instances | Num Inputs (Xs) | Num Classes (Ys) |
| ---- | ------------- | ---------------------------------------------- | ------------- | --------------- | ---------------- |
| 1    | Diabetes      | ë‹¹ë‡¨ë³‘ í™˜ì ë°ì´í„° (0, 1)                      | 768           | 8               | 2                |
| 2    | Digits        | 0~9ê¹Œì§€ì˜ ìˆ«ì Dataset. Mini MNIST (8*8 Image) | 1797          | 64              | 10               |
| 4    | Breast Cancer | ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„° (0, 1)                  | 569           | 30              | 2                |



### Algorithms

- Algorithmì€ ë³¸ ì‹¤í—˜ì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Linear SVMê³¼ Kernel SVMì„ ì‚¬ìš©í•œë‹¤. 2ê°œì˜ Parameterë¥¼ Grid Searchí•¨ (7*6 ì¡°í•©)
- ë¹„ì •í˜• ë°ì´í„°ì— ì••ë„ì ì¸ Artifical Neural Networkê³„ì—´ì¸, Basic ANNê³¼ Google(2019)ì—ì„œ ë§Œë“  Tabularì— íŠ¹í™”ëœ TabNetì„ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ì¶”ê°€í•˜ì˜€ë‹¤. Neural Networkê³„ì—´ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ, ë”°ë¡œ Grid Searchí•˜ì§€ ì•ŠìŒ
- Boosting ê³„ì—´ì¸ XGBoost, LightGBM, CatBoostë¥¼ ë¹„êµêµ°ìœ¼ë¡œ ë‘ì—ˆë‹¤. 3ê°€ì§€ ëª¨ë‘ Kaggleê³¼ ê°™ì€ Competitionì—ì„œ Tabular Datasetì— ëŒ€í•˜ì—¬ ë§¤ìš° ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.(ë”¥ëŸ¬ë‹ ë³´ë‹¤ ì¢‹ì€ ê²½ìš° ë§ìŒ). 2ê°œì˜ Parameterë¥¼ Grid Searchí•¨ (7*6 ì¡°í•©)
- Baggingê³„ì—´ì´ë©° ì‹¤ì œë¡œë„ í˜„ì—…ì—ì„œ ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ê°–ê³  ìˆëŠ” Random Forestë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµí•˜ì˜€ë‹¤. 2ê°œì˜ Parameterë¥¼ Grid Searchí•¨ (7*6 ì¡°í•©)
- SVMì˜ Bayesianë²„ì „ì¸ RVM(Relevance Vector Machine) ë„ ë¹„êµêµ°ìœ¼ë¡œ ì‚¼ìŒ. MSê°€ íŠ¹í—ˆë¥¼ ê°–ê³  ìˆì—ˆìŒ (2019ë…„ íŠ¹í—ˆ ë§Œë£Œ) RVMì€ Hyperparameterê°€ ì ì€ í¸ì´ë©° ìë™ ì¶”ì •ì´ ê°€ëŠ¥í•¨. ê·¸ëŸ¬ë‚˜ Training ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ ë”°ë¡œ Grid Searchí•˜ì§€ ì•ŠìŒ. íŠ¹íˆ Trainingì‹œ O(N^3)ë¼ ì˜¤ë˜ ê±¸ë¦¼. Inference Timeì€ ë§¤ìš° ì§§ìŒ. SVMê³¼ ë‹¬ë¦¬ Global Optimumì„ ë³´ì¥í•˜ì§€ëŠ” ì•Šìœ¼ë‚˜, ì‹¤í—˜ì ìœ¼ë¡œ SVMë³´ë‹¤ Generalizationì— ì¢‹ì€ í¸ì„. RVMì€ Big Datasetì—ì„œ ë§¤ìš° ëŠë¦¬ë‹¤ëŠ” ë‹¨ì ê³¼ í•¨ê»˜, RVMì´ ëœ° ë•Œ ì¯¤, Deep Learningì´ ë– ì„œ ì•ˆíƒ€ê¹ê²Œ ì˜ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠìŒ. Small Datasetì—ì„œëŠ” Gaussian Processì™€ í•¨ê»˜ ë§¤ìš° Accuracyê°€ ë†’ë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŒ

|      | Algorithm                    | Description                                                  |
| ---- | ---------------------------- | ------------------------------------------------------------ |
| 1    | Linear SVM                   | ì„ í˜• SVM                                                     |
| 2    | Kernel SVM                   | ì„ í˜• SVM + Kernel Trick(using rbf kernel)                    |
| 3    | Basic ANN                    | ê¸°ë³¸ì ì¸ Artifical Neural Network. 1ê°œì˜ Intput Layerì™€ 2ê°œì˜ Hidden Layer, 1ê°œì˜ Output Layerë¡œ êµ¬ì„±ë¨. 200 epochs ëŒë¦¼. Dropout ìµœì í™” ë° SELU Activationì„ ì‚¬ìš©ìœ¼ë¡œ Small Tabular Datasetìƒì—ì„œ ê²½í—˜ì ìœ¼ë¡œ ì¼ë¶€ë¶„ ìµœì í™”ëœ ì„¸íŒ…ì„ ê°–ì¶¤ |
| 4    | TabNet (Deep Learning Model) | Transformerë¥¼ ì‚¬ìš©í•œ Tabularì— íŠ¹í™”ëœ ìµœì‹ (2019) ë”¥ëŸ¬ë‹ ëª¨ë¸. Kaggleì—ì„œ Boostingê³„ì—´ì„ ë›°ì–´ë„˜ëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ„. íŠ¹íˆ Dataê°€ ë§ì„ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒì¹˜ê°€ ë†’ìŒ. 100 epochs ëŒë¦¼. Small Datasetì—ì„œëŠ” ì•½í•¨ |
| 5    | XGBoost                      | Boostingê³„ì—´ì˜ ëŒ€í‘œì£¼ì. ì†ë„ë¥¼ ê·¹í•œìœ¼ë¡œ ì¶”êµ¬í•˜ë©°, ë™ì‹œì— Generalization ëŠ¥ë ¥ë„ íƒì›”í•¨. Hyperparameterê°€ ë§¤ìš° ë§ìŒ |
| 6    | LightGBM                     | Boostingê³„ì—´ì˜ ëŒ€í‘œì£¼ì 2. Hyperparameterê°€ ë§¤ìš° ë§ìŒ        |
| 7    | CatBoost                     | Boostingê³„ì—´ì˜ ëŒ€í‘œì£¼ì 3. Hyperparameterê°€ ë§¤ìš° ë§ìŒ        |
| 8    | Random Forest                | Baggingê³„ì—´ì˜ ëŒ€í‘œì£¼ì. ë§ì€ Caseì— ìˆì–´ì„œ SVMë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ„. ê·¸ëŸ¬ë‚˜ SVMë³´ë‹¤ Hyper parameterê°€ ë§ì€ê²Œ ë˜ ë‹¨ì ì„ |
| 9    | Linear RVM                   | Linearë²„ì „ì˜ RVM. Bayesianë°©ì‹ìœ¼ë¡œ ë³€í˜•ëœ ë²„ì „ì˜ SVM.        |
| 10   | Kernel RVM                   | RVMì˜ rbf kernelì„ ì‚¬ìš©í•œ ë²„ì „.                              |





## 3. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : ì •í™•ë„ %
- Datasetì€ Testset 20%, Training 72%, Validation 8%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨

|      | Algorithm                    | Diabetes     | Digits       | Iris         | Breast Cancer |
| ---- | ---------------------------- | ------------ | ------------ | ------------ | ------------- |
| 1    | Linear SVM                   | <u>81.16</u> | 97.77        | 96.66        | 98.24         |
| 2    | Kernel SVM (rbf)             | **83.11**    | **99.16**    | 96.66        | 98.24         |
| 3    | Basic ANN                    | 78.57        | <u>97.78</u> | **100**      | <u>98.25</u>  |
| 4    | TabNet (Deep Learning Model) | 79.22        | 96.94        | 76.66        | 92.98         |
| 5    | XGBoost                      | **83.11**    | 96.11        | 96.66        | 95.61         |
| 6    | LightGBM                     | 77.92        | 96.66        | 96.66        | 93.85         |
| 7    | CatBoost                     | 80.51        | 97.77        | 96.66        | 96.49         |
| 8    | Random Forest                | 75.97        | 96.66        | 96.66        | 97.36         |
| 9    | Linear RVM                   | **83.11**    | 95.55        | <u>96.99</u> | **99.12**     |
| 10   | Kernel RVM                   | **83.11**    | 95.00        | 96.66        | 97.36         |





## 4. Result_Training Time

- ì¸¡ì • ë‹¨ìœ„ : Second
- CPU : AMD Ryzen 7 5800U ì‚¬ìš© (Balanced Mode)
- GPU : Nvidia Mobile 3050ti ì‚¬ìš©

|      | Algorithm                    | Diabetes                | Digits                  | Iris        | Breast Cancer |
| ---- | ---------------------------- | ----------------------- | ----------------------- | ----------- | ------------- |
| 1    | Linear SVM                   | **0.69**                | **1.62**                | **0.08**    | **0.21**      |
| 2    | Kernel SVM                   | 4.30                    | <u>34.91</u>            | <u>0.58</u> | <u>1.83</u>   |
| 3    | Basic ANN                    | 110.09 (GPU, 200epochs) | 174.86 (GPU, 200epochs) | 7.39        | 27.62 (GPU)   |
| 4    | TabNet (Deep Learning Model) | 149.32 (GPU, 100epochs) | 345.10 (GPU, 100epochs) | 29.91       | 106.47 (GPU)  |
| 5    | XGBoost                      | 9.44                    | 47.81                   | 8.34        | 7.70          |
| 6    | LightGBM                     | <u>4.02</u>             | 44.31                   | 4.59        | 6.03          |
| 7    | CatBoost                     | 20.90                   | 58.90                   | 18.41       | 41.69         |
| 8    | Random Forest                | 25.47                   | 41.16                   | 24.65       | 33.16         |
| 9    | Linear RVM                   | 102.08                  | 2371.65 (39ë¶„ ì†Œìš”!)    | 45.46       | 74.22         |
| 10   | Kernel RVM                   | 85.93                   | 1846.79 (30ë¶„ ì†Œìš”!)    | 30.16       | 80.05         |





## 5. Result_Inference Time

- ì¸¡ì • ë‹¨ìœ„ : Second
- CPU : AMD Ryzen 7 5800U ì‚¬ìš© (Balanced Mode)
- GPU : Nvidia Mobile 3050ti ì‚¬ìš©

|      | Algorithm                    | Diabetes      | Digits           | Iris          | Breast Cancer |
| ---- | ---------------------------- | ------------- | ---------------- | ------------- | ------------- |
| 1    | Linear SVM                   | 0.0033        | 0.0088           | **0.0001**    | <u>0.0009</u> |
| 2    | Kernel SVM                   | 0.0061        | 0.042            | 0.0010        | 0.0019        |
| 3    | Basic ANN                    | 0.0016 (GPU)  | **0.0029 (GPU)** | 0.0020 (GPU)  | 0.0019 (GPU)  |
| 4    | TabNet (Deep Learning Model) | 0.1037 (GPU)  | 0.2753 (GPU)     | 0.0263 (GPU)  | 0.1176 (GPU)  |
| 5    | XGBoost                      | 0.0020        | 0.0053           | 0.0030        | 0.0034        |
| 6    | LightGBM                     | **0.0009**    | 0.0050           | 0.0010        | <u>0.0009</u> |
| 7    | CatBoost                     | 0.0019        | 0.0110           | <u>0.0009</u> | 0.0046        |
| 8    | Random Forest                | 0.0029        | 0.0288           | 0.0029        | 0.0066        |
| 9    | Linear RVM                   | <u>0.0010</u> | 0.0076           | 0.0020        | 0.0010        |
| 10   | Kernel RVM                   | 0.0013        | <u>0.0037</u>    | 0.0032        | **0.0001**    |





# Final Insights

## 1. Training Time ê´€ì 

- **SVMì€ íƒ€ ë°©ì‹ ëŒ€ë¹„ ì „ë°˜ì ìœ¼ë¡œ ì¥ì ì„ ê°–ê³  ìˆìŒ** âœ…

- SVMì€ ì „ë°˜ì ìœ¼ë¡œ Training Timeì´ ë§¤ìš° ìš°ìˆ˜í•˜ë©°, ê°™ì€ ì‹œê°„ ëŒ€ë¹„ Boosting, Bagging, NN, RVM ê³„ì—´ë“¤ ëŒ€ë¹„ ë‹¤ì–‘í•œ Hyper Parameterë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆìŒ. SVMë³´ë‹¤ ë¹ ë¥¸ ë°©ì‹ì€ í¬ê²Œ ì°¨ì´ëŠ” ë‚˜ì§€ ì•Šìœ¼ë‚˜ Diabetes Datasetì—ì„œì˜ LightGBMì •ë„ ë°–ì— ì¡´ì¬í•˜ì§€ ì•Šì•˜ìŒ (ê·¸ëŸ¬ë‚˜ Parallel Threadingì„ ì“´ë‹¤ë©´ Boosting ê³„ì—´ì´ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ)
- ë¬¼ë¡  Training Time ë“±ì˜ ì†ë„ëŠ” ì–´ë– í•œ ì–¸ì–´ë¡œ ëœ êµ¬í˜„ì²´(ex. C, Rust ë“±)ì¸ì§€ì— ë”°ë¼ ë‹¤ë¥´ê³ , Coding Trickì„ ì¼ëŠëƒì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ê·¸ë¦¬ê³  SVMì€ ë§¤ìš° Optimizationëœ Codeë¡œ Scikit-Learnì— êµ¬í˜„ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ì„ ê²ƒ ê°™ìŒ. ê·¸ëŸ¬ë‚˜ XGBoostë‚˜ LightGBMê°™ì€ ê²½ìš°ë„ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ì‹(ex. Cache Hit Optimization ë“±)ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ì™„ì „ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì´ ë¶ˆë¦¬í•˜ë‹¤ê³  ë³´ê¸°ëŠ” í˜ë“¬.
- ê·¸ëŸ¬ë‚˜ XGBoost ê°™ì€ ê²½ìš° ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ë°©ì‹ ìì²´ê°€ Thread Processingì„ ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, single threadê¸°ë°˜ì—ì„œëŠ” ì†ë„ì  ì´ë“ì„ ì–»ê¸°ê°€ í˜ë“œë¦¬ë¼ ë´„
- ë”°ë¼ì„œ ARM Cortex Aê°€ ì•„ë‹Œ, ê·¸ ì™¸ì˜ Real-Time Embedded Systemì—ì„œì˜ Trainingì—ì„œëŠ” SVMì´ íƒ€ ì•Œê³ ë¦¬ì¦˜ì„ ì••ë„í•˜ëŠ” ì†ë„ì ì¸ ì´ìµì„ ì–»ì„ ê²ƒì´ë¼ ìƒê°ë¨
- ê·¸ë¦¬ê³  Tabular Dataì—ì„œ Hyper Parmeter Searchingì„ ê°€ë¯¸í•œ Baseline ëª¨ë¸ì„ ì°¾ì•„ë‚´ëŠ”ë° SVMì´ ë§¤ìš° ì í•©í•˜ë¦¬ë¼ ìƒê°ë¨



## 2. Inference Time ê´€ì 

- **SVMì€ íƒ€ ë°©ì‹ ëŒ€ë¹„ Big Datasetì—ì„œëŠ” ë‹¨ì ì„ ê°–ê³  ìˆìŒ** âŒ
- **ê·¸ëŸ¬ë‚˜ SVMì€ íƒ€ ë°©ì‹ ëŒ€ë¹„ Small Datasetì—ì„œëŠ” ë‚˜ì˜ì§€ ì•ŠëŠ” ì†ë„ë¥¼ ê°–ê³  ìˆìŒ(Single Thread Embedded Real-Time Systemì— ì í•©)** âœ…

- DiabetesëŠ” 700ê°œ ì •ë„ì˜ Datasetì¸ë°, ì´ì •ë„ í¬ê¸° ì´í›„ë¶€í„° Inferenceì—ì„œëŠ” SVMì´ ë‹¤ë¥¸ ë°©ì‹ ëŒ€ë¹„ ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì´ì§€ëŠ” ì•ŠìŒ. ë¬¼ë¡  GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” NNê³„ì—´ë³´ë‹¤ëŠ” ë¹ ë¥¼ ìˆ˜ ìˆê² ìœ¼ë‚˜, Boosting, Random Forest, RVM ë“±ì— ëª¨ë‘ ì†ë„ê°€ ë°€ë¦¼

- ê·¸ëŸ¬ë‚˜ Small Datasetì„ í•œë²ˆì— Inferenceí•  ë•Œì—ëŠ” ì¥ì ì„ ê°–ì¶”ê³  ìˆìŒ. ì´ë¥¼ ë³´ì•˜ì„ë•Œ Single Threadì˜ Embedded Systemì—ì„œ Real-Time Inferenceë‚˜ FPGA, ASICìœ¼ë¡œ êµ¬í˜„í–ˆì„ë•Œì˜ ì†ë„ëŠ” SVMì´ ê°€ì¥ ë¹ ë¥´ë¦¬ë¼ ìƒê°ë¨ (íŠ¹íˆ Inference Timeì— Single Instanceë¥¼ ì²˜ë¦¬í•˜ëŠ” ì†ë„ê°€ ê°€ì¥ ë¹ ë¥´ë¦¬ë¼ ì˜ˆìƒ)

- ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„±ìƒ Boostingê³„ì—´ì—ì„œ ì •ë ¬ ë“±ì´ í•„ìš”í•˜ì—¬ ì´ë•Œ Boot-upë˜ëŠ” ì†ë„ê°€ SVMëŒ€ë¹„ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •ë¨

  



## 3. Accuracy ê´€ì  

- **SVMì´ íƒ€ ë°©ì‹ ëŒ€ë¹„ Accuracyê°€ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ì¢‹ì€ ê²½ìš°ë„ ìˆìŒ** âœ…
- Accuracyë„ SVMì´ ì „ì²´ì ìœ¼ë¡œ ëª¨ë“  Datasetì—ì„œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ëŒ€ë¹„ ê°€ì¥ ì¢‹ê±°ë‚˜(Diabetes, Digits), 2~3ìœ„ ìˆ˜ì¤€(Irs, Breast Cancer)ì˜ Accuracyë¥¼ ë‚˜íƒ€ëƒ„. 
- íŠ¹íˆë‚˜ Linear SVM (Soft Margin)ì´ ë‹¤ë¥¸ ëª¨ë¸ë“¤ ëŒ€ë¹„ í¬ê²Œ ë–¨ì–´ì§€ì§€ ì•ŠëŠ” Accuracyë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨, Kernelì´ í•„ìš”í•œ íŠ¹ì´í•œ Caseì˜ ì„ì˜ë¡œ ìƒì„±ëœ Datasetì´ ì•„ë‹Œí•œ, ë§¤ìš° ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ Classificationì„ í•˜ëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ì—ˆë‹¤ê³  ë³´ì—¬ì§. íŠ¹íˆë‚˜ Dimensionì´ ì»¤ì§€ë©´ì„œ Linear Modelë¡œ ë¶„ë¥˜ë˜ëŠ” Hyper Planeì„ ì°¾ê¸°ê°€ ë” ì‰½ì§€ ì•Šì„ê¹Œ ìƒê°ë¨.
- ë¬¼ë¡  Datasetì´ í¬ì§€ ì•Šê³  ë‹¨ìˆœí•˜ë©°, ì „ë°˜ì ìœ¼ë¡œ Accuracyê°€ ìœ ì‚¬í•˜ê²Œ ë†’ìœ¼ë¯€ë¡œ(ë¬¼ë¡  Use-Caseì— ë”°ë¼ì„œ 0.1% Accuracyë„ ë§¤ìš° í¬ë‹¤ê³  ë³¼ ìˆ˜ ìˆê¸´ í•˜ì§€ë§Œ) ì´ ê²°ê³¼ë¡œë§Œ ê°€ì§€ê³  SVMì´ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ í™•ì‹¤íˆ ë›°ì–´ë‚˜ë‹¤ê³  ë³¼ìˆ˜ëŠ” ì—†ìŒ.
- ê·¸ëŸ¬ë‚˜ í™•ì‹¤í•œê±´, **Training Timeê³¼ Inference TimeëŒ€ë¹„, SVMì´ ë‹¤ë¥¸ ìµœì‹ ì˜ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë–¨ì–´ì§„ë‹¤ê³  ë³´ê¸° ì–´ë ¤ìš°ë©°, ì˜¤íˆë ¤ ë” ì¢‹ì€ ê²½ìš° ìˆë‹¤ê³  ë§í•  ìˆ˜ ìˆìŒ**.  ë”°ë¼ì„œ Silver Bulletì€ ì—†ìœ¼ë¯€ë¡œ, SVMì•Œê³ ë¦¬ì¦˜ì„ ì‹¤ë¬´ì—ì„œ ê¼­ ê²€ì¦ í•´ ë³¼ í•„ìš”ëŠ” ìˆìŒ



## 4. ê·¸ ì™¸ì˜ ìƒê°ë“¤

- ê·¸ ì™¸ Tabnetê°™ì€ ê²½ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, Datasetì— ì ì€ ê²½ìš° íŠ¹íˆë‚˜ ê·¸ëŸ¬í•˜ë©°(Iris, Breast Cancer), Tabnetì€ category encodingì— ìœ ë¦¬í•œ ì¸¡ë©´ì´ ìˆë‹¤ê³  ë³´ì—¬, ì¢€ ë” ë³µì¡í•œ columnì„ ê°€ì§„ dataset, ì¢€ ë” í° datasetì—ì„œ í™œìš©ë˜ë©´ ì¢‹ì„ ê²ƒì´ë¼ ìƒê°ë¨
- ê·¸ë¦¬ê³  Basic ANNì€ Dropoutê³¼ SELUë“±ì˜ í™œìš©ìœ¼ë¡œ Tabularì—ì„œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ëª»ì§€ì•Šì€ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•¨
- ë˜í•œ RVMì€ íƒ€ ì•Œê³ ë¦¬ì¦˜ ëŒ€ë¹„ Training Timeì´ ìƒëŒ€ì ìœ¼ë¡œ ë§¤ìš° ê¸¸ì—ˆì§€ë§Œ, Inference Timeì€ íƒ€ ì•Œê³ ë¦¬ì¦˜ëŒ€ë¹„ ë§¤ìš° ë¹ ë¥¸ í¸ì„. ë˜í•œ ì„±ëŠ¥ë„ DIgits Dataset ì™¸ì—ëŠ” íƒ€ ì•Œê³ ë¦¬ì¦˜ ëŒ€ë¹„ ìƒëŒ€ì ìœ¼ë¡œ  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŒ. 
- ê·¸ëŸ¬ë‚˜ RVMì€ Digits ë°ì´í„°ê°€ 1700ê°œ ê°€ëŸ‰ ë°–ì— ì•ˆë˜ëŠ” ê²½ìš°ì—ë„, 35ë¶„ì´ë‚˜ ê±¸ë¦¬ëŠ” ì•„ì£¼ ê¸´ Trainingì‹œê°„ì„ ê°–ìŒ. ë”°ë¼ì„œ RVMì€ Small Datasetì—ì„œë§Œ ê¼­ ê³ ë ¤í•´ì•¼í•  ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìƒê°ë¨(Hyper Parameter Tunningë„ ìë™ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë©°, Outputì— ëŒ€í•œ Uncertaintyë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ)
- RVMì€ Gaussian Processì™€ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°ê°€ ì»¤ì§ì— ë”°ë¼ ë§¤ìš° ëŠë ¤ì§€ê³  O(N^3), Featureì˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ Accuracyê°€ ë–¨ì–´ì§€ëŠ” ê²½í–¥ì„ ë³´ì´ì§€ ì•Šë‚˜ ìƒê°ì´ ë“¬
- RVM Trainingì˜ ë¹ ë¥¸ êµ¬í˜„ Tipping, M., & Faul, A. (2003). Fast marginal likelihood maximization for sparse Bayesian modelsë„ ì¡´ì¬í•˜ë¯€ë¡œ, í•´ë‹¹ êµ¬í˜„ì„ ì‚¬ìš©í•œë‹¤ë©´ ì´ë²ˆ Tutorialì˜ êµ¬í˜„ë³´ë‹¤ëŠ” ë” ê¸ì •ì ì¸ ëŠë‚Œì„ ë°›ì•˜ìœ¼ë¦¬ë¼ ìƒê°ë¨
- Random ForestëŠ” ì „ë°˜ì ìœ¼ë¡œ Training Timeì´ë‚˜ Inference Timeì—ì„œ í° ì¥ì ì€ ì—†ì—ˆìœ¼ë©°(ë¹ ë¥¸ í¸ì´ ì•„ë‹ˆì—ˆìŒ), Accuracy ì„±ëŠ¥ ì—­ì‹œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ë³´ë‹¤ ë”±íˆ ë›°ì–´ë‚˜ë‹¤ê³  ë³´ì´ì§€ëŠ” ì•ŠìŒ(Breaset Cancerì œì™¸ Boostingë³´ë‹¤ ì „ë°˜ì ìœ¼ë¡œ ë–¨ì–´ì§). í–¥í›„ì—ëŠ” Random Forestë³´ë‹¤ëŠ”, SVM, Boosting, RVMì„ ì¢€ ë” ê³ ë ¤í•˜ì§€ ì•Šì„ê¹Œ ìƒê°ì´ ë“¬ (ë¬¼ë¡  Hyper Parameterë¥¼ ì¢€ ë” í…ŒìŠ¤íŠ¸ í•˜ë©´ ë‹¤ë¥¼ ìˆ˜ëŠ” ìˆì„ ë“¯)
- Boosting ê³„ì—´ì—ì„œ ë¹„êµí•˜ìë©´, ì†ë„ ì¸¡ë©´ì—ì„œëŠ” LightGBMì´ ë¹ ë¥¸ í¸ì´ì§€ë§Œ Accuracy ì¸¡ë©´ì—ì„œëŠ” XGBoostë‚˜ CatBoostê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ë©° ì†ë„ë„ í¬ê²Œ ëŠë¦¬ì§€ëŠ” ì•ŠìŒ. LightGBMë³´ë‹¤ XGBoostë‚˜ CatBootë¥¼ ì¢€ ë” ê³ ë ¤í•˜ëŠ”ê²Œ ì¢‹ì§€ ì•Šì„ê¹Œ ìƒê°ë¨



## 5. ê²°ë¡ 

- ê²°ë¡ ì ìœ¼ë¡œëŠ” SVMì•Œê³ ë¦¬ì¦˜ì€ ì§€ê¸ˆë„ ì“¸ë§Œí•œ ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  ë§í•  ìˆ˜ ìˆìŒ.

