{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔥 MixMatch : A Holistic Approach Semi-Super Vised Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import models.wideresnet as models\n",
    "from utils.logger import *\n",
    "from utils.misc import *\n",
    "from utils.accuracy import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyper-Parameters\n",
    "\n",
    "Hyper-Parameter를 정의해 둠. 대부분은 Original Paper에 기반으로 정의 되어 있음. 사용자가 조절할만한 Hyper-Parameter들은 아래와 같음.\n",
    "\n",
    "- epochs : 학습시킬 epochs 지정\n",
    "- batch_size : batch 크기.\n",
    "- random_seed : random seed값 설정.\n",
    "- lr : learning rate. batch_size에 따라서 조절 필요. batch_size가 k배 커지만 lr은 sqrt(k)배 만큼 크게 만들면 좋음\n",
    "- is_continue_train : 이전 saved best model을 불러와서 이어서 학습할지 여부 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.75\n",
    "lambda_u = 75\n",
    "# train_iteration = 1024\n",
    "# train_iteration = 256 \n",
    "train_iteration = 512 \n",
    "ema_decay = 0.999\n",
    "Temperature = 0.5\n",
    "\n",
    "is_ema = True \n",
    "is_continue_train = False \n",
    "start_epoch = 0\n",
    "epochs = 1000\n",
    "# num_labeled = 500 \n",
    "num_labeled = 250 \n",
    "\n",
    "\n",
    "use_interleaving = False # Interleaving을 False로 할 경우, Batchsize는 3배로 늘어나므로 sqrt(3)배를 lr에 곱해야 함\n",
    "\n",
    "# batch_size = 64 # Original Batch Size\n",
    "batch_size = 128 # x2배 Batch Size\n",
    "\n",
    "# lr = 0.002 # Original lr (interleaving), batch_size = 64\n",
    "# lr = 0.00346 # sqrt('batch size') * 'Original lr' (non-interleaving), batch_size = 64\n",
    "lr = 0.0049 # sqrt('batch size') * 'Original lr' (non-interleaving), batch_size = 128 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(512)\n",
    "\n",
    "# best_acc = 0  # best test accuracy\n",
    "\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465) \n",
    "CIFAR10_STD = (0.2471, 0.2435, 0.2616)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functions for Augmentation \n",
    "\n",
    "Data Augmentation을 위한 Function들을 모아둠. Pytorch Data Loader를 통해 Data를 Loading할 때 마다 Real-Time으로 Augmentation을 진행함\n",
    "\n",
    "- Image Normalization (Standard Scaling)\n",
    "- Image Transpose (from CIFAR-10 shape to Pytorch Shape)\n",
    "- Augmentation Method #1 : Image Padding & Cropping\n",
    "- Augmentation Method #2 : Image Horizontal Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Normalization (Standard Scaling)\n",
    "def normalize(x, mean=CIFAR10_MEAN, std=CIFAR10_STD):\n",
    "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
    "    x -= mean*255\n",
    "    x *= 1.0/(255*std)\n",
    "    return x\n",
    "\n",
    "# Transpose Image Shape for Pytorch shape from CIFAR-10 Original shape\n",
    "def transpose(x, source='NHWC', target='NCHW'):\n",
    "    return x.transpose([source.index(d) for d in target]) \n",
    "\n",
    "# Padding to image borders for cropping image\n",
    "def pad(x, border=4):\n",
    "    return np.pad(x, [(0, 0), (border, border), (border, border)], mode='reflect')\n",
    "\n",
    "# Augmentation Method 1 : Random Padding and Cropping \n",
    "class RandomPadandCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = pad(x, 4)\n",
    "\n",
    "        h, w = x.shape[1:]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        x = x[:, top: top + new_h, left: left + new_w]\n",
    "\n",
    "        return x\n",
    "\n",
    "# Augmentation Method 2 : Random Horizontal Flipping \n",
    "class RandomFlip(object):\n",
    "    def __call__(self, x):\n",
    "        if np.random.rand() < 0.5:\n",
    "            x = x[:, :, ::-1]\n",
    "\n",
    "        return x.copy()\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, x):\n",
    "        x = torch.from_numpy(x)\n",
    "        return x\n",
    "\n",
    "def split_dataset(labels, num_train):\n",
    "    labels = np.array(labels)\n",
    "    index_train_labeled = []\n",
    "    index_train_unlabeled = []\n",
    "    index_valid = []\n",
    "\n",
    "    num_labels = len(np.unique(labels))\n",
    "    num_labeled_per_class = int(num_train / num_labels)\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        index = np.where(labels == i)[0]\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        index_train_labeled.extend(index[:num_labeled_per_class])\n",
    "        index_train_unlabeled.extend(index[num_labeled_per_class:-500])\n",
    "        index_valid.extend(index[-500:])\n",
    "\n",
    "    np.random.shuffle(index_train_labeled)\n",
    "    np.random.shuffle(index_train_unlabeled)\n",
    "    np.random.shuffle(index_valid)\n",
    "\n",
    "    return index_train_labeled, index_train_unlabeled, index_valid\n",
    "\n",
    "# Twice Augmentation for Unlabeld Image (eg. K=2)\n",
    "class Multi_Augmentation:\n",
    "    def __init__(self, transform_method):\n",
    "        self.transform_method = transform_method\n",
    "        # self.num_transform = num_transform\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        aug_out_1 = self.transform_method(inp)\n",
    "        aug_out_2 = self.transform_method(inp)\n",
    "        return aug_out_1, aug_out_2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions for loading CIFAR-10 dataset \n",
    "\n",
    "Pytorch의 dataloader에서 사용할 함수들. Data를 불러오면서 augmentation을 수행해 줌. unlabeled data의 augmentation횟수는 original paper에 따라 K=2로 맞춰서 2번 augmentation수행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 Labeled Images with Random Augmentation\n",
    "class get_cifar10_labeled(torchvision.datasets.CIFAR10):\n",
    "\n",
    "    def __init__(self, path_cifar10, indexs=None, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(get_cifar10_labeled, self).__init__(path_cifar10, train=train,\n",
    "                 transform=transform, target_transform=target_transform,\n",
    "                 download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "        self.data = transpose(normalize(self.data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_x, target_y = self.data[index], self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data_x = self.transform(data_x)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target_y = self.target_transform(target_y)\n",
    "\n",
    "        return data_x, target_y\n",
    "    \n",
    "\n",
    "# Load CIFAR-10 Unlabeled Images with Random Augmentation (K=2)\n",
    "class get_cifar10_unlabeled(get_cifar10_labeled):\n",
    "\n",
    "    def __init__(self, path_cifar10, indexs, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(get_cifar10_unlabeled, self).__init__(path_cifar10, indexs, train=train,\n",
    "                 transform=transform, target_transform=target_transform,\n",
    "                 download=download)\n",
    "        self.targets = np.array([-1 for i in range(len(self.targets))])\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functions for MixMatch learning method\n",
    "\n",
    "MixMatch를 학습할 때 필요한 중요한 기능들을 Function으로 정의해 둠\n",
    "- Ramp-Up Function (weight balancing betweein supervised and unsupervised loss)\n",
    "- Calculation of semi-supervised loss (Each_Loss)\n",
    "- Exponential Moving Average(EMA) Function\n",
    "- Interleaving Functions : 이번 Tutorial에서 확인할 함수. MixUp된 Labeled Data와 Unlabeled Data를 섞어줌.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ramp-up Function for balancing the weight between supervised loss and unsupervised loss\n",
    "# - loss_total = loss_l + weight * loss_u\n",
    "def ramp_up(current, rampup_length=epochs):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)\n",
    "\n",
    "class Each_Loss(object):\n",
    "    def __call__(self, pred_l, target_l, pred_u, target_u, epoch):\n",
    "        probs_u = torch.softmax(pred_u, dim=1)\n",
    "\n",
    "        loss_l = -torch.mean(torch.sum(F.log_softmax(pred_l, dim=1) * target_l, dim=1))\n",
    "        loss_u = torch.mean((probs_u - target_u)**2)\n",
    "\n",
    "        return loss_l, loss_u, lambda_u * ramp_up(epoch)\n",
    "\n",
    "class exponential_moving_average(object):\n",
    "    def __init__(self, student_model, teacher_model, alpha=0.999):\n",
    "        self.model = student_model\n",
    "        self.ema_model = teacher_model\n",
    "        self.alpha = alpha\n",
    "        self.student_params = list(student_model.state_dict().values())\n",
    "        self.teacher_params = list(teacher_model.state_dict().values())\n",
    "        self.wd = 0.02 * lr\n",
    "\n",
    "        for param, ema_param in zip(self.student_params, self.teacher_params):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        one_minus_alpha = 1.0 - self.alpha\n",
    "        for student_param, teacher_param in zip(self.student_params, self.teacher_params):\n",
    "            if teacher_param.dtype==torch.float32:\n",
    "                teacher_param.mul_(self.alpha)\n",
    "                teacher_param.add_(student_param * one_minus_alpha)\n",
    "                # customized weight decay\n",
    "                student_param.mul_(1 - self.wd)\n",
    "\n",
    "def interleave_offsets(batch, nu):\n",
    "    groups = [batch // (nu + 1)] * (nu + 1)\n",
    "    for x in range(batch - sum(groups)):\n",
    "        groups[-x - 1] += 1\n",
    "    offsets = [0]\n",
    "    for g in groups:\n",
    "        offsets.append(offsets[-1] + g)\n",
    "    assert offsets[-1] == batch\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def interleave(xy, batch):\n",
    "    nu = len(xy) - 1\n",
    "    offsets = interleave_offsets(batch, nu)\n",
    "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
    "    for i in range(1, nu + 1):\n",
    "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
    "    return [torch.cat(v, dim=0) for v in xy]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Utility Functions (Tedious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='./Results/', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Function\n",
    "\n",
    "MixMatch구현에 있어서 가장 중요한 함수이다. 이 함수에서 MixMatch의 모든 기법이 사용되며, 알고리즘의 Sequence대로 구현이 되어있다.\n",
    "또한 이 함수에 Interleaving이 존재한다. Batch-Normalization을 고려하여, Supervised Logits과 Unsupervised Logits을 Model에 각각 태우면 Interleaving이 필요하며,\n",
    "그렇지 않고 한번에 Model에 태워 계산 할 경우 Interleaving은 필요하지 않다.\n",
    "\n",
    "- 1. Data Augmentation\n",
    "- 2. Label Guessing and Label Sharpening\n",
    "- 3. MixUp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, each_loss, epoch, use_cuda, is_ema=False):\n",
    "\n",
    "    losses_total = AverageMeter()\n",
    "    losses_l = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    weights = AverageMeter()\n",
    "\n",
    "\n",
    "    iter_train_labeled = iter(labeled_trainloader)\n",
    "    iter_train_unlabeled = iter(unlabeled_trainloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch_index in range(train_iteration):\n",
    "        #########################################\n",
    "        # 1. Data Augmentation\n",
    "        #########################################\n",
    "        try:\n",
    "            inputs_l, targets_l = next(iter_train_labeled)\n",
    "        except:\n",
    "            iter_train_labeled = iter(labeled_trainloader)\n",
    "            inputs_l, targets_l = next(iter_train_labeled)\n",
    "\n",
    "\n",
    "        try:\n",
    "            (inputs_u, inputs_u2), _ = next(iter_train_unlabeled)\n",
    "        except:\n",
    "            iter_train_unlabeled = iter(unlabeled_trainloader)\n",
    "            (inputs_u, inputs_u2), _ = next(iter_train_unlabeled)\n",
    "\n",
    "        # Transform label to one-hot\n",
    "        batch_size = inputs_l.size(0)\n",
    "        targets_l = torch.zeros(batch_size, 10).scatter_(1, targets_l.view(-1,1).long(), 1)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs_l, targets_l = inputs_l.cuda(), targets_l.cuda(non_blocking=True)\n",
    "            inputs_u = inputs_u.cuda()\n",
    "            inputs_u2 = inputs_u2.cuda()\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        # 2. Label Guessing & Label Sharpening \n",
    "        #########################################\n",
    "        with torch.no_grad():\n",
    "            # compute guessed labels of unlabel samples\n",
    "            outputs_u = model(inputs_u)\n",
    "            outputs_u2 = model(inputs_u2)\n",
    "            p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "            pt = p**(1/Temperature)\n",
    "            targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "            targets_u = targets_u.detach()\n",
    "\n",
    "        #########################################\n",
    "        # 3. MixUp \n",
    "        #########################################\n",
    "        all_inputs = torch.cat([inputs_l, inputs_u, inputs_u2], dim=0)\n",
    "        all_targets = torch.cat([targets_l, targets_u, targets_u], dim=0)\n",
    "\n",
    "        Lambda_ = np.random.beta(alpha, alpha)\n",
    "\n",
    "        Lambda_ = max(Lambda_, 1-Lambda_)\n",
    "\n",
    "        idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "        input_a, input_b = all_inputs, all_inputs[idx]\n",
    "        target_a, target_b = all_targets, all_targets[idx]\n",
    "\n",
    "        mixed_input = Lambda_ * input_a + (1 - Lambda_) * input_b\n",
    "        mixed_target = Lambda_ * target_a + (1 - Lambda_) * target_b\n",
    "\n",
    "\n",
    "        ########################################\n",
    "        # 4. Interleaving or No-Interleaving\n",
    "        ########################################\n",
    "        if use_interleaving:\n",
    "            # 1) interleave labeled and unlabed images between batches to get correct batchnorm calculation \n",
    "            mixed_input = list(torch.split(mixed_input, batch_size))\n",
    "            mixed_input = interleave(mixed_input, batch_size)\n",
    "\n",
    "            # 2) labeled prediction\n",
    "            logits = [model(mixed_input[0])]\n",
    "            \n",
    "            # 3) unlabeled prediction\n",
    "            for input in mixed_input[1:]:\n",
    "                logits.append(model(input))\n",
    "\n",
    "            # 4) de_interleave to calculate labeled supervised loss and unlabeld unsupervised loss properly \n",
    "            logits = interleave(logits, batch_size)\n",
    "\n",
    "            logits_l = logits[0]\n",
    "            logits_u = torch.cat(logits[1:], dim=0)\n",
    "        else:\n",
    "            # No Interleaving and calculate both labeled and unlabeled sample.\n",
    "            # The model is used only once to predict logits. So A calculation of batchnorm is proper.\n",
    "            # But if you want to use this no-interleaving method then you should adjust the learning rate (with multiply sqrt(k))\n",
    "            # k means k-times of increased batch size\n",
    "            logits = model(mixed_input)\n",
    "            split_logits = list(torch.split(logits, batch_size))\n",
    "            logits_l = split_logits[0]\n",
    "            logits_u = torch.cat(split_logits[1:], dim=0)\n",
    "\n",
    "\n",
    "        ########################################\n",
    "        # 5. Semi-Supervised Loss \n",
    "        ########################################\n",
    "        loss_l, loss_u, weight = each_loss(logits_l, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_index/train_iteration)\n",
    "\n",
    "        loss_total = loss_l + weight * loss_u\n",
    "\n",
    "        # record loss\n",
    "        losses_total.update(loss_total.item(), inputs_l.size(0))\n",
    "        losses_l.update(loss_l.item(), inputs_l.size(0))\n",
    "        losses_u.update(loss_u.item(), inputs_l.size(0))\n",
    "        weights.update(weight, inputs_l.size(0))\n",
    "\n",
    "        ########################################\n",
    "        # 6. Backpropagation \n",
    "        ########################################\n",
    "        optimizer.zero_grad()\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        ########################################\n",
    "        # 7. EMA Learning for Teacher Model \n",
    "        ########################################\n",
    "        if is_ema is True:\n",
    "            ema_optimizer.step()\n",
    "\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'{batch_index+1}/{train_iteration} : Total Loss {losses_total.avg:.3f}')\n",
    "\n",
    "    return (losses_total.avg, losses_l.avg, losses_u.avg,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valloader, model, criterion, use_cuda, mode):\n",
    "    losses_total = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, targets) in enumerate(valloader):\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            ori_targets = targets\n",
    "            ori_targets = ori_targets.cuda(non_blocking=True)\n",
    "\n",
    "            targets = torch.zeros(batch_size, 10).scatter_(1, targets.view(-1,1).long(), 1)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # accuracy top-1 and top-5\n",
    "            prec1, prec5 = accuracy(outputs, ori_targets, topk=(1, 1))\n",
    "\n",
    "            losses_total.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "\n",
    "            if batch_index % 10 == 0:\n",
    "                print(f'{batch_index+1}/{train_iteration} : Total Loss {losses_total.avg:.3f}, Top1 Acc {top1.avg:.3f}')\n",
    "    return (losses_total.avg, top1.avg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 🌊Main - Start to Train the MixMatch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Load Dataset (CIFAR-10)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Labeled: 250 Unlabeled: 44750 Validation: 5000\n",
      "Epoch: 1 / 1000\n",
      "\n",
      "1/512 : Total Loss 2.300\n",
      "11/512 : Total Loss 2.187\n",
      "21/512 : Total Loss 2.090\n",
      "31/512 : Total Loss 2.017\n",
      "41/512 : Total Loss 1.946\n",
      "51/512 : Total Loss 1.902\n",
      "61/512 : Total Loss 1.865\n",
      "71/512 : Total Loss 1.833\n",
      "81/512 : Total Loss 1.761\n",
      "91/512 : Total Loss 1.760\n",
      "101/512 : Total Loss 1.723\n",
      "111/512 : Total Loss 1.708\n",
      "121/512 : Total Loss 1.680\n",
      "131/512 : Total Loss 1.662\n",
      "141/512 : Total Loss 1.659\n",
      "151/512 : Total Loss 1.653\n",
      "161/512 : Total Loss 1.646\n",
      "171/512 : Total Loss 1.634\n",
      "181/512 : Total Loss 1.620\n",
      "191/512 : Total Loss 1.616\n",
      "201/512 : Total Loss 1.604\n",
      "211/512 : Total Loss 1.583\n",
      "221/512 : Total Loss 1.574\n",
      "231/512 : Total Loss 1.548\n",
      "241/512 : Total Loss 1.530\n",
      "251/512 : Total Loss 1.510\n",
      "261/512 : Total Loss 1.493\n",
      "271/512 : Total Loss 1.488\n",
      "281/512 : Total Loss 1.476\n",
      "291/512 : Total Loss 1.463\n",
      "301/512 : Total Loss 1.453\n",
      "311/512 : Total Loss 1.448\n",
      "321/512 : Total Loss 1.440\n",
      "331/512 : Total Loss 1.421\n",
      "341/512 : Total Loss 1.420\n",
      "351/512 : Total Loss 1.413\n",
      "361/512 : Total Loss 1.400\n",
      "371/512 : Total Loss 1.384\n",
      "381/512 : Total Loss 1.369\n",
      "391/512 : Total Loss 1.357\n",
      "401/512 : Total Loss 1.354\n",
      "411/512 : Total Loss 1.349\n",
      "421/512 : Total Loss 1.345\n",
      "431/512 : Total Loss 1.339\n",
      "441/512 : Total Loss 1.336\n",
      "451/512 : Total Loss 1.331\n",
      "461/512 : Total Loss 1.321\n",
      "471/512 : Total Loss 1.319\n",
      "481/512 : Total Loss 1.316\n",
      "491/512 : Total Loss 1.313\n",
      "501/512 : Total Loss 1.300\n",
      "511/512 : Total Loss 1.297\n",
      "1/512 : Total Loss 1.940, Top1 Acc 32.812\n",
      "1/512 : Total Loss 2.104, Top1 Acc 21.875\n",
      "11/512 : Total Loss 2.107, Top1 Acc 24.219\n",
      "21/512 : Total Loss 2.112, Top1 Acc 24.368\n",
      "31/512 : Total Loss 2.112, Top1 Acc 24.345\n",
      "1/512 : Total Loss 2.011, Top1 Acc 29.688\n",
      "11/512 : Total Loss 2.080, Top1 Acc 26.847\n",
      "21/512 : Total Loss 2.087, Top1 Acc 27.046\n",
      "31/512 : Total Loss 2.087, Top1 Acc 26.689\n",
      "41/512 : Total Loss 2.083, Top1 Acc 26.829\n",
      "51/512 : Total Loss 2.085, Top1 Acc 26.593\n",
      "61/512 : Total Loss 2.081, Top1 Acc 26.844\n",
      "71/512 : Total Loss 2.083, Top1 Acc 26.926\n",
      "Epoch: 2 / 1000\n",
      "\n",
      "1/512 : Total Loss 1.391\n",
      "11/512 : Total Loss 1.025\n",
      "21/512 : Total Loss 1.004\n",
      "31/512 : Total Loss 1.088\n",
      "41/512 : Total Loss 1.030\n",
      "51/512 : Total Loss 1.066\n",
      "61/512 : Total Loss 1.075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mmean(test_accs[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]))\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 121\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn [23], line 87\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m is_ema \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     train_loss, train_loss_x, train_loss_u \u001b[39m=\u001b[39m train(train_loader_labeled, train_loader_unlabeled, student_model, optimizer, ema_optimizer, train_criterion, epoch, use_cuda, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     88\u001b[0m     _, train_acc \u001b[39m=\u001b[39m validate(train_loader_labeled, teacher_model, criterion, use_cuda, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     89\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m validate(valid_loader, teacher_model, criterion, use_cuda, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValid Accuracy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [20], line 127\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, each_loss, epoch, use_cuda, is_ema)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m########################################\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# 7. EMA Learning for Teacher Model \u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m########################################\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m is_ema \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     ema_optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m batch_index \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    131\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mbatch_index\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtrain_iteration\u001b[39m}\u001b[39;00m\u001b[39m : Total Loss \u001b[39m\u001b[39m{\u001b[39;00mlosses_total\u001b[39m.\u001b[39mavg\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [18], line 38\u001b[0m, in \u001b[0;36mexponential_moving_average.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m teacher_param\u001b[39m.\u001b[39madd_(student_param \u001b[39m*\u001b[39m one_minus_alpha)\n\u001b[0;32m     37\u001b[0m \u001b[39m# customized weight decay\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m student_param\u001b[39m.\u001b[39;49mmul_(\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwd)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global start_epoch\n",
    "    best_acc = 0\n",
    "\n",
    "    output_folder = './Results/'\n",
    "    path_cifar10 = './data'\n",
    "\n",
    "    if is_continue_train == True:\n",
    "        load_model = output_folder+'model_best.pth.tar'\n",
    "\n",
    "\n",
    "    if not os.path.isdir(output_folder):\n",
    "        mkdir_p(output_folder)\n",
    "\n",
    "    print('1. Load Dataset (CIFAR-10)')\n",
    "    augmentation_train = transforms.Compose([\n",
    "        RandomPadandCrop(32), # 32x32 Random Cropping\n",
    "        RandomFlip(), # Random Horizontal Flipping\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "    augmentation_test = transforms.Compose([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Preparing CIFAR-10 Dataset (train, valid, test sets)\n",
    "    cifar10_dataset = torchvision.datasets.CIFAR10(path_cifar10, train=True, download=True)\n",
    "    train_labeled_index, train_unlabeled_index, valid_index = split_dataset(cifar10_dataset.targets, num_labeled)\n",
    "\n",
    "    train_labeled_dataset = get_cifar10_labeled(path_cifar10, train_labeled_index, train=True, transform=augmentation_train)\n",
    "    train_unlabeled_dataset = get_cifar10_unlabeled(path_cifar10, train_unlabeled_index, train=True, transform=Multi_Augmentation(augmentation_train))\n",
    "    valid_dataset = get_cifar10_labeled(path_cifar10, valid_index, train=True, transform=augmentation_test, download=True)\n",
    "    test_dataset = get_cifar10_labeled(path_cifar10, train=False, transform=augmentation_test, download=True)\n",
    "\n",
    "    print (f\"Labeled: {len(train_labeled_index)} Unlabeled: {len(train_unlabeled_index)} Validation: {len(valid_index)}\")\n",
    "\n",
    "    # Define DataLoader for pytorch\n",
    "    train_loader_labeled = data.DataLoader(train_labeled_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "    train_loader_unlabeled = data.DataLoader(train_unlabeled_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "    valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Define Model (WideResNet)\n",
    "    student_model = models.WideResNet(num_classes=10).cuda()\n",
    "    teacher_model = models.WideResNet(num_classes=10).cuda()\n",
    "\n",
    "    for param in teacher_model.parameters():\n",
    "        param.detach_()\n",
    "\n",
    "\n",
    "    train_criterion = Each_Loss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "    # if is_ema is True:\n",
    "    ema_optimizer= exponential_moving_average(student_model, teacher_model, alpha=ema_decay)\n",
    "\n",
    "    # Load Model & Logger Setting\n",
    "    title = 'MixMatch Semi-Supervised Learning'\n",
    "    if is_continue_train is True:\n",
    "        print(' > Load Checkpoint')\n",
    "        output_folder = os.path.dirname(load_model)\n",
    "        checkpoint = torch.load(load_model)\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        student_model.load_state_dict(checkpoint['state_dict'])\n",
    "        teacher_model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logger = Logger(os.path.join(output_folder, 'log.txt'), title=title, resume=True)\n",
    "    else:\n",
    "        logger = Logger(os.path.join(output_folder, 'log.txt'), title=title)\n",
    "        logger.set_names(['Train Loss', 'Train Loss X', 'Train Loss U',  'Valid Loss', 'Valid Acc.', 'Test Loss', 'Test Acc.'])\n",
    "\n",
    "    test_accs = []\n",
    "\n",
    "\n",
    "    print('2. Start to train the MixMatch Model')\n",
    "    ####################################################\n",
    "    # Train and Validation \n",
    "    ####################################################\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        print(f'Epoch: {epoch+1} / {epochs}\\n')\n",
    "\n",
    "        if is_ema is True:\n",
    "            train_loss, train_loss_x, train_loss_u = train(train_loader_labeled, train_loader_unlabeled, student_model, optimizer, ema_optimizer, train_criterion, epoch, use_cuda, True)\n",
    "            _, train_acc = validate(train_loader_labeled, teacher_model, criterion, use_cuda, mode='Train Accuracy')\n",
    "            val_loss, val_acc = validate(valid_loader, teacher_model, criterion, use_cuda, mode='Valid Accuracy')\n",
    "            test_loss, test_acc = validate(test_loader, teacher_model, criterion, use_cuda, mode='Test Accuracy')\n",
    "        else:\n",
    "            train_loss, train_loss_x, train_loss_u = train(train_loader_labeled, train_loader_unlabeled, student_model, optimizer, ema_optimizer, train_criterion, epoch, use_cuda, False)\n",
    "            _, train_acc = validate(train_loader_labeled, student_model, criterion, use_cuda, mode='Train Accuracy')\n",
    "            val_loss, val_acc = validate(valid_loader, student_model, criterion, use_cuda, mode='Valid Accuracy')\n",
    "            test_loss, test_acc = validate(test_loader, student_model, criterion, use_cuda, mode='Test Accuracy')\n",
    "\n",
    "        # append logger file\n",
    "        logger.append([train_loss, train_loss_x, train_loss_u, val_loss, val_acc, test_loss, test_acc])\n",
    "\n",
    "        # save model\n",
    "        is_best = val_acc > best_acc\n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': student_model.state_dict(),\n",
    "                'ema_state_dict': teacher_model.state_dict(),\n",
    "                'acc': val_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best)\n",
    "        test_accs.append(test_acc)\n",
    "    logger.close()\n",
    "\n",
    "    print('Best acc:')\n",
    "    print(best_acc)\n",
    "\n",
    "    print('Mean acc:')\n",
    "    print(np.mean(test_accs[-20:]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_python_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78d04299e464758119aa473303693f33db2a1bc5c94011f00bbd9c1618e77f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
