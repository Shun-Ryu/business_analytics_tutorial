{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ MixMatch : A Holistic Approach Semi-Super Vised Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import models.wideresnet as models\n",
    "from utils.logger import *\n",
    "from utils.misc import *\n",
    "from utils.accuracy import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyper-Parameters\n",
    "\n",
    "Hyper-Parameterë¥¼ ì •ì˜í•´ ë‘ . ëŒ€ë¶€ë¶„ì€ Original Paperì— ê¸°ë°˜ìœ¼ë¡œ ì •ì˜ ë˜ì–´ ìˆìŒ. ì‚¬ìš©ìê°€ ì¡°ì ˆí• ë§Œí•œ Hyper-Parameterë“¤ì€ ì•„ë˜ì™€ ê°™ìŒ.\n",
    "\n",
    "- epochs : í•™ìŠµì‹œí‚¬ epochs ì§€ì •\n",
    "- batch_size : batch í¬ê¸°.\n",
    "- random_seed : random seedê°’ ì„¤ì •.\n",
    "- lr : learning rate. batch_sizeì— ë”°ë¼ì„œ ì¡°ì ˆ í•„ìš”. batch_sizeê°€ kë°° ì»¤ì§€ë§Œ lrì€ sqrt(k)ë°° ë§Œí¼ í¬ê²Œ ë§Œë“¤ë©´ ì¢‹ìŒ\n",
    "- is_continue_train : ì´ì „ saved best modelì„ ë¶ˆëŸ¬ì™€ì„œ ì´ì–´ì„œ í•™ìŠµí• ì§€ ì—¬ë¶€ ì„¸íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.75\n",
    "lambda_u = 75\n",
    "# train_iteration = 1024\n",
    "# train_iteration = 256 \n",
    "train_iteration = 512 \n",
    "ema_decay = 0.999\n",
    "Temperature = 0.5\n",
    "\n",
    "is_ema = True \n",
    "is_continue_train = False \n",
    "start_epoch = 0\n",
    "epochs = 1000\n",
    "# num_labeled = 500 \n",
    "num_labeled = 250 \n",
    "\n",
    "\n",
    "use_interleaving = False # Interleavingì„ Falseë¡œ í•  ê²½ìš°, BatchsizeëŠ” 3ë°°ë¡œ ëŠ˜ì–´ë‚˜ë¯€ë¡œ sqrt(3)ë°°ë¥¼ lrì— ê³±í•´ì•¼ í•¨\n",
    "\n",
    "# batch_size = 64 # Original Batch Size\n",
    "batch_size = 128 # x2ë°° Batch Size\n",
    "\n",
    "# lr = 0.002 # Original lr (interleaving), batch_size = 64\n",
    "# lr = 0.00346 # sqrt('batch size') * 'Original lr' (non-interleaving), batch_size = 64\n",
    "lr = 0.0049 # sqrt('batch size') * 'Original lr' (non-interleaving), batch_size = 128 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(512)\n",
    "\n",
    "# best_acc = 0  # best test accuracy\n",
    "\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465) \n",
    "CIFAR10_STD = (0.2471, 0.2435, 0.2616)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functions for Augmentation \n",
    "\n",
    "Data Augmentationì„ ìœ„í•œ Functionë“¤ì„ ëª¨ì•„ë‘ . Pytorch Data Loaderë¥¼ í†µí•´ Dataë¥¼ Loadingí•  ë•Œ ë§ˆë‹¤ Real-Timeìœ¼ë¡œ Augmentationì„ ì§„í–‰í•¨\n",
    "\n",
    "- Image Normalization (Standard Scaling)\n",
    "- Image Transpose (from CIFAR-10 shape to Pytorch Shape)\n",
    "- Augmentation Method #1 : Image Padding & Cropping\n",
    "- Augmentation Method #2 : Image Horizontal Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Normalization (Standard Scaling)\n",
    "def normalize(x, mean=CIFAR10_MEAN, std=CIFAR10_STD):\n",
    "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
    "    x -= mean*255\n",
    "    x *= 1.0/(255*std)\n",
    "    return x\n",
    "\n",
    "# Transpose Image Shape for Pytorch shape from CIFAR-10 Original shape\n",
    "def transpose(x, source='NHWC', target='NCHW'):\n",
    "    return x.transpose([source.index(d) for d in target]) \n",
    "\n",
    "# Padding to image borders for cropping image\n",
    "def pad(x, border=4):\n",
    "    return np.pad(x, [(0, 0), (border, border), (border, border)], mode='reflect')\n",
    "\n",
    "# Augmentation Method 1 : Random Padding and Cropping \n",
    "class RandomPadandCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = pad(x, 4)\n",
    "\n",
    "        h, w = x.shape[1:]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        x = x[:, top: top + new_h, left: left + new_w]\n",
    "\n",
    "        return x\n",
    "\n",
    "# Augmentation Method 2 : Random Horizontal Flipping \n",
    "class RandomFlip(object):\n",
    "    def __call__(self, x):\n",
    "        if np.random.rand() < 0.5:\n",
    "            x = x[:, :, ::-1]\n",
    "\n",
    "        return x.copy()\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, x):\n",
    "        x = torch.from_numpy(x)\n",
    "        return x\n",
    "\n",
    "def split_dataset(labels, num_train):\n",
    "    labels = np.array(labels)\n",
    "    index_train_labeled = []\n",
    "    index_train_unlabeled = []\n",
    "    index_valid = []\n",
    "\n",
    "    num_labels = len(np.unique(labels))\n",
    "    num_labeled_per_class = int(num_train / num_labels)\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        index = np.where(labels == i)[0]\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        index_train_labeled.extend(index[:num_labeled_per_class])\n",
    "        index_train_unlabeled.extend(index[num_labeled_per_class:-500])\n",
    "        index_valid.extend(index[-500:])\n",
    "\n",
    "    np.random.shuffle(index_train_labeled)\n",
    "    np.random.shuffle(index_train_unlabeled)\n",
    "    np.random.shuffle(index_valid)\n",
    "\n",
    "    return index_train_labeled, index_train_unlabeled, index_valid\n",
    "\n",
    "# Twice Augmentation for Unlabeld Image (eg. K=2)\n",
    "class Multi_Augmentation:\n",
    "    def __init__(self, transform_method):\n",
    "        self.transform_method = transform_method\n",
    "        # self.num_transform = num_transform\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        aug_out_1 = self.transform_method(inp)\n",
    "        aug_out_2 = self.transform_method(inp)\n",
    "        return aug_out_1, aug_out_2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions for loading CIFAR-10 dataset \n",
    "\n",
    "Pytorchì˜ dataloaderì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜ë“¤. Dataë¥¼ ë¶ˆëŸ¬ì˜¤ë©´ì„œ augmentationì„ ìˆ˜í–‰í•´ ì¤Œ. unlabeled dataì˜ augmentationíšŸìˆ˜ëŠ” original paperì— ë”°ë¼ K=2ë¡œ ë§ì¶°ì„œ 2ë²ˆ augmentationìˆ˜í–‰í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 Labeled Images with Random Augmentation\n",
    "class get_cifar10_labeled(torchvision.datasets.CIFAR10):\n",
    "\n",
    "    def __init__(self, path_cifar10, indexs=None, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(get_cifar10_labeled, self).__init__(path_cifar10, train=train,\n",
    "                 transform=transform, target_transform=target_transform,\n",
    "                 download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "        self.data = transpose(normalize(self.data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_x, target_y = self.data[index], self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data_x = self.transform(data_x)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target_y = self.target_transform(target_y)\n",
    "\n",
    "        return data_x, target_y\n",
    "    \n",
    "\n",
    "# Load CIFAR-10 Unlabeled Images with Random Augmentation (K=2)\n",
    "class get_cifar10_unlabeled(get_cifar10_labeled):\n",
    "\n",
    "    def __init__(self, path_cifar10, indexs, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(get_cifar10_unlabeled, self).__init__(path_cifar10, indexs, train=train,\n",
    "                 transform=transform, target_transform=target_transform,\n",
    "                 download=download)\n",
    "        self.targets = np.array([-1 for i in range(len(self.targets))])\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functions for MixMatch learning method\n",
    "\n",
    "MixMatchë¥¼ í•™ìŠµí•  ë•Œ í•„ìš”í•œ ì¤‘ìš”í•œ ê¸°ëŠ¥ë“¤ì„ Functionìœ¼ë¡œ ì •ì˜í•´ ë‘ \n",
    "- Ramp-Up Function (weight balancing betweein supervised and unsupervised loss)\n",
    "- Calculation of semi-supervised loss (Each_Loss)\n",
    "- Exponential Moving Average(EMA) Function\n",
    "- Interleaving Functions : ì´ë²ˆ Tutorialì—ì„œ í™•ì¸í•  í•¨ìˆ˜. MixUpëœ Labeled Dataì™€ Unlabeled Dataë¥¼ ì„ì–´ì¤Œ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ramp-up Function for balancing the weight between supervised loss and unsupervised loss\n",
    "# - loss_total = loss_l + weight * loss_u\n",
    "def ramp_up(current, rampup_length=epochs):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)\n",
    "\n",
    "class Each_Loss(object):\n",
    "    def __call__(self, pred_l, target_l, pred_u, target_u, epoch):\n",
    "        probs_u = torch.softmax(pred_u, dim=1)\n",
    "\n",
    "        loss_l = -torch.mean(torch.sum(F.log_softmax(pred_l, dim=1) * target_l, dim=1))\n",
    "        loss_u = torch.mean((probs_u - target_u)**2)\n",
    "\n",
    "        return loss_l, loss_u, lambda_u * ramp_up(epoch)\n",
    "\n",
    "class exponential_moving_average(object):\n",
    "    def __init__(self, student_model, teacher_model, alpha=0.999):\n",
    "        self.model = student_model\n",
    "        self.ema_model = teacher_model\n",
    "        self.alpha = alpha\n",
    "        self.student_params = list(student_model.state_dict().values())\n",
    "        self.teacher_params = list(teacher_model.state_dict().values())\n",
    "        self.wd = 0.02 * lr\n",
    "\n",
    "        for param, ema_param in zip(self.student_params, self.teacher_params):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        one_minus_alpha = 1.0 - self.alpha\n",
    "        for student_param, teacher_param in zip(self.student_params, self.teacher_params):\n",
    "            if teacher_param.dtype==torch.float32:\n",
    "                teacher_param.mul_(self.alpha)\n",
    "                teacher_param.add_(student_param * one_minus_alpha)\n",
    "                # customized weight decay\n",
    "                student_param.mul_(1 - self.wd)\n",
    "\n",
    "def interleave_offsets(batch, nu):\n",
    "    groups = [batch // (nu + 1)] * (nu + 1)\n",
    "    for x in range(batch - sum(groups)):\n",
    "        groups[-x - 1] += 1\n",
    "    offsets = [0]\n",
    "    for g in groups:\n",
    "        offsets.append(offsets[-1] + g)\n",
    "    assert offsets[-1] == batch\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def interleave(xy, batch):\n",
    "    nu = len(xy) - 1\n",
    "    offsets = interleave_offsets(batch, nu)\n",
    "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
    "    for i in range(1, nu + 1):\n",
    "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
    "    return [torch.cat(v, dim=0) for v in xy]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Utility Functions (Tedious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='./Results/', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Function\n",
    "\n",
    "MixMatchêµ¬í˜„ì— ìˆì–´ì„œ ê°€ì¥ ì¤‘ìš”í•œ í•¨ìˆ˜ì´ë‹¤. ì´ í•¨ìˆ˜ì—ì„œ MixMatchì˜ ëª¨ë“  ê¸°ë²•ì´ ì‚¬ìš©ë˜ë©°, ì•Œê³ ë¦¬ì¦˜ì˜ SequenceëŒ€ë¡œ êµ¬í˜„ì´ ë˜ì–´ìˆë‹¤.\n",
    "ë˜í•œ ì´ í•¨ìˆ˜ì— Interleavingì´ ì¡´ì¬í•œë‹¤. Batch-Normalizationì„ ê³ ë ¤í•˜ì—¬, Supervised Logitsê³¼ Unsupervised Logitsì„ Modelì— ê°ê° íƒœìš°ë©´ Interleavingì´ í•„ìš”í•˜ë©°,\n",
    "ê·¸ë ‡ì§€ ì•Šê³  í•œë²ˆì— Modelì— íƒœì›Œ ê³„ì‚° í•  ê²½ìš° Interleavingì€ í•„ìš”í•˜ì§€ ì•Šë‹¤.\n",
    "\n",
    "- 1. Data Augmentation\n",
    "- 2. Label Guessing and Label Sharpening\n",
    "- 3. MixUp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, each_loss, epoch, use_cuda, is_ema=False):\n",
    "\n",
    "    losses_total = AverageMeter()\n",
    "    losses_l = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    weights = AverageMeter()\n",
    "\n",
    "\n",
    "    iter_train_labeled = iter(labeled_trainloader)\n",
    "    iter_train_unlabeled = iter(unlabeled_trainloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch_index in range(train_iteration):\n",
    "        #########################################\n",
    "        # 1. Data Augmentation\n",
    "        #########################################\n",
    "        try:\n",
    "            inputs_l, targets_l = next(iter_train_labeled)\n",
    "        except:\n",
    "            iter_train_labeled = iter(labeled_trainloader)\n",
    "            inputs_l, targets_l = next(iter_train_labeled)\n",
    "\n",
    "\n",
    "        try:\n",
    "            (inputs_u, inputs_u2), _ = next(iter_train_unlabeled)\n",
    "        except:\n",
    "            iter_train_unlabeled = iter(unlabeled_trainloader)\n",
    "            (inputs_u, inputs_u2), _ = next(iter_train_unlabeled)\n",
    "\n",
    "        # Transform label to one-hot\n",
    "        batch_size = inputs_l.size(0)\n",
    "        targets_l = torch.zeros(batch_size, 10).scatter_(1, targets_l.view(-1,1).long(), 1)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs_l, targets_l = inputs_l.cuda(), targets_l.cuda(non_blocking=True)\n",
    "            inputs_u = inputs_u.cuda()\n",
    "            inputs_u2 = inputs_u2.cuda()\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        # 2. Label Guessing & Label Sharpening \n",
    "        #########################################\n",
    "        with torch.no_grad():\n",
    "            # compute guessed labels of unlabel samples\n",
    "            outputs_u = model(inputs_u)\n",
    "            outputs_u2 = model(inputs_u2)\n",
    "            p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "            pt = p**(1/Temperature)\n",
    "            targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "            targets_u = targets_u.detach()\n",
    "\n",
    "        #########################################\n",
    "        # 3. MixUp \n",
    "        #########################################\n",
    "        all_inputs = torch.cat([inputs_l, inputs_u, inputs_u2], dim=0)\n",
    "        all_targets = torch.cat([targets_l, targets_u, targets_u], dim=0)\n",
    "\n",
    "        Lambda_ = np.random.beta(alpha, alpha)\n",
    "\n",
    "        Lambda_ = max(Lambda_, 1-Lambda_)\n",
    "\n",
    "        idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "        input_a, input_b = all_inputs, all_inputs[idx]\n",
    "        target_a, target_b = all_targets, all_targets[idx]\n",
    "\n",
    "        mixed_input = Lambda_ * input_a + (1 - Lambda_) * input_b\n",
    "        mixed_target = Lambda_ * target_a + (1 - Lambda_) * target_b\n",
    "\n",
    "\n",
    "        ########################################\n",
    "        # 4. Interleaving or No-Interleaving\n",
    "        ########################################\n",
    "        if use_interleaving:\n",
    "            # 1) interleave labeled and unlabed images between batches to get correct batchnorm calculation \n",
    "            mixed_input = list(torch.split(mixed_input, batch_size))\n",
    "            mixed_input = interleave(mixed_input, batch_size)\n",
    "\n",
    "            # 2) labeled prediction\n",
    "            logits = [model(mixed_input[0])]\n",
    "            \n",
    "            # 3) unlabeled prediction\n",
    "            for input in mixed_input[1:]:\n",
    "                logits.append(model(input))\n",
    "\n",
    "            # 4) de_interleave to calculate labeled supervised loss and unlabeld unsupervised loss properly \n",
    "            logits = interleave(logits, batch_size)\n",
    "\n",
    "            logits_l = logits[0]\n",
    "            logits_u = torch.cat(logits[1:], dim=0)\n",
    "        else:\n",
    "            # No Interleaving and calculate both labeled and unlabeled sample.\n",
    "            # The model is used only once to predict logits. So A calculation of batchnorm is proper.\n",
    "            # But if you want to use this no-interleaving method then you should adjust the learning rate (with multiply sqrt(k))\n",
    "            # k means k-times of increased batch size\n",
    "            logits = model(mixed_input)\n",
    "            split_logits = list(torch.split(logits, batch_size))\n",
    "            logits_l = split_logits[0]\n",
    "            logits_u = torch.cat(split_logits[1:], dim=0)\n",
    "\n",
    "\n",
    "        ########################################\n",
    "        # 5. Semi-Supervised Loss \n",
    "        ########################################\n",
    "        loss_l, loss_u, weight = each_loss(logits_l, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_index/train_iteration)\n",
    "\n",
    "        loss_total = loss_l + weight * loss_u\n",
    "\n",
    "        # record loss\n",
    "        losses_total.update(loss_total.item(), inputs_l.size(0))\n",
    "        losses_l.update(loss_l.item(), inputs_l.size(0))\n",
    "        losses_u.update(loss_u.item(), inputs_l.size(0))\n",
    "        weights.update(weight, inputs_l.size(0))\n",
    "\n",
    "        ########################################\n",
    "        # 6. Backpropagation \n",
    "        ########################################\n",
    "        optimizer.zero_grad()\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        ########################################\n",
    "        # 7. EMA Learning for Teacher Model \n",
    "        ########################################\n",
    "        if is_ema is True:\n",
    "            ema_optimizer.step()\n",
    "\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'{batch_index+1}/{train_iteration} : Total Loss {losses_total.avg:.3f}')\n",
    "\n",
    "    return (losses_total.avg, losses_l.avg, losses_u.avg,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valloader, model, criterion, use_cuda, mode):\n",
    "    losses_total = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, targets) in enumerate(valloader):\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            ori_targets = targets\n",
    "            ori_targets = ori_targets.cuda(non_blocking=True)\n",
    "\n",
    "            targets = torch.zeros(batch_size, 10).scatter_(1, targets.view(-1,1).long(), 1)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # accuracy top-1 and top-5\n",
    "            prec1, prec5 = accuracy(outputs, ori_targets, topk=(1, 1))\n",
    "\n",
    "            losses_total.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "\n",
    "            if batch_index % 10 == 0:\n",
    "                print(f'{batch_index+1}/{train_iteration} : Total Loss {losses_total.avg:.3f}, Top1 Acc {top1.avg:.3f}')\n",
    "    return (losses_total.avg, top1.avg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ğŸŒŠMain - Start to Train the MixMatch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Load Dataset (CIFAR-10)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Labeled: 250 Unlabeled: 44750 Validation: 5000\n",
      "Epoch: 1 / 1000\n",
      "\n",
      "1/512 : Total Loss 2.300\n",
      "11/512 : Total Loss 2.187\n",
      "21/512 : Total Loss 2.090\n",
      "31/512 : Total Loss 2.017\n",
      "41/512 : Total Loss 1.946\n",
      "51/512 : Total Loss 1.902\n",
      "61/512 : Total Loss 1.865\n",
      "71/512 : Total Loss 1.833\n",
      "81/512 : Total Loss 1.761\n",
      "91/512 : Total Loss 1.760\n",
      "101/512 : Total Loss 1.723\n",
      "111/512 : Total Loss 1.708\n",
      "121/512 : Total Loss 1.680\n",
      "131/512 : Total Loss 1.662\n",
      "141/512 : Total Loss 1.659\n",
      "151/512 : Total Loss 1.653\n",
      "161/512 : Total Loss 1.646\n",
      "171/512 : Total Loss 1.634\n",
      "181/512 : Total Loss 1.620\n",
      "191/512 : Total Loss 1.616\n",
      "201/512 : Total Loss 1.604\n",
      "211/512 : Total Loss 1.583\n",
      "221/512 : Total Loss 1.574\n",
      "231/512 : Total Loss 1.548\n",
      "241/512 : Total Loss 1.530\n",
      "251/512 : Total Loss 1.510\n",
      "261/512 : Total Loss 1.493\n",
      "271/512 : Total Loss 1.488\n",
      "281/512 : Total Loss 1.476\n",
      "291/512 : Total Loss 1.463\n",
      "301/512 : Total Loss 1.453\n",
      "311/512 : Total Loss 1.448\n",
      "321/512 : Total Loss 1.440\n",
      "331/512 : Total Loss 1.421\n",
      "341/512 : Total Loss 1.420\n",
      "351/512 : Total Loss 1.413\n",
      "361/512 : Total Loss 1.400\n",
      "371/512 : Total Loss 1.384\n",
      "381/512 : Total Loss 1.369\n",
      "391/512 : Total Loss 1.357\n",
      "401/512 : Total Loss 1.354\n",
      "411/512 : Total Loss 1.349\n",
      "421/512 : Total Loss 1.345\n",
      "431/512 : Total Loss 1.339\n",
      "441/512 : Total Loss 1.336\n",
      "451/512 : Total Loss 1.331\n",
      "461/512 : Total Loss 1.321\n",
      "471/512 : Total Loss 1.319\n",
      "481/512 : Total Loss 1.316\n",
      "491/512 : Total Loss 1.313\n",
      "501/512 : Total Loss 1.300\n",
      "511/512 : Total Loss 1.297\n",
      "1/512 : Total Loss 1.940, Top1 Acc 32.812\n",
      "1/512 : Total Loss 2.104, Top1 Acc 21.875\n",
      "11/512 : Total Loss 2.107, Top1 Acc 24.219\n",
      "21/512 : Total Loss 2.112, Top1 Acc 24.368\n",
      "31/512 : Total Loss 2.112, Top1 Acc 24.345\n",
      "1/512 : Total Loss 2.011, Top1 Acc 29.688\n",
      "11/512 : Total Loss 2.080, Top1 Acc 26.847\n",
      "21/512 : Total Loss 2.087, Top1 Acc 27.046\n",
      "31/512 : Total Loss 2.087, Top1 Acc 26.689\n",
      "41/512 : Total Loss 2.083, Top1 Acc 26.829\n",
      "51/512 : Total Loss 2.085, Top1 Acc 26.593\n",
      "61/512 : Total Loss 2.081, Top1 Acc 26.844\n",
      "71/512 : Total Loss 2.083, Top1 Acc 26.926\n",
      "Epoch: 2 / 1000\n",
      "\n",
      "1/512 : Total Loss 1.391\n",
      "11/512 : Total Loss 1.025\n",
      "21/512 : Total Loss 1.004\n",
      "31/512 : Total Loss 1.088\n",
      "41/512 : Total Loss 1.030\n",
      "51/512 : Total Loss 1.066\n",
      "61/512 : Total Loss 1.075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mmean(test_accs[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]))\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 121\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn [23], line 87\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m is_ema \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     train_loss, train_loss_x, train_loss_u \u001b[39m=\u001b[39m train(train_loader_labeled, train_loader_unlabeled, student_model, optimizer, ema_optimizer, train_criterion, epoch, use_cuda, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     88\u001b[0m     _, train_acc \u001b[39m=\u001b[39m validate(train_loader_labeled, teacher_model, criterion, use_cuda, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     89\u001b[0m     val_loss, val_acc \u001b[39m=\u001b[39m validate(valid_loader, teacher_model, criterion, use_cuda, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValid Accuracy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [20], line 127\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, each_loss, epoch, use_cuda, is_ema)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m########################################\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# 7. EMA Learning for Teacher Model \u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m########################################\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m is_ema \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     ema_optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m batch_index \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    131\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mbatch_index\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtrain_iteration\u001b[39m}\u001b[39;00m\u001b[39m : Total Loss \u001b[39m\u001b[39m{\u001b[39;00mlosses_total\u001b[39m.\u001b[39mavg\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [18], line 38\u001b[0m, in \u001b[0;36mexponential_moving_average.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m teacher_param\u001b[39m.\u001b[39madd_(student_param \u001b[39m*\u001b[39m one_minus_alpha)\n\u001b[0;32m     37\u001b[0m \u001b[39m# customized weight decay\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m student_param\u001b[39m.\u001b[39;49mmul_(\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwd)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global start_epoch\n",
    "    best_acc = 0\n",
    "\n",
    "    output_folder = './Results/'\n",
    "    path_cifar10 = './data'\n",
    "\n",
    "    if is_continue_train == True:\n",
    "        load_model = output_folder+'model_best.pth.tar'\n",
    "\n",
    "\n",
    "    if not os.path.isdir(output_folder):\n",
    "        mkdir_p(output_folder)\n",
    "\n",
    "    print('1. Load Dataset (CIFAR-10)')\n",
    "    augmentation_train = transforms.Compose([\n",
    "        RandomPadandCrop(32), # 32x32 Random Cropping\n",
    "        RandomFlip(), # Random Horizontal Flipping\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "    augmentation_test = transforms.Compose([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Preparing CIFAR-10 Dataset (train, valid, test sets)\n",
    "    cifar10_dataset = torchvision.datasets.CIFAR10(path_cifar10, train=True, download=True)\n",
    "    train_labeled_index, train_unlabeled_index, valid_index = split_dataset(cifar10_dataset.targets, num_labeled)\n",
    "\n",
    "    train_labeled_dataset = get_cifar10_labeled(path_cifar10, train_labeled_index, train=True, transform=augmentation_train)\n",
    "    train_unlabeled_dataset = get_cifar10_unlabeled(path_cifar10, train_unlabeled_index, train=True, transform=Multi_Augmentation(augmentation_train))\n",
    "    valid_dataset = get_cifar10_labeled(path_cifar10, valid_index, train=True, transform=augmentation_test, download=True)\n",
    "    test_dataset = get_cifar10_labeled(path_cifar10, train=False, transform=augmentation_test, download=True)\n",
    "\n",
    "    print (f\"Labeled: {len(train_labeled_index)} Unlabeled: {len(train_unlabeled_index)} Validation: {len(valid_index)}\")\n",
    "\n",
    "    # Define DataLoader for pytorch\n",
    "    train_loader_labeled = data.DataLoader(train_labeled_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "    train_loader_unlabeled = data.DataLoader(train_unlabeled_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "    valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Define Model (WideResNet)\n",
    "    student_model = models.WideResNet(num_classes=10).cuda()\n",
    "    teacher_model = models.WideResNet(num_classes=10).cuda()\n",
    "\n",
    "    for param in teacher_model.parameters():\n",
    "        param.detach_()\n",
    "\n",
    "\n",
    "    train_criterion = Each_Loss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "    # if is_ema is True:\n",
    "    ema_optimizer= exponential_moving_average(student_model, teacher_model, alpha=ema_decay)\n",
    "\n",
    "    # Load Model & Logger Setting\n",
    "    title = 'MixMatch Semi-Supervised Learning'\n",
    "    if is_continue_train is True:\n",
    "        print(' > Load Checkpoint')\n",
    "        output_folder = os.path.dirname(load_model)\n",
    "        checkpoint = torch.load(load_model)\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        student_model.load_state_dict(checkpoint['state_dict'])\n",
    "        teacher_model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logger = Logger(os.path.join(output_folder, 'log.txt'), title=title, resume=True)\n",
    "    else:\n",
    "        logger = Logger(os.path.join(output_folder, 'log.txt'), title=title)\n",
    "        logger.set_names(['Train Loss', 'Train Loss X', 'Train Loss U',  'Valid Loss', 'Valid Acc.', 'Test Loss', 'Test Acc.'])\n",
    "\n",
    "    test_accs = []\n",
    "\n",
    "\n",
    "    print('2. Start to train the MixMatch Model')\n",
    "    ####################################################\n",
    "    # Train and Validation \n",
    "    ####################################################\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        print(f'Epoch: {epoch+1} / {epochs}\\n')\n",
    "\n",
    "        if is_ema is True:\n",
    "            train_loss, train_loss_x, train_loss_u = train(train_loader_labeled, train_loader_unlabeled, student_model, optimizer, ema_optimizer, train_criterion, epoch, use_cuda, True)\n",
    "            _, train_acc = validate(train_loader_labeled, teacher_model, criterion, use_cuda, mode='Train Accuracy')\n",
    "            val_loss, val_acc = validate(valid_loader, teacher_model, criterion, use_cuda, mode='Valid Accuracy')\n",
    "            test_loss, test_acc = validate(test_loader, teacher_model, criterion, use_cuda, mode='Test Accuracy')\n",
    "        else:\n",
    "            train_loss, train_loss_x, train_loss_u = train(train_loader_labeled, train_loader_unlabeled, student_model, optimizer, ema_optimizer, train_criterion, epoch, use_cuda, False)\n",
    "            _, train_acc = validate(train_loader_labeled, student_model, criterion, use_cuda, mode='Train Accuracy')\n",
    "            val_loss, val_acc = validate(valid_loader, student_model, criterion, use_cuda, mode='Valid Accuracy')\n",
    "            test_loss, test_acc = validate(test_loader, student_model, criterion, use_cuda, mode='Test Accuracy')\n",
    "\n",
    "        # append logger file\n",
    "        logger.append([train_loss, train_loss_x, train_loss_u, val_loss, val_acc, test_loss, test_acc])\n",
    "\n",
    "        # save model\n",
    "        is_best = val_acc > best_acc\n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': student_model.state_dict(),\n",
    "                'ema_state_dict': teacher_model.state_dict(),\n",
    "                'acc': val_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best)\n",
    "        test_accs.append(test_acc)\n",
    "    logger.close()\n",
    "\n",
    "    print('Best acc:')\n",
    "    print(best_acc)\n",
    "\n",
    "    print('Mean acc:')\n",
    "    print(np.mean(test_accs[-20:]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_python_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78d04299e464758119aa473303693f33db2a1bc5c94011f00bbd9c1618e77f98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
