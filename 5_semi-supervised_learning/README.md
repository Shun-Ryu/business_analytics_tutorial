# ğŸ¤”MixMatchë¥¼ êµ¬í˜„í•  ë•Œ ìƒê°í•´ë´ì•¼ í•˜ëŠ” ê²ƒë“¤

## The things to consider for implementing MixMatch Algorithm

![image-20221226195436600](./attachments/image-20221226195436600.png)



ğŸ”¥ì´ë²ˆ Tutorialì—ì„œëŠ” **Semi-Supervised Learning Method** ì¤‘, **Holisticí•œ ì ‘ê·¼ ë°©ë²•ì¸ MixMatch**ë¥¼ êµ¬í˜„í•´ë³´ë©´ì„œ, **êµ¬í˜„ ìƒì— ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤**ì— ëŒ€í•´ì„œ ìƒê°í•´ ë³´ëŠ” ì‹œê°„ì„ ê°–ìœ¼ë ¤ê³  í•œë‹¤. Githubì— ìˆëŠ” ì—¬ëŸ¬ê°€ì§€ êµ¬í˜„ì²´ë“¤ì„ í™•ì¸í•˜ì˜€ëŠ”ë°, ë…¼ë¬¸ì—ëŠ” ë“œëŸ¬ë‚˜ì§€ ì•Šì€ ì‚¬í•­ë“¤ì´ ë³´ì´ê¸°ì— ì´ Tutorialì—ì„œ ì—¬ëŸ¬ê°€ì§€ í…ŒìŠ¤íŠ¸ì™€ í•¨ê»˜ In-Depthí•˜ê²Œ ì•Œì•„ë³´ê³ ì í•œë‹¤.

ì´ëŠ” ì‚¬ì‹¤ MixMatch ë¿ë§Œ ì•„ë‹ˆë¼, Original MixMatch (from Google) ì•Œê³ ë¦¬ì¦˜ì—ì„œ íŒŒìƒë˜ì–´ ë‚˜ì˜¨ **FixMatch**ì—ì„œë„ ë™ì¼í•œ êµ¬í˜„ìƒ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ì‚¬í•­ì´ë¯€ë¡œ, ì´ë²ˆ Tutorialì„ í†µí•´ ì˜ ë°°ì›Œë³´ëŠ” ì‹œê°„ì„ ê°–ë„ë¡ í•˜ì.

ë…¼ë¬¸ì—ì„œë„ ì˜ ë‹¤ë£¨ì§€ ì•Šê³ , êµ¬í˜„ì²´ì—ì„œë„ ì˜ ì„¤ëª…ì´ ì—†ëŠ” ì•„ë˜ì˜ 2ê°€ì§€ ì£¼ì œë¡œ Tutorialì„ ì „ê°œí•˜ë ¤ í•œë‹¤. ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ëŒ€ë¶€ë¶„ ê¹Œë‹¤ë¡­ì§€ ì•Šë‹¤.

- MixMatchì—ì„œ **EMA(Exponential Moving Average)ë¡œ Teacherëª¨ë¸**ì„ ë§Œë“œëŠ” ê²ƒì´ ì¤‘ìš”í•œê°€? 
- MixMatchì—ì„œ **Interleaving** êµ¬í˜„ì€ ì¤‘ìš”í•œê°€?



# Table of Contents

- [Background of MixMatch](#Background-of-MixMatch)

  - [1. Data Augmentation](#1-Data-Augmentation)
  - [2. Label Guessing and Label Sharpening](#2-Label-Guessing-&-Label-Sharpening)
  - [3. MixUp](#3-MixUp)

- [Tutorial. Deep Understanding of MixMatch Implementation](#Tutorial-Deep-Understanding-of-MixMatch-Implementation)

  - [1. Tutorial Notebook](#1-Tutorial-Notebook)
  - [2. Setting](#2-Setting)
  - [3. Usage Code](#3-Usage-Code)
  - [4. Result (Accuracy)](#4-Result_Accuracy)

- [Final Insights](#Final-Insights)

- [Conclusion](#Conclusion)

- [References](#References)

  

-------

# Background of MixMatch

## 1. Basic Concept

MixMatchëŠ” ê¸°ì¡´ì˜ Consistency Regularizationê³¼ Pseudo-Labelingê³¼ ê°™ì€ ê¸°ë²•ì—ì„œ ë²—ì–´ë‚˜ì„œ, ê¸°ì¡´ì˜ ë°©ë²•ë¡ ë“¤ì„ ì—¬ëŸ¬ê°œë¥¼ ê²°í•©í•˜ì—¬ Holistic(ì „ì²´ë¡ ì )ì¸ Approachë¡œ ì ‘ê·¼í•œ ìµœì´ˆì˜ ë…¼ë¬¸ì´ë‹¤. (from Google Research). ì§€ê¸ˆì€ ì˜ ëª¨ë¥¼ìˆ˜ë„ ìˆìœ¼ë‚˜, ì²˜ìŒ ì´ ë°©ì‹ì´ ë‚˜ì™”ì„ ë•Œ, ê¸°ì¡´ì˜ ë°©ë²•ë¡ ë“¤ì„ ì••ë„í•˜ëŠ” Performanceë¡œ ë§ì€ ì´ë“¤ì—ê²Œ ì‹ ì„ í•œ ì¶©ê²©ì„ ì„ ì‚¬í•˜ì˜€ë˜ ë…¼ë¬¸ì´ë‹¤. 



**MixMatchì—ì„œ ì°¨ìš©í•˜ê³  ìˆëŠ” ì „ì²´ì ì¸ ë°©ë²•ë¡ ë“¤ì€ ì•„ë˜ì™€ ê°™ë‹¤.**

1. Data Augmentation 
2. Label Guessing & Label Sharpening
3. MixUp



ìœ„ì˜ ê²ƒë“¤ì€ ëª¨ë‘ ê³¼ê±°ì˜ ë…¼ë¬¸ë“¤ì—ì„œ ë§ì´ ë³´ì•„ì™”ë˜ ê¸°ë²•ë“¤ì¸ë°, ì´ëŸ¬í•œ **ì—¬ëŸ¬ê¸°ë²•ë“¤ì„ ì¡°í•©í•˜ì—¬ ì „ì²´ì ì¸ Architecture êµ¬ì¡° í˜•íƒœë¡œ ì¢‹ì€ ì„±ëŠ¥**ì„ ê°€ì ¸ê°”ë‹¤ëŠ” ê²ƒì´, MixMatch ë¥¼ í¬í•¨í•œ í›„ì† Matchì‹œë¦¬ì¦ˆ ê¸°ë²•ë“¤ì˜ íŠ¹ì§•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹íˆ Holisticí•˜ë‹¤ê³  í•˜ì§€ë§Œ, MixMatch ê°™ì€ ê²½ìš° íŠ¹íˆë‚˜ êµ¬í˜„ê³¼ êµ¬ì¡°ê°€ ë§¤ìš° ë‹¨ìˆœí•˜ë©´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì ¸ì™”ê¸° ë•Œë¬¸ì— ì„ êµ¬ìì ì¸ ë…¼ë¬¸ ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê² ë‹¤.

ì´ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ MixUpì´ë©° ì´ë¥¼ Shuffleêµ¬ì¡°ë¥¼ ê°€ì ¸ê°€ë©° Dataì™€ Labelì„ Matchingí•´ ì£¼ì—ˆê¸° ë•Œë¬¸ì— MixMatchë¼ê³  ë¶€ë¥´ì§€ ì•Šë‚˜ ì‹¶ë‹¤.







> ğŸ”¥ì´ì œ ì•Œê³ ë¦¬ì¦˜ì„ ìˆœì„œ(Sequence)ëŒ€ë¡œ ì•Œì•„ë³´ì..!



## 1) Data Augmentation

ë¨¼ì € ëª¨ë“  Dataì˜ Data Augmentationì„ ìˆ˜í–‰í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ Computer Vision Deep Learning Modelë“¤ì´ ê·¸ë ‡ë“¯, **Regularizationë¡œì¨ Augmentationì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒ**ì‹œí‚¨ë‹¤. íŠ¹íˆ ë‚˜ì¤‘ì— ìˆì„ MixUpì—ì„œ ë” ë‹¤ì–‘í•œ ì¡°í•©ìœ¼ë¡œ Labeled Dataì™€ Unlabeled Dataë¥¼ ì„ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ ì¤€ë‹¤.

![image-20221226232414467](./attachments/image-20221226232414467.png)

ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ Labeled DataëŠ” Batchë³„ë¡œ 1íšŒ Random Augmentationì„, Unlabeled DataëŠ” Batchë³„ë¡œ KíšŒ Random Augmentationì„ ì§„í–‰í•œë‹¤. ì´ë•Œ ë…¼ë¬¸ì—ì„œ ì§„í–‰í•œ Augmentationì€ ì•„ë˜ì™€ ê°™ì´ **Random horizontal flipsì™€ Random Crops**ì´ë‹¤.



> ![image-20221226232556152](./attachments/image-20221226232556152.png)



ë˜í•œ ë…¼ë¬¸ì—ì„œëŠ” Unlabeled Dataì— ëŒ€í•´ì„œëŠ” 2íšŒì˜ Augmentationì„ ì§„í–‰í•˜ë„ë¡ Hyper-Parameterë¥¼ ì„¸íŒ…í•˜ì˜€ë‹¤.

>  ![image-20221227010258137](./attachments/image-20221227010258137.png)





## 2) Label Guessing & Label Sharpening

ì´ ë°©ì‹ì€ Pseudo Labelingê³¼ ë™ì¼í•œ ë°©ì‹ì´ë©°, **Only Unlabeled Dataì— ëŒ€í•´ì„œë§Œ Label Guessing**ì„ ìˆ˜í–‰í•œë‹¤. ë˜í•œ ë§ˆì°¬ê°€ì§€ë¡œ Guessingëœ **Unlabeled Dataì˜ Labelì— ëŒ€í•´ì„œë§Œ Label Sharpening**ì„ ì§„í–‰í•œë‹¤. ì „ì²´ì ì¸ FlowëŠ” ì•„ë˜ì™€ ê°™ë‹¤. 

>  êµ¬í˜„ ì‹œì— Augmentedëœ Label Dataì— Guessingì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹˜ì„ ê¼­ ìœ ì˜ í•´ì•¼ í•œë‹¤.



![image-20221226200950638](./attachments/image-20221226200950638.png)

ìœ„ì™€ ê°™ì´ Batchë³„ë¡œ ìë™ì°¨ë¥¼ Kê°œì˜ Random Augmentation í•œí›„, Guessingëœ Labelë“¤ì„ Averageí•˜ê³ , ê·¸ ê°’ì„ Sharpeningí•œë‹¤. Sharpeningì´ë¼ëŠ” ê²ƒì€ í™•ë¥ ì´ ë†’ì€ ê²ƒì„ ì¢€ ë” ê°•ì¡°(Temperatureë¼ëŠ” Hyper-Parameter Të¥¼ ì‚¬ìš©í•˜ì—¬, ì–¼ë§ˆë‚˜ ê°•ì¡°í• ì§€ ì¡°ì •í•œë‹¤.) ì´ Label Sharpenigì„ í†µí•´ Unlabeled Dataì˜ Pseudo-Labelì— ëŒ€í•œ Entropyê°€ Minimizationëœë‹¤. (ì¦‰, í•˜ë‚˜ì˜ Guessing Labelì„ ë” ê°•ì¡°í•œë‹¤ëŠ” ì´ì•¼ê¸°ì„. ì „ì²´ Guessing Labelì´ Unifromí˜•íƒœë¥¼ ëˆë‹¤ë©´, Entropyê°€ Maximizationì´ ëœë‹¤.) 

> íŠ¹íˆ Entropy Minimizationì€ 2005ë…„ Semi-supervised learning by entropy minimization (Yves Grandvalet and Yoshua Bengio) ë…¼ë¬¸ì˜ ê´€ì°°ì„ í†µí•´ Ideaë¥¼ ì–»ì—ˆë‹¤ê³  ì €ìë“¤ì€ ì´ì•¼ê¸° í•œë‹¤. ì´ëŠ” ê·¸ë¦¬ê³  High-Density Region Assumptionì„ ë” ê°•ì¡°í•˜ê¸° ìœ„í•¨ì´ë‹¤.

![image-20221226234523055](./attachments/image-20221226234523055.png)

![image-20221226233648148](./attachments/image-20221226233648148.png)

ìœ„ì™€ ê°™ì´ ì¼ë°˜ì ì¸ Softmaxì˜ í™•ë¥ ê°’ì— 1/T ìŠ¹ì„ í•˜ì—¬ ê°’ì„ ê²°ì •í•˜ê²Œ ëœë‹¤. Tê°’ì´ 2.0, 0.5, 0.1ì— ë”°ë¼ì„œ, ê°’ì´ ë” ì‘ì„ ìˆ˜ë¡ Winnner-Take-Allì„ í•˜ê²Œ ëœë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ Hyper-Parameter Tê°’ì„ ì•„ë˜ì™€ ê°™ì´ 0.5ë¡œ ì„¸íŒ… í•˜ì˜€ìœ¼ë©°, ìš°ë¦¬ì˜ êµ¬í˜„ì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ 0.5ë¡œ ì„¸íŒ… í•  ìƒê°ì´ë‹¤.

> ![image-20221226233938750](./attachments/image-20221226233938750.png)



## 3) MixUp

> MixMatchì˜ ì´ë¦„ì— ì™œ Mixê°€ ë“¤ì–´ê°”ëŠ”ì§€ì— ëŒ€í•œ ì´ìœ ì´ë‹¤. ê·¸ë§Œí¼ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë‹¤.

MixUp(2018, mixup: BEYOND EMPIRICAL RISK MINIMIZATION)ì— ë‚˜ì˜¨ ê¸°ë²•ìœ¼ë¡œì¨, ì›ë˜ëŠ” ì¢€ ë” Decision Boundaryë¥¼ Smoothí•˜ê²Œ í•˜ì—¬ Emperical Risk Minimizationì„ Supervised Learningì—ì„œ í™œìš©í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë‹¨ìˆœí•œ ê¸°ë²•ì´ë‹¤. ê·¸ëŸ¬ë‚˜ 2019ë…„ Interpolation consistency training for semi-supervised learning(ICT)ì—ì„œ ì²˜ìŒìœ¼ë¡œ ì´ ë°©ë²•ì„ Semi-Supervised Learningì— ì ìš©ì„ í•˜ì˜€ë‹¤. **MixUpì„ í†µí•´ì„œ ì¢€ ë” Smoothí•œ Boundaryë¥¼ ê°•ì œí•˜ëŠ” Regularization**ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. 



>  Unlabeld Dataì— ëŒ€í•´ì„œë§Œ MixUpì„ ì§„í–‰í•œ ê¸°ì¡´ì˜ ICT ë°©ì‹ê³¼ ë‹¤ë¥´ê²Œ, MixMatchëŠ” Labeled Dataì™€ Unlabeled Dataëª¨ë‘ì— MixUpì„ ìˆ˜í–‰í•œë‹¤. 



ì¼ì¢…ì˜ Data Agumentationê³¼ ìœ ì‚¬í•œë°, ìœ„ì˜ 1)ì—ì„œ Augmentationí•´ì„œ ìƒê²¨ë‚œ Dataì— í•œë²ˆë” MixUpì´ë¼ëŠ” Augmentationì„ ì§„í–‰í•˜ê³ , ê·¸ X Dataì— ëŒ€í•œ ì˜ˆì¸¡ê³¼ Augmentedëœ Target Yê°’ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ìƒì„¸í•˜ê²ŒëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

![image-20221227010035007](./attachments/image-20221227010035007.png)





- **1) Augmented Setì„ ì¤€ë¹„í•œë‹¤.**
  - ìœ„ì—ì„œ ì§„í–‰í•œ Labeld Setì˜ Dataì— 1íšŒ Augmentation
  - Unlabeld Set Dataì— KíšŒ Augmentation
- **2) Shuffle Setì„ ì¤€ë¹„í•œë‹¤.**
  - Augmentation Setì„ Copyí•˜ì—¬ ë³µì‚¬í•˜ê³ , Labeled Setê³¼ UnLabeled Setì„ ëª¨ë‘ í•©ì³ì„œ ì„ëŠ”ë‹¤. ì´ë•Œ Data Xì™€ Target Yë„ ê°ê° ì„ì–´ ì¤€ë‹¤.
- **3) MixUpì„ ì§„í–‰í•œë‹¤.**
  - 1)ê³¼ 2)ì—ì„œ ì¤€ë¹„í•œ 2ê°œì˜ Setì„ Beta ë¶„í¬ì—ì„œ ë½‘ì€ Lambdaê°’ì„ í†µí•´ì„œ Weighted Sumì„ í†µí•´ Mixupí•œë‹¤. ì´ë•Œ LambdaëŠ” 0.5~1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì£¼ë„ë¡ í•˜ì—¬, Shuffle Setë³´ë‹¤ Augmented Setì— ë” Weightë¥¼ ì£¼ë„ë¡ í•œë‹¤. ê·¸ ì´ìœ ëŠ” ê°ê°ì˜ Target_L(Labeled)ê³¼ Target_U(Unlabeled)ì— ëŒ€í•˜ì—¬, Matchingë˜ë„ë¡ ê°’ì„ ì£¼ê¸° ìœ„í•¨ì´ë‹¤. (**ì•„ë§ˆ ì´ ë•Œë¬¸ì— MixMatchë¼ê³  ë¶€ë¥´ëŠ”ê²Œ ì•„ë‹Œê°€ ì‹¶ë‹¤.**) ê·¸ë ‡ì§€ ì•ŠëŠ”ë‹¤ë©´ Shuffle Setì— Biasë˜ê¸° ë•Œë¬¸ì— ì˜ Matchingë˜ë„ë¡ Lambdaë¥¼ ì¡°ì ˆí•œë‹¤.
  - ê·¸ë¦¬ê³  MixUpëœ Data_L(Labeled)ê³¼ Data_U(Unlabeled)ì— ëŒ€í•´ì„œëŠ” Modelì— ë„£ê³ , ê°ê°ì˜ ì˜ˆì¸¡ ê°’ì¸ Pred_L(Labeled)ê³¼ Pred_U(Unlabeled)ë¥¼ ë½‘ëŠ”ë‹¤.
  - ê·¸ë¦¬ê³  Pred_Lê³¼ Target_Lì‚¬ì´ì˜ Lossë¥¼ Cross Entropyë¥¼ í†µí•´ Supervised Lossë¡œ ì‚¬ìš©í•˜ê³ , Pred_Uì™€ Target_Uì‚¬ì´ì˜ ê°’ì€ Distance Metric(ex. MSE Loss)ì„ ì‚¬ìš©í•´ Unsupervised Lossë¥¼ êµ¬í•œë‹¤.
  - ê·¸ë¦¬ê³  2ê°œì˜ Lossë¥¼ ë¯¸ë¦¬ì •í•´ì§„ Hyper-Parameterê°’ Weightë¥¼ í†µí•´ ì¡°ì ˆí•œë‹¤. (ê·¸ë¦¬ê³  ì´ ê°’ì€ Learning Rateì²˜ëŸ¼, Ramp-Upì„ í†µí•´ ì¡°ì ˆí•˜ë„ë¡ í•œë‹¤.)





----

# Tutorial. Deep Understanding of MixMatch Implementation

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì „ì²´ì ì¸ MixMatchë¥¼ Scratchë¡œ êµ¬í˜„í•´ ë³´ë©´ì„œ ì•Œê³ ë¦¬ì¦˜ì„ ì´í•´í•´ ë³´ë ¤í•œë‹¤. íŠ¹íˆ ì› ì €ìë“¤ì´ ë…¼ë¬¸ì—ì„œ ì œëŒ€ë¡œ ê±´ë“œë¦¬ì§€ ì•Šì•˜ìœ¼ë‚˜(í˜¹ì€ ë°˜ëŒ€ë¡œ ì´ì•¼ê¸°í–ˆìœ¼ë‚˜!), ì›ì €ìë“¤ì˜ êµ¬í˜„ì²´ í˜¹ì€ ê¸°íƒ€ ë‹¤ë¥¸ êµ¬í˜„ì²´ë“¤ì—ì„œ ì´ë¯¸ êµ¬í˜„í•˜ê³  ìˆì—ˆìœ¼ë‚˜ ì œëŒ€ë¡œ ë…¼ì˜ê°€ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ ì˜ì—­ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¤í—˜ê³¼ í•¨ê»˜ ê°€ì ¸ê°€ë ¤ í•œë‹¤. ì œê¸°í•˜ê³  ì‹¶ì€ ì˜ë¬¸ì€ 2ê°€ì§€ì´ë‹¤.

- MixMatchì—ì„œ **EMA(Exponential Moving Average)**ë¡œ Teacherëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ ì¤‘ìš”í•œê°€? 
- MixMatchì—ì„œ **Interleaving** êµ¬í˜„ì€ ì¤‘ìš”í•œê°€?

>  ë™ì‹œì— í•´ë‹¹ ë…¼ë¬¸ì— ëŒ€í•´ì„œ Reviewerë“¤ì´ ì´ì•¼ê¸°í•œ ë‚´ìš©, ê·¸ë¦¬ê³  êµ¬í˜„ì²´ë“¤ì˜ Issueì— ì œê¸°í•œ ì˜ë¬¸ë“¤ë„ í•¨ê»˜ ë³´ë©´ì„œ ì´ì•¼ê¸° í•´ ë³´ë ¤ í•œë‹¤.



## 1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/5_semi-supervised_learning/Tutorials/Tutorial_MixMatch.ipynb)



## 2. Setting

### Datasets

ë°ì´í„°ëŠ” ìœ ëª…í•œ CIFAR-10ì„ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤. 10ê°œì˜ Classë¥¼ ê°–ê³  ìˆëŠ” 32x32x3 Shapeì˜ Imagesetì´ë‹¤.

![image-20221227100541139](./attachments/image-20221227100541139.png)

|      | Datasets                  | Description                               | Num Instances                                   | Image Size | Num Channels | Single Output Classes |
| ---- | ------------------------- | ----------------------------------------- | ----------------------------------------------- | ---------- | ------------ | --------------------- |
| 1    | CIFAR-10 (Classification) | 10ê°œì˜ Classë¥¼ ê°€ì§„ ì‘ì€ ì´ë¯¸ì§€ ë°ì´í„° ì…‹ | Training Set : 50,000<br />Testing Set : 10,000 | 32 x 32    | 3            | 10                    |

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê²Œ ëœë‹¤. 

```python
# Load CIFAR-10 Labeled Images with Random Augmentation
class get_cifar10_labeled(torchvision.datasets.CIFAR10):

    def __init__(self, path_cifar10, indexs=None, train=True,
                 transform=None, target_transform=None,
                 download=False):
        super(get_cifar10_labeled, self).__init__(path_cifar10, train=train,
                 transform=transform, target_transform=target_transform,
                 download=download)
        if indexs is not None:
            self.data = self.data[indexs]
            self.targets = np.array(self.targets)[indexs]
        self.data = transpose(normalize(self.data))

    def __getitem__(self, index):
        data_x, target_y = self.data[index], self.targets[index]

        if self.transform is not None:
            data_x = self.transform(data_x)

        if self.target_transform is not None:
            target_y = self.target_transform(target_y)

        return data_x, target_y
    

# Load CIFAR-10 Unlabeled Images with Random Augmentation (K=2)
class get_cifar10_unlabeled(get_cifar10_labeled):

    def __init__(self, path_cifar10, indexs, train=True,
                 transform=None, target_transform=None,
                 download=False):
        super(get_cifar10_unlabeled, self).__init__(path_cifar10, indexs, train=train,
                 transform=transform, target_transform=target_transform,
                 download=download)
        self.targets = np.array([-1 for i in range(len(self.targets))])

```

ì´ë•Œ Datasetì€ Pytorchì˜ Dataloaderë¥¼ í†µí•´ Training Timeì— ë¶ˆëŸ¬ì§€ê²Œ ë˜ë©°, Datasetì€ ë™ì‹œì— ì•„ë˜ì˜ ì½”ë“œë¡œ Augmentationì´ ì´ë£¨ì–´ ì§„ë‹¤. Labeled DataëŠ” 1ë²ˆì˜ Random Augmentationì´ ì§„í–‰ë˜ë©°, Unlabeled DataëŠ” K=2ë¡œì¨ 2ë²ˆì˜ Data Augmentationì´ ìˆ˜í–‰ëœë‹¤. ë‹¤ìŒì˜ ê¸°ëŠ¥ì´ ì•„ë˜ì˜ ì½”ë“œë¡œ ì´ë£¨ì–´ ì§„ë‹¤.

- Image Normalization (Standard Scaling)
- Image Transpose (from CIFAR-10 shape to Pytorch Shape)
- Augmentation Method #1 : Image Padding & Cropping
- Augmentation Method #2 : Image Horizontal Flipping

```python
# Dataset Normalization (Standard Scaling)
def normalize(x, mean=CIFAR10_MEAN, std=CIFAR10_STD):
    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]
    x -= mean*255
    x *= 1.0/(255*std)
    return x

# Transpose Image Shape for Pytorch shape from CIFAR-10 Original shape
def transpose(x, source='NHWC', target='NCHW'):
    return x.transpose([source.index(d) for d in target]) 

# Padding to image borders for cropping image
def pad(x, border=4):
    return np.pad(x, [(0, 0), (border, border), (border, border)], mode='reflect')

# Augmentation Method 1 : Random Padding and Cropping 
class RandomPadandCrop(object):
    def __init__(self, output_size):
        assert isinstance(output_size, (int, tuple))
        if isinstance(output_size, int):
            self.output_size = (output_size, output_size)
        else:
            assert len(output_size) == 2
            self.output_size = output_size

    def __call__(self, x):
        x = pad(x, 4)

        h, w = x.shape[1:]
        new_h, new_w = self.output_size

        top = np.random.randint(0, h - new_h)
        left = np.random.randint(0, w - new_w)

        x = x[:, top: top + new_h, left: left + new_w]

        return x

# Augmentation Method 2 : Random Horizontal Flipping 
class RandomFlip(object):
    def __call__(self, x):
        if np.random.rand() < 0.5:
            x = x[:, :, ::-1]

        return x.copy()

class ToTensor(object):
    def __call__(self, x):
        x = torch.from_numpy(x)
        return x

def split_dataset(labels, num_train):
    labels = np.array(labels)
    index_train_labeled = []
    index_train_unlabeled = []
    index_valid = []

    num_labels = len(np.unique(labels))
    num_labeled_per_class = int(num_train / num_labels)

    for i in range(num_labels):
        index = np.where(labels == i)[0]
        np.random.shuffle(index)
        
        index_train_labeled.extend(index[:num_labeled_per_class])
        index_train_unlabeled.extend(index[num_labeled_per_class:-500])
        index_valid.extend(index[-500:])

    np.random.shuffle(index_train_labeled)
    np.random.shuffle(index_train_unlabeled)
    np.random.shuffle(index_valid)

    return index_train_labeled, index_train_unlabeled, index_valid

# Twice Augmentation for Unlabeld Image (eg. K=2)
class Multi_Augmentation:
    def __init__(self, transform_method):
        self.transform_method = transform_method
        # self.num_transform = num_transform

    def __call__(self, inp):
        aug_out_1 = self.transform_method(inp)
        aug_out_2 = self.transform_method(inp)
        return aug_out_1, aug_out_2
```



### Algorithms

ìš°ë¦¬ëŠ” MixMatch 1ê°€ì§€ë§Œ ì§‘ì¤‘ì ìœ¼ë¡œ íŒŒ ë³´ê³ ì í•œë‹¤. íŠ¹íˆ ìœ„ì—ì„œ ì œê¸°í•œ 2ê°€ì§€, EMA ë°©ì‹ì˜ Teacher Networkì™€ Interleaveì— ëŒ€í•œ ì´í•´ë¥¼ íŒŒë³´ê² ë‹¤.

| Algorithm | Target         | Description                                                  |
| --------- | -------------- | ------------------------------------------------------------ |
| MixMatch  | Classification | WideResNetì„ Backboneìœ¼ë¡œ ì‚¬ìš©í•œ Holistic Semi-Supervised Learning ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©. |



## 3. Implementation of MixMatch

MixMatchëŠ” ìœ„ì—ì„œ ì„¤ëª…í•˜ì˜€ë˜, Data Augmentation, Label Guessing & Sharpening ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ MixUpì„ ê²°í•©í•œ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ëœë‹¤.  ìƒì„¸í•˜ê²Œ ê·¸ êµ¬í˜„ì²´ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ì.



### 3-1. Loss, EMA, Interleaving Functions

MixMatchì˜ Learningê¸°ëŠ¥ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ì˜ ì§‘í•©ì´ë‹¤. íŠ¹íˆë‚˜ Trainingì„ ìœ„í•œ í•¨ìˆ˜ë“¤ì´ë©°, ì°¨ê·¼ì°¨ê·¼ ìƒì„¸íˆ ì•Œì•„ë³´ì



> Semi-Supervised Loss / Ramp-Up Function

ì•„ë˜ëŠ” Semi-Supervised Lossë¥¼ êµ¬í•˜ê¸° ìœ„í•œ Each_Loss Classì™€ ê·¸ 2ê°œì˜ supervised loss, unsupervised lossì˜ weighted sumì„ êµ¬í•˜ê¸° ìœ„í•œ ramp_up í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤. ramp_upì€ ì´ epochìˆ˜ì— ë”°ë¼ êµ¬ì„±ì´ ë˜ë©°, ì„ í˜•ì ìœ¼ë¡œ weightê°€ ì¦ê°€ë˜ë„ë¡ êµ¬í˜„ì´ ë˜ì–´ìˆë‹¤. each_lossëŠ” labeled dataì˜ targetê°’ê³¼ predictionê°’ì„ í†µí•´ supervised loss(loss_l)ë¥¼ êµ¬í•˜ê³ , unlabeled dataì˜ targetê°’ê³¼ predictionê°’ì„ í†µí•´ unsupervised loss(loss_u)ë¥¼ êµ¬í•œë‹¤. 

```python
# Ramp-up Function for balancing the weight between supervised loss and unsupervised loss
# - loss_total = loss_l + weight * loss_u
def ramp_up(current, rampup_length=epochs):
    if rampup_length == 0:
        return 1.0
    else:
        current = np.clip(current / rampup_length, 0.0, 1.0)
        return float(current)
    
class Each_Loss(object):
    def __call__(self, pred_l, target_l, pred_u, target_u, epoch):
        probs_u = torch.softmax(pred_u, dim=1)

        loss_l = -torch.mean(torch.sum(F.log_softmax(pred_l, dim=1) * target_l, dim=1))
        loss_u = torch.mean((probs_u - target_u)**2)

        return loss_l, loss_u, lambda_u * ramp_up(epoch)

```



> ğŸ”¥ Exponential Moving Average for training Teacher Model Function

Fire Emoticonì„ ë¶™ì˜€ë‹¤. ê·¸ë§Œí¼ ì¤‘ìš”í•˜ë‹¤ëŠ” ëœ». Exponential Moving Average(EMA)ë¥¼ í†µí•˜ì—¬ Student Modelì˜ Parameterë¥¼ Teacher Modelì˜ Parameterë¡œ ì „ì´í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. ë°‘ì—ì„œ ì‹¤í—˜ì„ í†µí•´ ì•Œì•„ë³´ê² ì§€ë§Œ, ì´ EMAêµ¬í˜„ì´ ë˜ì–´ì§€ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì€ ì œëŒ€ë¡œ í•™ìŠµì´ ì´ë£¨ì–´ ì§€ì§€ ì•ŠëŠ”ë‹¤. (ë…¼ë¬¸ì—ì„œëŠ” ì˜¤íˆë ¤ ë°˜ëŒ€ë˜ëŠ” ì„¤ëª…ì„ í•˜ê³ ìˆë‹¤.) ì„±ëŠ¥ í–¥ìƒì— í•„ìˆ˜ì ì¸ í•¨ìˆ˜ì´ë©°, ê·¸ì— ëŒ€í•œ ì„¤ëª…ì´ ì—†ìœ¼ë¯€ë¡œ ê¼­ êµ¬í˜„ì„ í•´ì•¼ë§Œ í•œë‹¤. Student Modelë§Œìœ¼ë¡œëŠ” Predictionì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤.

```python
class exponential_moving_average(object):
    def __init__(self, student_model, teacher_model, alpha=0.999):
        self.model = student_model
        self.ema_model = teacher_model
        self.alpha = alpha
        self.student_params = list(student_model.state_dict().values())
        self.teacher_params = list(teacher_model.state_dict().values())
        self.wd = 0.02 * lr

        for param, ema_param in zip(self.student_params, self.teacher_params):
            param.data.copy_(ema_param.data)

    def step(self):
        one_minus_alpha = 1.0 - self.alpha
        for student_param, teacher_param in zip(self.student_params, self.teacher_params):
            if teacher_param.dtype==torch.float32:
                teacher_param.mul_(self.alpha)
                teacher_param.add_(student_param * one_minus_alpha)
                # customized weight decay
                student_param.mul_(1 - self.wd)
```



> ğŸ”¥ Interleaving Function

ì—­ì‹œë‚˜ Fire Emoticonì„ ë¶™ì˜€ë‹¤. ì•„ë˜ì˜ Interleavingì˜ êµ¬í˜„ì€ ë…¼ë¬¸ ì› ì €ìì˜ êµ¬í˜„ì²´ì—ë„ ì¡´ì¬í•˜ë©°, ê·¸ ì™¸ì˜ ëŒ€ë¶€ë¶„ì˜ ì¸ê¸°ìˆëŠ” êµ¬í˜„ì²´ì—ì„œë„ ì•„ë˜ì˜ ë°©ì‹ìœ¼ë¡œ Interleavingì„ ì‚¬ìš©í•œë‹¤. Interleavingì€ Labeled Dataì™€ Unlabeled Dataì˜ ê°’ë“¤ì„ ì„œë¡œ Mixingí•´ì£¼ì–´, Modelì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì¡´ì¬í•œë‹¤. ì™œëƒí•˜ë©´, ëŒ€ë¶€ë¶„ì˜ êµ¬í˜„ì²´ì— ìˆì–´ Labeled Dataë¥¼ Modelì— ì…ë ¥í•˜ì—¬ Supervised Lossë¥¼ êµ¬í•˜ê³ , ê·¸ ì´í›„ì— Unlabeled Dataë¥¼ Modelì— ì…ë ¥í•˜ì—¬ Unsupervised Lossë¥¼ êµ¬í•˜ê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ì´ë ‡ê²Œ ê°ê°ì˜ Dataë¥¼ Modelì— ë”°ë¡œ, 2ë²ˆ íƒœì›Œì„œ ê³„ì‚°í•  ê²½ìš°, Batch Normalizationì„ ì§„í–‰ í•  ë•Œì— ë‘ê°œì˜ ë¶„í¬ê°€ ì „ì²´ì˜ Batchì˜ ë¶„í¬ë¥¼ ëŒ€ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, í•™ìŠµì‹œì— Biasê°€ ìƒê¸°ê²Œ ëœë‹¤.

ë”°ë¼ì„œ ì´ë²ˆ Tutorialì—ì„œëŠ” Interleavingì„ ì‚¬ìš©í•œ ê²½ìš°ì™€, ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì˜ í•™ìŠµ ë°©ë²•ì— ëŒ€í•´ì„œ ë‘ê°€ì§€ ëª¨ë‘ êµ¬í˜„ì„ ì§„í–‰í•˜ì˜€ë‹¤.(ì½”ë“œì˜ Trainingë¶€ë¶„ì„ í™•ì¸í•˜ì„¸ìš”..!) ê·¸ë¦¬ê³  ì•„ë˜ì˜ Testì—ì„œ Interleavingì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ” í•™ìŠµ ë°©ë²•ì— ìˆì–´ì„œ, í•™ìŠµì´ ì˜ ë˜ì§€ ì•ŠëŠ” ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì„ í†µí•´ ë°í˜€ë‚´ë„ë¡ í•˜ê² ë‹¤.

```python
def interleave_offsets(batch, nu):
    groups = [batch // (nu + 1)] * (nu + 1)
    for x in range(batch - sum(groups)):
        groups[-x - 1] += 1
    offsets = [0]
    for g in groups:
        offsets.append(offsets[-1] + g)
    assert offsets[-1] == batch
    return offsets


def interleave(xy, batch):
    nu = len(xy) - 1
    offsets = interleave_offsets(batch, nu)
    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]
    for i in range(1, nu + 1):
        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]
    return [torch.cat(v, dim=0) for v in xy]
```





### 3-2. Training Functions

Training Functionì€ MixMatchì˜ ìˆœì„œì— ë”°ë¼ êµ¬í˜„ë˜ì–´ ìˆë‹¤. ì‚¬ì‹¤ ì–´ë ¤ìš´ ì½”ë“œëŠ” ì•„ë‹ˆë©°, ì§ê´€ì ìœ¼ë¡œ ì´í•´ê°€ ë˜ë¯€ë¡œ ê°„ë‹¨í•˜ê²Œ ìˆœì„œëŒ€ë¡œ í•œë²ˆ ì‚´í´ë³´ë„ë¡ í•˜ì.



> 1. Data Augmentation

Data Augmentationì„ ì§„í–‰í•˜ëŠ” ì½”ë“œì´ë‹¤. ìœ„ì—ì„œ ì •ì˜í–ˆë˜ CIFAR-10ì„ ë¶ˆëŸ¬ì˜¤ë©´ì„œ Augmentationì„ í•˜ë„ë¡ Pytorchì˜ DataLoaderë¥¼ ë§Œë“¤ì—ˆê³ , ê·¸ì— ë”°ë¼ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê²Œ ëœë‹¤. 

- Labeled Data : inputs_l / target_l
- Unlabeld Data : inputs_u, inputs_u2 (ì¦‰, K=2)

ì—¬ê¸°ì— try, exceptê°€ ìˆëŠ”ë° ì´ëŠ” ë³´í†µ supervisedì—ì„œëŠ” trainingì‹œì— batchì— ë§ì¶°ì„œ dataë¥¼ loadí•˜ì§€ë§Œ, MixMatchê°™ì€ ê²½ìš° Labeled Dataì™€ Unlabeled Dataê°€ ì„œë¡œ ê°œìˆ˜ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—(ë¹„ìœ¨ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ Unlabeledê°€ ë” ë§ê±°ë‚˜ ê°™ìŒ), Batchì™€ ê´€ê³„ì—†ì´ training_iteractioníšŸìˆ˜ì— ë”°ë¼ Dataë¥¼ loadingí•˜ê¸° ë•Œë¬¸ì— ëª¨ë“  Data Loaderì—ì„œ samplingì„ ë‹¤ ìˆ˜í–‰í–ˆì„ ê²½ìš°, ë‹¤ì‹œê¸ˆ Data Loaderë¥¼ ìœ„í•œ Iteratorë¥¼ ë§Œë“¤ê¸° ìœ„í•´ except ì½”ë“œê°€ ì¡´ì¬í•œë‹¤. í¬ê²Œ ì–´ë ¤ìš¸ ê²ƒ ì—†ëŠ” ì½”ë“œì´ë‹¤.

```python
#########################################
# 1. Data Augmentation
#########################################
try:
    inputs_l, targets_l = next(iter_train_labeled)
except:
    iter_train_labeled = iter(labeled_trainloader)
    inputs_l, targets_l = next(iter_train_labeled)


try:
    (inputs_u, inputs_u2), _ = next(iter_train_unlabeled)
except:
    iter_train_unlabeled = iter(unlabeled_trainloader)
    (inputs_u, inputs_u2), _ = next(iter_train_unlabeled)

# Transform label to one-hot
batch_size = inputs_l.size(0)
targets_l = torch.zeros(batch_size, 10).scatter_(1, targets_l.view(-1,1).long(), 1)

if use_cuda:
    inputs_l, targets_l = inputs_l.cuda(), targets_l.cuda(non_blocking=True)
    inputs_u = inputs_u.cuda()
    inputs_u2 = inputs_u2.cuda()
```



> 2. Label Guessing and Label Sharpening

Pseudo Labelingì„ ìœ„í•˜ì—¬ Unlabeled Dataë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‹¨ê³„ì´ë‹¤. ì˜ˆì¸¡ì„ í•˜ê³  ë‚˜ì„œ Temperature Hyper-Parameterë¥¼ í†µí•´ labelê°„ì˜ í™•ë¥ ì„ sharpeningí•œë‹¤. ì´ê²ƒë„ ì–´ë ¤ìš¸ ê²Œ ì—†ëŠ” ì½”ë“œì´ë‹¤.

```python
#########################################
# 2. Label Guessing & Label Sharpening 
#########################################
with torch.no_grad():
    # compute guessed labels of unlabel samples
    outputs_u = model(inputs_u)
    outputs_u2 = model(inputs_u2)
    p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2
    pt = p**(1/Temperature)
    targets_u = pt / pt.sum(dim=1, keepdim=True)
    targets_u = targets_u.detach()
```



> 3. MixUp

ë§¤ìš° ì¤‘ìš”í•œ ë¶€ë¶„ì´ë‹¤. MixMatchì˜ í•µì‹¬ì´ ë˜ëŠ” ì½”ë“œë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. (ê·¸ëŸ¬ë‚˜ êµ¬í˜„ì´ ì–´ë µì§€ ì•Šë‹¤.). ì•„ë˜ì˜ 2ê°œì˜ Dataë¥¼ MixUpí•´ ì¤€ë‹¤.

- ì›ë˜ì˜ Augmentedëœ Labeled Dataì™€ Unlabeled Data
- ì¶”ê°€ì ìœ¼ë¡œ Labeled Data+Unlabeld Dataë¥¼ ë¶™ì—¬ì„œ Shuffleí•œ Data

ê·¸ ë•Œì— ë‘ê°œì˜ Dataë¥¼ Weighted Sumì„ í•´ ì£¼ëŠ”ë°, ì›ë˜ì˜ Augmented Dataì— ì¢€ ë” ê°€ì¤‘ ì¹˜ë¥¼ ë” ì£¼ì–´, ëª©í‘œí•˜ëŠ” Targetê°’ê³¼ Matchingë˜ë„ë¡ êµ¬ì„±ì„ í•œë‹¤. ê·¸ë˜ì„œ ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ì´ MixMatchì´ì§€ ì•Šì„ê¹Œ ì‹¶ë‹¤.

```python
#########################################
# 3. MixUp 
#########################################
all_inputs = torch.cat([inputs_l, inputs_u, inputs_u2], dim=0)
all_targets = torch.cat([targets_l, targets_u, targets_u], dim=0)

Lambda_ = np.random.beta(alpha, alpha)

Lambda_ = max(Lambda_, 1-Lambda_)

idx = torch.randperm(all_inputs.size(0))

input_a, input_b = all_inputs, all_inputs[idx]
target_a, target_b = all_targets, all_targets[idx]

mixed_input = Lambda_ * input_a + (1 - Lambda_) * input_b
mixed_target = Lambda_ * target_a + (1 - Lambda_) * target_b
```



> 4. Interleaving or No-Interleaving

ì´ê²Œ ë…¼ë¬¸ì—ì„œë„ ê·¸ë ‡ê³  ì¸í„°ë„·ì—ë„ ê·¸ë ‡ê³  ì˜ ì„¤ëª…ì´ ë˜ì–´ìˆì§€ ì•ŠëŠ” ë¶€ë¶„ì´ë‹¤. ëŒ€ë¶€ë¶„ì˜ githubì˜ êµ¬í˜„ì²´ì—ì„œ interleavingë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” mixedëœ labeled dataì™€ unlabeled dataë¥¼ ì„œë¡œ interleaving(ë°ì´í„°ì˜ sampleì˜ ì¼ë¶€ë¶„ì„ ì„œë¡œ êµí™˜)í•˜ë„ë¡ í•˜ì—¬, batchë³„ë¡œ batch-normalizationì„ í• ë•Œ ê·¸ ë¶„í¬ê°€ ë³€í™”ë˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•´ ë†“ì€ ê²ƒì´ë‹¤. ì™œ batch-normalizationí•  ë•Œ ë¶„í¬ê°€ ì˜í•™ìŠµë˜ì§€ ì•Šëƒí•˜ë©´, ì•„ë˜ì˜ êµ¬í˜„ì—ì„œ ì²˜ëŸ¼ labeled dataë¥¼ modelì— ë”°ë¡œ í˜ë¦¬ê³ , unlabeled dataë¥¼ modelì— ë”°ë¡œ í˜ë¦¬ê¸° ë•Œë¬¸ì´ë‹¤.

ğŸ”¥ì´ ë•Œë¬¸ì— ê°ê°ì˜ modelì´ batch-normalizationì˜ parameterê°€ ê°œë³„ì ìœ¼ë¡œ biasë˜ë©° í•™ìŠµì´ ì´ë£¨ì–´ì§€ ê¸° ë•Œë¬¸ì´ë‹¤.

ğŸ”¥ë”°ë¼ì„œ MixMatchë¿ë§Œ ì•„ë‹ˆë¼, FixMatchê°™ì€ ê²½ìš°ë„ ë§ˆì°¬ê°€ì§€ì¸ë°, ì´ë ‡ê²Œ Labeled Dataì™€ Unlabeled Dataë¥¼ Modelì— ë”°ë¡œ í˜ë¦´ê²½ìš°..ê·¸ë¦¬ê³  Backbone Modelì—ì„œ batch-normalizationì„ ì‚¬ìš©í•  ê²½ìš°ëŠ” ê¼­ Interleaving functionì„ êµ¬í˜„í•˜ì—¬ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš° í•™ìŠµ ìì²´ê°€ ì˜ ì´ë£¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤!

ğŸ”¥ë˜í•œ Google Reseasrchì˜ Fix Matchì €ìë“¤ì´ github issuesì— ë‹µë³€í•˜ê¸°ë¥¼, Multiple-GPUsë¥¼ ì‚¬ìš©í•  ê²½ìš°, Interleavingì„ í†µí•´ ë°ì´í„°ë¥¼ ì„ì–´ì¤€ë‹¤ìŒ ê° GPUë¡œ í˜ë ¸ì„ë•Œ ì—­ì‹œë‚˜ Batch-Normì´ ì˜ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ì´ë ‡ê²Œ êµ¬í˜„í•˜ì˜€ë‹¤ê³ í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” Tensorflowì˜ ê²½ìš°ì´ê³  PytorchëŠ” Multi-GPUsë¥¼ ìœ„í•œ Batch-Norm êµ¬í˜„ì´ ë”°ë¡œ ìˆì´ë¯€ë¡œ, ì´ë ‡ê²Œ í•  í•„ìš”ëŠ” ì—†ë‹¤ê³  ìƒê°ëœë‹¤.

ğŸ”¥ê·¸ëŸ¬ë‚˜ Labeled Dataì™€ Unlabeled Dataë¥¼ í•©ì³ì„œ, í•œë²ˆì— Modelì— í˜ë¦´ ê²½ìš°ëŠ” ì´ì•¼ê¸°ê°€ ë‹¬ë¼ì§„ë‹¤. ì´ ë•Œì—ëŠ” Interleavingì´ í•„ìš”ê°€ ì—†ìœ¼ë©°, ë‹¹ì—°íˆ í•œë²ˆì˜ Batchì— 2ê°œì˜ ë°ì´í„° í˜•íƒœê°€ ë™ì‹œì— ë“¤ì–´ê°€ë¯€ë¡œ, ë¶„í¬ì˜ ë³€í™”ê°€ ì ì ˆíˆ ì˜ í•™ìŠµì´ ëœë‹¤. 

ğŸ”¥ê·¸ëŸ°ë° ì—¬ê¸°ì—ì„œ ì£¼ì˜í•´ì•¼ í•  êµ¬í˜„ ì‚¬í•­ì´ ìˆëŠ”ë°, interleavingì„ í•  ê²½ìš° Batch-Sizeê°€ 1:2ë¡œ ë‚˜ë‰˜ë©´ì„œ í•™ìŠµì´ ë˜ì§€ë§Œ, No-interleavingì¼ ê²½ìš° Batch-Sizeê°€ 3ë°°ê°€ ë˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ì˜ Learning Rateë¡œëŠ” í•™ìŠµì´ ëŠë ¤ì§€ê²Œ ëœë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ë§ì´ë“¤ ì‚¬ìš©í•˜ëŠ” Batch Sizeê°€ kë°°ê°€ ë˜ë©´ Learning RateëŠ” sqrt(k)ë°°ë¡œ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ë©´ í•™ìŠµì´ ì˜ ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.

```python
########################################
# 4. Interleaving or No-Interleaving
########################################
if use_interleaving:
    # 1) interleave labeled and unlabed images between batches to get correct batchnorm calculation 
    mixed_input = list(torch.split(mixed_input, batch_size))
    mixed_input = interleave(mixed_input, batch_size)

    # 2) labeled prediction
    logits = [model(mixed_input[0])]
    
    # 3) unlabeled prediction
    for input in mixed_input[1:]:
        logits.append(model(input))

    # 4) de_interleave to calculate labeled supervised loss and unlabeld unsupervised loss properly 
    logits = interleave(logits, batch_size)

    logits_l = logits[0]
    logits_u = torch.cat(logits[1:], dim=0)
else:
    # No Interleaving and calculate both labeled and unlabeled sample.
    # The model is used only once to predict logits. So A calculation of batchnorm is proper.
    # But if you want to use this no-interleaving method then you should adjust the learning rate (with multiply sqrt(k))
    # k means k-times of increased batch size
    logits = model(mixed_input)
    split_logits = list(torch.split(logits, batch_size))
    logits_l = split_logits[0]
    logits_u = torch.cat(split_logits[1:], dim=0)
```



> 5. Semi-Supervised Loss

ê°„ë‹¨íˆ êµ¬í˜„í•˜ëŠ” Semi-Supervised Lossì´ë‹¤. ê°„ë‹¨í•˜ë‹¤. 2ê°œì˜ Lossë¥¼ ê°ê° êµ¬í•´ì„œ weighted sumì„ ìˆ˜í–‰í•œë‹¤.

```python
########################################
# 5. Semi-Supervised Loss 
########################################
loss_l, loss_u, weight = each_loss(logits_l, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_index/train_iteration)

loss_total = loss_l + weight * loss_u

# record loss
losses_total.update(loss_total.item(), inputs_l.size(0))
losses_l.update(loss_l.item(), inputs_l.size(0))
losses_u.update(loss_u.item(), inputs_l.size(0))
weights.update(weight, inputs_l.size(0))

```



> 6. Backpropagation

ë‹¹ì—°í•œ backpropagation. Backpropagationì€ Student Modelì—ë§Œ í•™ìŠµì´ ì§„í–‰ëœë‹¤. Teacherëª¨ë¸ì€ ì•„ë˜ì˜ EMA Learningìœ¼ë¡œ ì§„í–‰ëœë‹¤.

```python
########################################
# 6. Backpropagation 
########################################
optimizer.zero_grad()
loss_total.backward()
optimizer.step()

```



> 7. EMA Learning for Teacher Model

EMAì„ í†µí•œ Teacher Modelì˜ í•™ìŠµì´ë‹¤. ê²°êµ­ì—ëŠ” ìš°ë¦¬ê°€ ì‚¬ìš©í•  ëª¨ë¸ì€ Teacherëª¨ë¸ì´ë©°(Student Modelì€ í•™ìŠµì´ ì˜ ì´ë£¨ì–´ì§€ì§€ ì•Šê³ , ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤.), ì €ìë“¤ì€ EMAê°€ ì˜¤íˆë ¤ ëª¨ë¸ ì„±ëŠ¥ì— ìƒì²˜(hurt)ë¥¼ ì¤€ë‹¤ê³  í•˜ì˜€ìœ¼ë‚˜, EMAë¥¼ í†µí•œ Teacher Modelì´ ê²°ë¡ ì ìœ¼ë¡œ í›¨ì”¬ í•™ìŠµë„ ì˜ë˜ê³  ì„±ëŠ¥ë„ ì˜ ë‚˜ì˜¨ë‹¤. ì´ëŠ” ì—¬ëŸ¬ êµ¬í˜„ì²´ì—ì„œ EMAë¥¼ ê¸°ë¸ìœ¼ë¡œ ê°€ì ¸ê°€ê³  ìˆê³ , ì¸í„°ë„· ì»¤ë®¤ë‹ˆí‹° ìƒì—ì„œë„ ë§ì€ ì´ë“¤ì´ EMAë¥¼ í†µí•´ ì„±ëŠ¥ì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆë‹¤ê³  ë§í•˜ê³  ìˆë‹¤.

```python
########################################
# 7. EMA Learning for Teacher Model 
########################################
if is_ema is True:
    ema_optimizer.step()
```







## 4. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : MAE (Mean Absolute Error)
- Datasetì€ Testset 20%, Training 64%, Validation 16%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨
- 3ê°œì˜ Datasetì— ëŒ€í•œ ê°ê°ì˜ LossëŠ” 3ê°€ì§€ë¡œ êµ¬ë¶„ëœë‹¤.
  - ì „ì²´ì˜ Average(Avg)
  - Normal Distribution(Many Shot)
  - Rare Distribution(Few Shot)


|      | Algorithm                     | Diabetes (Avg) | Diabetes (Many Shot) | Diabetes (Few Shot) | Boston House (Avg) | Boston House (Many Shot) | Boston House (Few Shot) | California House (Avg) | California House (Many Shot) | California House (Few Shot) |
| ---- | ----------------------------- | -------------- | -------------------- | ------------------- | ------------------ | ------------------------ | ----------------------- | ---------------------- | ---------------------------- | --------------------------- |
| 1    | MLP                           | 46.90          | 39.84                | 92.18               | 2.60               | 2.07                     | 8.09                    | 0.44                   | **0.36**                     | 1.00                        |
| 2    | Ensemble MLP (x3)             | **43.24**      | **36.47**            | **86.55**           | **2.41**           | 1.99                     | **6.71**                | 0.44                   | 0.37                         | **0.93**                    |
| 3    | Ensemble MLP with REBAGG (x3) | 44.35          | 37.38                | 89.11               | 2.59               | 2.10                     | 7.64                    | 0.44                   | **0.36**                     | 1.00                        |
| 4    | Ensemble MLP (x6)             | 43.88          | 36.60                | 90.60               | 2.50               | 2.06                     | 7.07                    | 0.44                   | 0.37                         | 0.94                        |
| 5    | Ensemble MLP with REBAGG (x6) | 44.70          | 37.66                | 89.92               | 2.42               | **1.96**                 | 7.11                    | 0.44                   | 0.37                         | 0.95                        |



----


# Final Insights

- Accuracy ê²°ê³¼ë¥¼ ë³´ë©´ Complexityê°€ ë†’ì€ MLPì™€ ê°™ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒí™©ì—ì„œëŠ” Baggingì„ ì‚¬ìš©í•˜ë©´ ê±°ì˜ ëŒ€ë¶€ë¶„ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.
- íŠ¹íˆë‚˜ ë‹¨ìˆœíˆ Ensemble(x3, x6 ëª¨ë‘)ì„ ì‚¬ìš©í–ˆìŒì—ë„, Average Accuracyë¿ë§Œ ì•„ë‹ˆë¼, Many Shotì—ì„œë„ Few Shotì—ì„œë„ ëª¨ë‘ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤.
- ì¦‰, Imbalanced Regression Taskí™˜ê²½ì—ì„œë„ ë‹¨ìˆœ Ensembleë¡œ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. Ensembleì´ Imbalanced Classificationì—ì„œëŠ” ëª‡ê°€ì§€ ë…¼ë¬¸ì´ ë‚˜ì™”ìœ¼ë‚˜, ì•„ì§ Imbalanced Regressionì—ëŠ” ê±°ì˜ ë…¼ë¬¸ì´ ì—†ëŠ” ê²ƒì„ ë³´ì•„ì„œëŠ” ì´ ë¶„ì•¼ì— ëŒ€í•´ì„œ ì¢€ ë” In-DepthìˆëŠ” ì—°êµ¬ë¥¼ í†µí•´ ì—°êµ¬ ì„±ê³¼ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ ê¸°ëŒ€í•´ ë³¼ ìˆ˜ ìˆê² ë‹¤.
- ê·¸ë‚˜ë§ˆ Imbalanced Regression Taskì— ì¡´ì¬í•˜ëŠ” REBAGGê³¼ ê°™ì€ ë°©ë²•ë¡ ì„ ì´ë²ˆ Tutorialì—ì„œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ ë³´ì•˜ìœ¼ë‚˜, ì˜¤íˆë ¤ ì „ë°˜ì ìœ¼ë¡œ Few-Shotì—ì„œ ë‹¨ìˆœ Ensembleì´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤. ê°œì¸ì ìœ¼ë¡œ SMOTE, SMOGNê°™ì€ ê³„ì—´ì„ ì „í˜€ ì„ í˜¸í•˜ì§€ ì•ŠëŠ”ë°, ì—¬ëŸ¬ Classification, Regression Taskì˜ Projectë“¤ì„ ì§„í–‰í•´ë´¤ì„ë•Œ ê±°ì˜ ì„±ëŠ¥ì˜ ìƒìŠ¹ íš¨ê³¼ê°€ Random Oversamplingë³´ë‹¤ë„, ê·¸ë¦¬ê³  ë‹¨ìˆœí•œ Loss Re-Weightingë³´ë‹¤ë„ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì´ë‹¤. ê±°ê¸°ë‹¤ê°€ Training Timeë§Œ ëŠ˜ë¦¬ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ SMOTE ê³„ì—´ì€ ë‚˜ëŠ” ê±°ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. REBAGGì˜ ë°©ë²•ë¡ ë„ SMOTE for Regressionë¥¼ ë§Œë“  ì—°êµ¬ì‹¤ì—ì„œ ë‚˜ì˜¨ ë°©ë²•ë¡ ì´ë°, ì—­ì‹œë‚˜ ì„±ëŠ¥ì˜ í–¥ìƒì´ ê±°ì˜ ì—†ë‹¤ê³  ìƒê°ì´ ë“ ë‹¤. ë‹¤ë¥¸ ì—°êµ¬ì›ë¶„ë“¤ë„ ê³¼ì œ í•˜ì‹¤ë•Œ ë§ì´ ì°¸ê³ í•˜ì…¨ìœ¼ë©´ ì¢‹ê² ë‹¤.
- ê·¸ë¦¬ê³  Ensembleì„ ë” ë§ì€ ëª¨ë¸ ìˆ˜ë¡œ ëŠ˜ë ¸ì„ë•Œ ì„±ëŠ¥ì´ ë” í–¥ìƒë  ì¤„ ì˜ˆìƒí•˜ì˜€ìœ¼ë‚˜, ì˜ˆìƒê³¼ ë‹¬ë¦¬ 3ê°œ(x3)ì¼ì„ ë•Œê°€ 6ê°œ(x6)ë¥¼ Ensemble í•˜ì˜€ì„ ë•Œ ë³´ë‹¤ ëª¨ë‘ ì„±ëŠ¥ì´ ì¢‹ì•˜ë‹¤. ì´ë¥¼ í†µí•´ì„œ Baggingì„ í†µí•œ Ensembleì‹œì— ì ì ˆí•œ Ensemble ê°œìˆ˜ì˜ ì„ íƒì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.
- ì¬ë¯¸ìˆëŠ” ì‚¬ì‹¤ì€ Datasetì— ë”°ë¼ì„œ ì´ Ensembleì˜ ì ì ˆí•œ ìˆ˜ê°€ ë‹¤ë¥¼ ê²ƒ ê°™ë‹¤ê³  ìƒê°ì´ ë“¤ì—ˆì§€ë§Œ, ì´ë²ˆì— Tutorialì—ì„œ í™œìš©í•œ 3ê°œì˜ Dataset ëª¨ë‘ 3ê°œì˜ Ensembleì—ì„œ ëª¨ë‘ 6ê°œì˜ Ensembleë³´ë‹¤ ì¢‹ì€ ê²ƒì„ ë³´ì•„ì„œëŠ” Ensembleì´ Datasetì— Dependecyê°€ ìˆì„ì§€ë„ ëª¨ë¥´ì§€ë§Œ ì¼ë‹¨ ê²°ê³¼ì ìœ¼ë¡œëŠ” Datasetì— ëŒ€í•œ DependencyëŠ” ì˜¤íˆë ¤ ì—†ì–´ ë³´ì´ëŠ”ê²Œ ì‹ ê¸°í•˜ì˜€ë‹¤.
- ë”°ë¼ì„œ Testë¥¼ í†µí•˜ì—¬ Ensemble ê°œìˆ˜ì˜ ìµœì ì ì„ ì°¾ëŠ” Caseë¥¼ Future Workë¡œ ì‹œë„í•´ë³¼ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ë‹¤.



# Conclusion

ê²°ë¡ ì ìœ¼ë¡œ, Ensembleì€ Imbalanced Data(ì—¬ê¸°ì„œëŠ” Imbalanced Regression)ì— íš¨ê³¼ê°€ ìˆë‹¤.



-----

# References

-  ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ê°•í•„ì„± êµìˆ˜ë‹˜ Business Analytics ê°•ì˜ ìë£Œ
- https://hipolarbear.tistory.com/19
- https://www.reddit.com/r/MachineLearning/comments/jb2egk/d_consitency_training_how_do_uda_or_fixmatch/
- https://github.com/kekmodel/FixMatch-pytorch/issues/19
- https://github.com/google-research/fixmatch/issues/20
- https://github.com/kekmodel/FixMatch-pytorch/issues/36

