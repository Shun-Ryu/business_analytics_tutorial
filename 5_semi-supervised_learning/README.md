# ğŸ¤”MixMatchë¥¼ êµ¬í˜„í•  ë•Œ ìƒê°í•´ë´ì•¼ í•˜ëŠ” ê²ƒë“¤

## The things to consider for implementing MixMatch Algorithm

![image-20221226195436600](./attachments/image-20221226195436600.png)



ğŸ”¥ì´ë²ˆ Tutorialì—ì„œëŠ” **Semi-Supervised Learning Method** ì¤‘, **Holisticí•œ ì ‘ê·¼ ë°©ë²•ì¸ MixMatch**ë¥¼ êµ¬í˜„í•´ë³´ë©´ì„œ, **êµ¬í˜„ ìƒì— ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤**ì— ëŒ€í•´ì„œ ìƒê°í•´ ë³´ëŠ” ì‹œê°„ì„ ê°–ìœ¼ë ¤ê³  í•œë‹¤. Githubì— ìˆëŠ” ì—¬ëŸ¬ê°€ì§€ êµ¬í˜„ì²´ë“¤ì„ í™•ì¸í•˜ì˜€ëŠ”ë°, ë…¼ë¬¸ì—ëŠ” ë“œëŸ¬ë‚˜ì§€ ì•Šì€ ì‚¬í•­ë“¤ì´ ë³´ì´ê¸°ì— ì´ Tutorialì—ì„œ ì—¬ëŸ¬ê°€ì§€ í…ŒìŠ¤íŠ¸ì™€ í•¨ê»˜ In-Depthí•˜ê²Œ ì•Œì•„ë³´ê³ ì í•œë‹¤.

ì´ëŠ” ì‚¬ì‹¤ MixMatch ë¿ë§Œ ì•„ë‹ˆë¼, Original MixMatch (from Google) ì•Œê³ ë¦¬ì¦˜ì—ì„œ íŒŒìƒë˜ì–´ ë‚˜ì˜¨ **FixMatch**ì—ì„œë„ ë™ì¼í•œ êµ¬í˜„ìƒ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ì‚¬í•­ì´ë¯€ë¡œ, ì´ë²ˆ Tutorialì„ í†µí•´ ì˜ ë°°ì›Œë³´ëŠ” ì‹œê°„ì„ ê°–ë„ë¡ í•˜ì.

ë…¼ë¬¸ì—ì„œë„ ì˜ ë‹¤ë£¨ì§€ ì•Šê³ , êµ¬í˜„ì²´ì—ì„œë„ ì˜ ì„¤ëª…ì´ ì—†ëŠ” ì•„ë˜ì˜ 2ê°€ì§€ ì£¼ì œë¡œ Tutorialì„ ì „ê°œí•˜ë ¤ í•œë‹¤. ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ëŒ€ë¶€ë¶„ ê¹Œë‹¤ë¡­ì§€ ì•Šë‹¤.

- MixMatchì—ì„œ **EMA(Exponential Moving Average)ë¡œ Teacherëª¨ë¸**ì„ ë§Œë“œëŠ” ê²ƒì´ ì¤‘ìš”í•œê°€? 
- MixMatchì—ì„œ **Interleaving** êµ¬í˜„ì€ ì¤‘ìš”í•œê°€?



# Table of Contents

- [Background of MixMatch](#Background-of-MixMatch)

  - [1. Data Augmentation](#1-Data-Augmentation)
  - [2. Label Guessing and Label Sharpening](#2-Label-Guessing-&-Label-Sharpening)
  - [3. MixUp](#3-MixUp)

- [Tutorial. Deep Understanding of MixMatch Implementation](#Tutorial-Deep-Understanding-of-MixMatch-Implementation)

  - [1. Tutorial Notebook](#1-Tutorial-Notebook)
  - [2. Setting](#2-Setting)
  - [3. Usage Code](#3-Usage-Code)
  - [4. Result (Accuracy)](#4-Result_Accuracy)

- [Final Insights](#Final-Insights)

- [Conclusion](#Conclusion)

- [References](#References)

  

-------

# Background of MixMatch

## 1. Basic Concept

MixMatchëŠ” ê¸°ì¡´ì˜ Consistency Regularizationê³¼ Pseudo-Labelingê³¼ ê°™ì€ ê¸°ë²•ì—ì„œ ë²—ì–´ë‚˜ì„œ, ê¸°ì¡´ì˜ ë°©ë²•ë¡ ë“¤ì„ ì—¬ëŸ¬ê°œë¥¼ ê²°í•©í•˜ì—¬ Holistic(ì „ì²´ë¡ ì )ì¸ Approachë¡œ ì ‘ê·¼í•œ ìµœì´ˆì˜ ë…¼ë¬¸ì´ë‹¤. (from Google Research). ì§€ê¸ˆì€ ì˜ ëª¨ë¥¼ìˆ˜ë„ ìˆìœ¼ë‚˜, ì²˜ìŒ ì´ ë°©ì‹ì´ ë‚˜ì™”ì„ ë•Œ, ê¸°ì¡´ì˜ ë°©ë²•ë¡ ë“¤ì„ ì••ë„í•˜ëŠ” Performanceë¡œ ë§ì€ ì´ë“¤ì—ê²Œ ì‹ ì„ í•œ ì¶©ê²©ì„ ì„ ì‚¬í•˜ì˜€ë˜ ë…¼ë¬¸ì´ë‹¤. 



**MixMatchì—ì„œ ì°¨ìš©í•˜ê³  ìˆëŠ” ì „ì²´ì ì¸ ë°©ë²•ë¡ ë“¤ì€ ì•„ë˜ì™€ ê°™ë‹¤.**

1. Data Augmentation 
2. Label Guessing & Label Sharpening
3. MixUp



ìœ„ì˜ ê²ƒë“¤ì€ ëª¨ë‘ ê³¼ê±°ì˜ ë…¼ë¬¸ë“¤ì—ì„œ ë§ì´ ë³´ì•„ì™”ë˜ ê¸°ë²•ë“¤ì¸ë°, ì´ëŸ¬í•œ **ì—¬ëŸ¬ê¸°ë²•ë“¤ì„ ì¡°í•©í•˜ì—¬ ì „ì²´ì ì¸ Architecture êµ¬ì¡° í˜•íƒœë¡œ ì¢‹ì€ ì„±ëŠ¥**ì„ ê°€ì ¸ê°”ë‹¤ëŠ” ê²ƒì´, MixMatch ë¥¼ í¬í•¨í•œ í›„ì† Matchì‹œë¦¬ì¦ˆ ê¸°ë²•ë“¤ì˜ íŠ¹ì§•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹íˆ Holisticí•˜ë‹¤ê³  í•˜ì§€ë§Œ, MixMatch ê°™ì€ ê²½ìš° íŠ¹íˆë‚˜ êµ¬í˜„ê³¼ êµ¬ì¡°ê°€ ë§¤ìš° ë‹¨ìˆœí•˜ë©´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì ¸ì™”ê¸° ë•Œë¬¸ì— ì„ êµ¬ìì ì¸ ë…¼ë¬¸ ì´ë¼ê³  ë³¼ ìˆ˜ ìˆê² ë‹¤.

ì´ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ MixUpì´ë©° ì´ë¥¼ Shuffleêµ¬ì¡°ë¥¼ ê°€ì ¸ê°€ë©° Dataì™€ Labelì„ Matchingí•´ ì£¼ì—ˆê¸° ë•Œë¬¸ì— MixMatchë¼ê³  ë¶€ë¥´ì§€ ì•Šë‚˜ ì‹¶ë‹¤.







> ğŸ”¥ì´ì œ ì•Œê³ ë¦¬ì¦˜ì„ ìˆœì„œ(Sequence)ëŒ€ë¡œ ì•Œì•„ë³´ì..!



## 1) Data Augmentation

ë¨¼ì € ëª¨ë“  Dataì˜ Data Augmentationì„ ìˆ˜í–‰í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ Computer Vision Deep Learning Modelë“¤ì´ ê·¸ë ‡ë“¯, **Regularizationë¡œì¨ Augmentationì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒ**ì‹œí‚¨ë‹¤. íŠ¹íˆ ë‚˜ì¤‘ì— ìˆì„ MixUpì—ì„œ ë” ë‹¤ì–‘í•œ ì¡°í•©ìœ¼ë¡œ Labeled Dataì™€ Unlabeled Dataë¥¼ ì„ì–´ì„œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ ì¤€ë‹¤.

![image-20221226232414467](./attachments/image-20221226232414467.png)

ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ Labeled DataëŠ” Batchë³„ë¡œ 1íšŒ Random Augmentationì„, Unlabeled DataëŠ” Batchë³„ë¡œ KíšŒ Random Augmentationì„ ì§„í–‰í•œë‹¤. ì´ë•Œ ë…¼ë¬¸ì—ì„œ ì§„í–‰í•œ Augmentationì€ ì•„ë˜ì™€ ê°™ì´ **Random horizontal flipsì™€ Random Crops**ì´ë‹¤.



> ![image-20221226232556152](./attachments/image-20221226232556152.png)



ë˜í•œ ë…¼ë¬¸ì—ì„œëŠ” Unlabeled Dataì— ëŒ€í•´ì„œëŠ” 2íšŒì˜ Augmentationì„ ì§„í–‰í•˜ë„ë¡ Hyper-Parameterë¥¼ ì„¸íŒ…í•˜ì˜€ë‹¤.

>  ![image-20221227010258137](./attachments/image-20221227010258137.png)





## 2) Label Guessing & Label Sharpening

ì´ ë°©ì‹ì€ Pseudo Labelingê³¼ ë™ì¼í•œ ë°©ì‹ì´ë©°, **Only Unlabeled Dataì— ëŒ€í•´ì„œë§Œ Label Guessing**ì„ ìˆ˜í–‰í•œë‹¤. ë˜í•œ ë§ˆì°¬ê°€ì§€ë¡œ Guessingëœ **Unlabeled Dataì˜ Labelì— ëŒ€í•´ì„œë§Œ Label Sharpening**ì„ ì§„í–‰í•œë‹¤. ì „ì²´ì ì¸ FlowëŠ” ì•„ë˜ì™€ ê°™ë‹¤. 

>  êµ¬í˜„ ì‹œì— Augmentedëœ Label Dataì— Guessingì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹˜ì„ ê¼­ ìœ ì˜ í•´ì•¼ í•œë‹¤.



![image-20221226200950638](./attachments/image-20221226200950638.png)

ìœ„ì™€ ê°™ì´ Batchë³„ë¡œ ìë™ì°¨ë¥¼ Kê°œì˜ Random Augmentation í•œí›„, Guessingëœ Labelë“¤ì„ Averageí•˜ê³ , ê·¸ ê°’ì„ Sharpeningí•œë‹¤. Sharpeningì´ë¼ëŠ” ê²ƒì€ í™•ë¥ ì´ ë†’ì€ ê²ƒì„ ì¢€ ë” ê°•ì¡°(Temperatureë¼ëŠ” Hyper-Parameter Të¥¼ ì‚¬ìš©í•˜ì—¬, ì–¼ë§ˆë‚˜ ê°•ì¡°í• ì§€ ì¡°ì •í•œë‹¤.) ì´ Label Sharpenigì„ í†µí•´ Unlabeled Dataì˜ Pseudo-Labelì— ëŒ€í•œ Entropyê°€ Minimizationëœë‹¤. (ì¦‰, í•˜ë‚˜ì˜ Guessing Labelì„ ë” ê°•ì¡°í•œë‹¤ëŠ” ì´ì•¼ê¸°ì„. ì „ì²´ Guessing Labelì´ Unifromí˜•íƒœë¥¼ ëˆë‹¤ë©´, Entropyê°€ Maximizationì´ ëœë‹¤.) 

> íŠ¹íˆ Entropy Minimizationì€ 2005ë…„ Semi-supervised learning by entropy minimization (Yves Grandvalet and Yoshua Bengio) ë…¼ë¬¸ì˜ ê´€ì°°ì„ í†µí•´ Ideaë¥¼ ì–»ì—ˆë‹¤ê³  ì €ìë“¤ì€ ì´ì•¼ê¸° í•œë‹¤. ì´ëŠ” ê·¸ë¦¬ê³  High-Density Region Assumptionì„ ë” ê°•ì¡°í•˜ê¸° ìœ„í•¨ì´ë‹¤.

![image-20221226234523055](./attachments/image-20221226234523055.png)

![image-20221226233648148](./attachments/image-20221226233648148.png)

ìœ„ì™€ ê°™ì´ ì¼ë°˜ì ì¸ Softmaxì˜ í™•ë¥ ê°’ì— 1/T ìŠ¹ì„ í•˜ì—¬ ê°’ì„ ê²°ì •í•˜ê²Œ ëœë‹¤. Tê°’ì´ 2.0, 0.5, 0.1ì— ë”°ë¼ì„œ, ê°’ì´ ë” ì‘ì„ ìˆ˜ë¡ Winnner-Take-Allì„ í•˜ê²Œ ëœë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ Hyper-Parameter Tê°’ì„ ì•„ë˜ì™€ ê°™ì´ 0.5ë¡œ ì„¸íŒ… í•˜ì˜€ìœ¼ë©°, ìš°ë¦¬ì˜ êµ¬í˜„ì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ 0.5ë¡œ ì„¸íŒ… í•  ìƒê°ì´ë‹¤.

> ![image-20221226233938750](./attachments/image-20221226233938750.png)



## 3) MixUp

> MixMatchì˜ ì´ë¦„ì— ì™œ Mixê°€ ë“¤ì–´ê°”ëŠ”ì§€ì— ëŒ€í•œ ì´ìœ ì´ë‹¤. ê·¸ë§Œí¼ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë‹¤.

MixUp(2018, mixup: BEYOND EMPIRICAL RISK MINIMIZATION)ì— ë‚˜ì˜¨ ê¸°ë²•ìœ¼ë¡œì¨, ì›ë˜ëŠ” ì¢€ ë” Decision Boundaryë¥¼ Smoothí•˜ê²Œ í•˜ì—¬ Emperical Risk Minimizationì„ Supervised Learningì—ì„œ í™œìš©í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë‹¨ìˆœí•œ ê¸°ë²•ì´ë‹¤. ê·¸ëŸ¬ë‚˜ 2019ë…„ Interpolation consistency training for semi-supervised learning(ICT)ì—ì„œ ì²˜ìŒìœ¼ë¡œ ì´ ë°©ë²•ì„ Semi-Supervised Learningì— ì ìš©ì„ í•˜ì˜€ë‹¤. **MixUpì„ í†µí•´ì„œ ì¢€ ë” Smoothí•œ Boundaryë¥¼ ê°•ì œí•˜ëŠ” Regularization**ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. 



>  Unlabeld Dataì— ëŒ€í•´ì„œë§Œ MixUpì„ ì§„í–‰í•œ ê¸°ì¡´ì˜ ICT ë°©ì‹ê³¼ ë‹¤ë¥´ê²Œ, MixMatchëŠ” Labeled Dataì™€ Unlabeled Dataëª¨ë‘ì— MixUpì„ ìˆ˜í–‰í•œë‹¤. 



ì¼ì¢…ì˜ Data Agumentationê³¼ ìœ ì‚¬í•œë°, ìœ„ì˜ 1)ì—ì„œ Augmentationí•´ì„œ ìƒê²¨ë‚œ Dataì— í•œë²ˆë” MixUpì´ë¼ëŠ” Augmentationì„ ì§„í–‰í•˜ê³ , ê·¸ X Dataì— ëŒ€í•œ ì˜ˆì¸¡ê³¼ Augmentedëœ Target Yê°’ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ìƒì„¸í•˜ê²ŒëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

![image-20221227010035007](./attachments/image-20221227010035007.png)





- **1) Augmented Setì„ ì¤€ë¹„í•œë‹¤.**
  - ìœ„ì—ì„œ ì§„í–‰í•œ Labeld Setì˜ Dataì— 1íšŒ Augmentation
  - Unlabeld Set Dataì— KíšŒ Augmentation
- **2) Shuffle Setì„ ì¤€ë¹„í•œë‹¤.**
  - Augmentation Setì„ Copyí•˜ì—¬ ë³µì‚¬í•˜ê³ , Labeled Setê³¼ UnLabeled Setì„ ëª¨ë‘ í•©ì³ì„œ ì„ëŠ”ë‹¤. ì´ë•Œ Data Xì™€ Target Yë„ ê°ê° ì„ì–´ ì¤€ë‹¤.
- **3) MixUpì„ ì§„í–‰í•œë‹¤.**
  - 1)ê³¼ 2)ì—ì„œ ì¤€ë¹„í•œ 2ê°œì˜ Setì„ Beta ë¶„í¬ì—ì„œ ë½‘ì€ Lambdaê°’ì„ í†µí•´ì„œ Weighted Sumì„ í†µí•´ Mixupí•œë‹¤. ì´ë•Œ LambdaëŠ” 0.5~1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì£¼ë„ë¡ í•˜ì—¬, Shuffle Setë³´ë‹¤ Augmented Setì— ë” Weightë¥¼ ì£¼ë„ë¡ í•œë‹¤. ê·¸ ì´ìœ ëŠ” ê°ê°ì˜ Target_L(Labeled)ê³¼ Target_U(Unlabeled)ì— ëŒ€í•˜ì—¬, Matchingë˜ë„ë¡ ê°’ì„ ì£¼ê¸° ìœ„í•¨ì´ë‹¤. (**ì•„ë§ˆ ì´ ë•Œë¬¸ì— MixMatchë¼ê³  ë¶€ë¥´ëŠ”ê²Œ ì•„ë‹Œê°€ ì‹¶ë‹¤.**) ê·¸ë ‡ì§€ ì•ŠëŠ”ë‹¤ë©´ Shuffle Setì— Biasë˜ê¸° ë•Œë¬¸ì— ì˜ Matchingë˜ë„ë¡ Lambdaë¥¼ ì¡°ì ˆí•œë‹¤.
  - ê·¸ë¦¬ê³  MixUpëœ Data_L(Labeled)ê³¼ Data_U(Unlabeled)ì— ëŒ€í•´ì„œëŠ” Modelì— ë„£ê³ , ê°ê°ì˜ ì˜ˆì¸¡ ê°’ì¸ Pred_L(Labeled)ê³¼ Pred_U(Unlabeled)ë¥¼ ë½‘ëŠ”ë‹¤.
  - ê·¸ë¦¬ê³  Pred_Lê³¼ Target_Lì‚¬ì´ì˜ Lossë¥¼ Cross Entropyë¥¼ í†µí•´ Supervised Lossë¡œ ì‚¬ìš©í•˜ê³ , Pred_Uì™€ Target_Uì‚¬ì´ì˜ ê°’ì€ Distance Metric(ex. MSE Loss)ì„ ì‚¬ìš©í•´ Unsupervised Lossë¥¼ êµ¬í•œë‹¤.
  - ê·¸ë¦¬ê³  2ê°œì˜ Lossë¥¼ ë¯¸ë¦¬ì •í•´ì§„ Hyper-Parameterê°’ Weightë¥¼ í†µí•´ ì¡°ì ˆí•œë‹¤. (ê·¸ë¦¬ê³  ì´ ê°’ì€ Learning Rateì²˜ëŸ¼, Ramp-Upì„ í†µí•´ ì¡°ì ˆí•˜ë„ë¡ í•œë‹¤.)





----

# Tutorial. Deep Understanding of MixMatch Implementation

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì „ì²´ì ì¸ MixMatchë¥¼ Scratchë¡œ êµ¬í˜„í•´ ë³´ë©´ì„œ ì•Œê³ ë¦¬ì¦˜ì„ ì´í•´í•´ ë³´ë ¤í•œë‹¤. íŠ¹íˆ ì› ì €ìë“¤ì´ ë…¼ë¬¸ì—ì„œ ì œëŒ€ë¡œ ê±´ë“œë¦¬ì§€ ì•Šì•˜ìœ¼ë‚˜(í˜¹ì€ ë°˜ëŒ€ë¡œ ì´ì•¼ê¸°í–ˆìœ¼ë‚˜!), ì›ì €ìë“¤ì˜ êµ¬í˜„ì²´ í˜¹ì€ ê¸°íƒ€ ë‹¤ë¥¸ êµ¬í˜„ì²´ë“¤ì—ì„œ ì´ë¯¸ êµ¬í˜„í•˜ê³  ìˆì—ˆìœ¼ë‚˜ ì œëŒ€ë¡œ ë…¼ì˜ê°€ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ ì˜ì—­ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¤í—˜ê³¼ í•¨ê»˜ ê°€ì ¸ê°€ë ¤ í•œë‹¤. ì œê¸°í•˜ê³  ì‹¶ì€ ì˜ë¬¸ì€ 2ê°€ì§€ì´ë‹¤.

- MixMatchì—ì„œ **EMA(Exponential Moving Average)**ë¡œ Teacherëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ ì¤‘ìš”í•œê°€? 
- MixMatchì—ì„œ **Interleaving** êµ¬í˜„ì€ ì¤‘ìš”í•œê°€?

>  ë™ì‹œì— í•´ë‹¹ ë…¼ë¬¸ì— ëŒ€í•´ì„œ Reviewerë“¤ì´ ì´ì•¼ê¸°í•œ ë‚´ìš©, ê·¸ë¦¬ê³  êµ¬í˜„ì²´ë“¤ì˜ Issueì— ì œê¸°í•œ ì˜ë¬¸ë“¤ë„ í•¨ê»˜ ë³´ë©´ì„œ ì´ì•¼ê¸° í•´ ë³´ë ¤ í•œë‹¤.



## 1. Tutorial Notebook 

### ğŸ”¥[Go to the tutorial notebook](https://github.com/Shun-Ryu/business_analytics_tutorial/blob/main/4_ensemble/Tutorials/Tutorial_Ensemble_Learning_On_Imbalacned_Regression.ipynb)



## 2. Setting

### Datasets

ë°ì´í„°ëŠ” ìœ ëª…í•œ CIFAR-10ì„ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤. 10ê°œì˜ Classë¥¼ ê°–ê³  ìˆëŠ” 32x32x3 Shapeì˜ Imagesetì´ë‹¤.

![image-20221227100541139](./attachments/image-20221227100541139.png)

|      | Datasets                  | Description                               | Num Instances                                   | Image Size | Num Channels | Single Output Classes |
| ---- | ------------------------- | ----------------------------------------- | ----------------------------------------------- | ---------- | ------------ | --------------------- |
| 1    | CIFAR-10 (Classification) | 10ê°œì˜ Classë¥¼ ê°€ì§„ ì‘ì€ ì´ë¯¸ì§€ ë°ì´í„° ì…‹ | Training Set : 50,000<br />Testing Set : 10,000 | 32 x 32    | 3            | 10                    |

ë°ì´í„°ì…‹ì€ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¤ê²Œ ëœë‹¤.

```python
# dataset_name = 'diabetes'
dataset_name = 'california_house'
# dataset_name = 'boston_house'

if dataset_name == 'diabetes':
    x, y= datasets.load_diabetes(return_X_y=True)
    threshold_rare = 270
    EPOCHS = 3500
    TRAIN_BATCH = 2048
elif dataset_name == 'california_house':
    data = datasets.fetch_california_housing()
    x = data.data
    y = data.target
    threshold_rare = 3.5
    EPOCHS = 800
    TRAIN_BATCH = 4096 
elif dataset_name == 'boston_house':
    data_url = "http://lib.stat.cmu.edu/datasets/boston"
    raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
    x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
    y = raw_df.values[1::2, 2]

    threshold_rare = 35
    EPOCHS = 3500
    TRAIN_BATCH = 2048


```

ë¶ˆëŸ¬ì§„ 3ê°œì˜ Datasetì— ëŒ€í•œ Yê°’ì˜ Sampling ë¶„í¬ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. íŠ¹ë³„íˆ Imbalanced Datasetì„ ê³ ë¥¸ ê²ƒë„ ì•„ë‹ˆì§€ë§Œ. ëª¨ë“  ë°ì´í„°ê°€ ì™¼ìª½ìœ¼ë¡œ Skewê°€ ëœ, Right-Side Long-tailed Regression Problemì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![image-20221201180832257](./attachments/image-20221201180832257.png)



ê° Regression Taskì—ì„œ Imbalanced Regressionì˜ ì •í™•ë„ë¥¼ êµ¬í•˜ê¸° ìœ„í•˜ì—¬, Many shotê³¼ Few Shotìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ë‚˜ëˆ„ì–´ ê³„ì‚°í•˜ë ¤ í•œë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì€ Thresholdê°’ì„ í†µí•´ ë°ì´í„°ë¥¼ 2ê°€ì§€ í˜•íƒœë¡œ êµ¬ë¶„í•˜ê³ , ê°ê°ì˜ êµ¬ë¶„ëœ Many shotê³¼ Few shotì˜ ì •í™•ë„ë¥¼ L1 Lossë¡œ êµ¬í•˜ê²Œ ëœë‹¤.

- **Diabetes : 270** 
- **Boston House Price : 35**
- **California House Price : 3.5**

ì´ëŸ¬í•œ ìˆ˜ì¹˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ SMOTE, SMOGN ë“±ì˜ ê¸°ë²•ë“¤ì„ êµ¬í˜„í•˜ ì €ìë“¤ì€ Relevance Functionì„ êµ¬í•˜ì—¬ ì •í•˜ê²Œ ë˜ëŠ”ë°, ì‚¬ì‹¤ íŠ¹ë³„í•œ ì°¨ì´ëŠ” ì—†ê¸° ë–„ë¬¸ì—, ê°„ë‹¨íˆ Constant Thresholdë¡œ Many Shotê³¼ Few Shotìœ¼ë¡œ êµ¬ë¶„ í•˜ì˜€ë‹¤. ìµœê·¼ì˜ Imbalanced Regression Task ë…¼ë¬¸ë“¤ì—ì„œë„ ìœ„ì™€ ìœ ì‚¬í•˜ê²Œ ì§„í–‰í•œë‹¤.



### Algorithms

ìš°ë¦¬ëŠ” MixMatch 1ê°€ì§€ë§Œ ì§‘ì¤‘ì ìœ¼ë¡œ íŒŒ ë³´ê³ ì í•œë‹¤. íŠ¹íˆ ìœ„ì—ì„œ ì œê¸°í•œ 2ê°€ì§€, EMA ë°©ì‹ì˜ Teacher Networkì™€ Interleaveì— ëŒ€í•œ ì´í•´ë¥¼ íŒŒë³´ê² ë‹¤.

| Algorithm | Target         | Description                                                  |
| --------- | -------------- | ------------------------------------------------------------ |
| MixMatch  | Classification | WideResNetì„ Backboneìœ¼ë¡œ ì‚¬ìš©í•œ Holistic Semi-Supervised Learning |



## 3. Implementation of MixMatch

### MLP

2ê°œì˜ Hidden Layerì™€ Input, Output Layerë¥¼ ê°€ì§„ ê°„ë‹¨í•œ MLPêµ¬ì¡°ë¥¼ Main Modelë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ Dropoutê³¼ Batchnormalization ë“±ì˜ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ Regularizationì„ í•˜ì˜€ìœ¼ë©°, Overfittingì „ì— Validation Setìœ¼ë¡œ ê²°ì •ëœ Best Modelì„ ì¤‘ê°„ì¤‘ê°„ ì €ì¥í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ í•´ë‹¹ MLPëª¨ë¸ì€ Variance Errorë¥¼ ë§ì´ ì¤„ì—¬ë‘” ìƒíƒœë¼ê³ ë„ ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹íˆë‚˜ Dropoutê°™ì€ ê²½ìš° 0.5 ì •ë„ë¡œ í¬ê²Œ ê±¸ì–´ì£¼ì—ˆê¸° ë•Œë¬¸ì—, Dropoutì´ Ensembleê³¼ ìœ ì‚¬í•œ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆì´ë¯€ë¡œ, í•œë²ˆ ì´ ìƒíƒœì—ì„œ Ensembleì˜ íš¨ê³¼ê°€ ê³¼ì—° ì¶”ê°€ì ìœ¼ë¡œ ìˆì„ì§€ ë³´ë„ë¡ í•˜ì.



> Model Code

```python
BATCH_SIZE = 2048 
LEARNING_RATE = 0.001

NUM_INPUT = x_train.shape[1]
NUM_OUTPUT = 1 
NUM_1ST_HIDDEN = 32 
NUM_2ND_HIDDEN = 16 
NUM_1ST_DROPOUT = 0.6
NUM_2ND_DROPOUT = 0.5

class BasicRegressor(nn.Module):
    def __init__(self) -> None:
        super(BasicRegressor, self).__init__()

        self.layer_1 = nn.Linear(NUM_INPUT, NUM_1ST_HIDDEN)
        self.layer_2 = nn.Linear(NUM_1ST_HIDDEN, NUM_2ND_HIDDEN)
        self.layer_out = nn.Linear(NUM_2ND_HIDDEN, NUM_OUTPUT)

        # self.actvation = nn.ReLU()
        self.actvation_1 = nn.ReLU()
        self.actvation_2 = nn.ReLU()
        self.dropout_1 = nn.Dropout(p=NUM_1ST_DROPOUT)
        self.dropout_2 = nn.Dropout(p=NUM_2ND_DROPOUT)
        self.batchnorm_1 = nn.BatchNorm1d(NUM_1ST_HIDDEN)
        self.batchnorm_2 = nn.BatchNorm1d(NUM_2ND_HIDDEN)
    
    def forward(self, inputs):
        x = self.actvation_1(self.layer_1(inputs))
        x = self.batchnorm_1(x)
        x = self.dropout_1(x)
        x = self.actvation_2(self.layer_2(x))
        x = self.batchnorm_2(x)
        x = self.dropout_2(x)
        x = self.layer_out(x)

        return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

```



> Training Code

í•™ìŠµì€ êµ‰ì¥íˆ ë‹¨ìˆœí•˜ë‹¤. í•˜ë‚˜ì˜ ëª¨ë¸ì„ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ëë‚œë‹¤. :) 

```python
num_train_data = len(train_loader)
num_eval_data = len(valid_loader)


elapsed_time_basic_ann = []

start_time = datetime.now()


best_model = train_model(num_train_data, num_eval_data)


elapsed_time_basic_ann.append((datetime.now()-start_time).total_seconds())
```



> Inference Code

Inferenceë„ êµ‰ì¥íˆ ë‹¨ìˆœí•˜ë‹¤. ì €ì¥ëœ Best Model 1ê°œë¡œ Test Datasetì— ëŒ€í•´ Evaluationí•˜ê³ , ê·¸ì— ëŒ€í•œ ê°œë³„ Lossë¥¼ êµ¬í•œë‹¤. (Few Shotê³¼ Many Shotì— ëŒ€í•œ ê°œë³„ì  L1 Lossë¥¼ ê³„ì‚°í•¨)

```python
best_model.eval()
data = torch.from_numpy(x_test).float().to(device)
answer = torch.from_numpy(y_test).float().to(device)

start_time = datetime.now()
output = best_model(data)
loss_basic_ann = calc_loss(output, answer)
elapsed_time_basic_ann.append((datetime.now()-start_time).total_seconds())

print('elapsed time ', elapsed_time_basic_ann)
```





## 4. Result_Accuracy

- ì¸¡ì • ë‹¨ìœ„ : MAE (Mean Absolute Error)
- Datasetì€ Testset 20%, Training 64%, Validation 16%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- AccuracyëŠ” Testsetì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ì˜€ë‹¤. (ë‹¹ì—°íˆ!)
- ëª¨ë¸ì€ Validation ê¸°ì¤€ìœ¼ë¡œ Lossê°€ ê°€ì¥ ì ì€ Best Modelë¡œ Testingì„ ì§„í–‰í•¨
- 3ê°œì˜ Datasetì— ëŒ€í•œ ê°ê°ì˜ LossëŠ” 3ê°€ì§€ë¡œ êµ¬ë¶„ëœë‹¤.
  - ì „ì²´ì˜ Average(Avg)
  - Normal Distribution(Many Shot)
  - Rare Distribution(Few Shot)


|      | Algorithm                     | Diabetes (Avg) | Diabetes (Many Shot) | Diabetes (Few Shot) | Boston House (Avg) | Boston House (Many Shot) | Boston House (Few Shot) | California House (Avg) | California House (Many Shot) | California House (Few Shot) |
| ---- | ----------------------------- | -------------- | -------------------- | ------------------- | ------------------ | ------------------------ | ----------------------- | ---------------------- | ---------------------------- | --------------------------- |
| 1    | MLP                           | 46.90          | 39.84                | 92.18               | 2.60               | 2.07                     | 8.09                    | 0.44                   | **0.36**                     | 1.00                        |
| 2    | Ensemble MLP (x3)             | **43.24**      | **36.47**            | **86.55**           | **2.41**           | 1.99                     | **6.71**                | 0.44                   | 0.37                         | **0.93**                    |
| 3    | Ensemble MLP with REBAGG (x3) | 44.35          | 37.38                | 89.11               | 2.59               | 2.10                     | 7.64                    | 0.44                   | **0.36**                     | 1.00                        |
| 4    | Ensemble MLP (x6)             | 43.88          | 36.60                | 90.60               | 2.50               | 2.06                     | 7.07                    | 0.44                   | 0.37                         | 0.94                        |
| 5    | Ensemble MLP with REBAGG (x6) | 44.70          | 37.66                | 89.92               | 2.42               | **1.96**                 | 7.11                    | 0.44                   | 0.37                         | 0.95                        |



----


# Final Insights

- Accuracy ê²°ê³¼ë¥¼ ë³´ë©´ Complexityê°€ ë†’ì€ MLPì™€ ê°™ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒí™©ì—ì„œëŠ” Baggingì„ ì‚¬ìš©í•˜ë©´ ê±°ì˜ ëŒ€ë¶€ë¶„ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.
- íŠ¹íˆë‚˜ ë‹¨ìˆœíˆ Ensemble(x3, x6 ëª¨ë‘)ì„ ì‚¬ìš©í–ˆìŒì—ë„, Average Accuracyë¿ë§Œ ì•„ë‹ˆë¼, Many Shotì—ì„œë„ Few Shotì—ì„œë„ ëª¨ë‘ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤.
- ì¦‰, Imbalanced Regression Taskí™˜ê²½ì—ì„œë„ ë‹¨ìˆœ Ensembleë¡œ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. Ensembleì´ Imbalanced Classificationì—ì„œëŠ” ëª‡ê°€ì§€ ë…¼ë¬¸ì´ ë‚˜ì™”ìœ¼ë‚˜, ì•„ì§ Imbalanced Regressionì—ëŠ” ê±°ì˜ ë…¼ë¬¸ì´ ì—†ëŠ” ê²ƒì„ ë³´ì•„ì„œëŠ” ì´ ë¶„ì•¼ì— ëŒ€í•´ì„œ ì¢€ ë” In-DepthìˆëŠ” ì—°êµ¬ë¥¼ í†µí•´ ì—°êµ¬ ì„±ê³¼ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ ê¸°ëŒ€í•´ ë³¼ ìˆ˜ ìˆê² ë‹¤.
- ê·¸ë‚˜ë§ˆ Imbalanced Regression Taskì— ì¡´ì¬í•˜ëŠ” REBAGGê³¼ ê°™ì€ ë°©ë²•ë¡ ì„ ì´ë²ˆ Tutorialì—ì„œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ ë³´ì•˜ìœ¼ë‚˜, ì˜¤íˆë ¤ ì „ë°˜ì ìœ¼ë¡œ Few-Shotì—ì„œ ë‹¨ìˆœ Ensembleì´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤. ê°œì¸ì ìœ¼ë¡œ SMOTE, SMOGNê°™ì€ ê³„ì—´ì„ ì „í˜€ ì„ í˜¸í•˜ì§€ ì•ŠëŠ”ë°, ì—¬ëŸ¬ Classification, Regression Taskì˜ Projectë“¤ì„ ì§„í–‰í•´ë´¤ì„ë•Œ ê±°ì˜ ì„±ëŠ¥ì˜ ìƒìŠ¹ íš¨ê³¼ê°€ Random Oversamplingë³´ë‹¤ë„, ê·¸ë¦¬ê³  ë‹¨ìˆœí•œ Loss Re-Weightingë³´ë‹¤ë„ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì´ë‹¤. ê±°ê¸°ë‹¤ê°€ Training Timeë§Œ ëŠ˜ë¦¬ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ SMOTE ê³„ì—´ì€ ë‚˜ëŠ” ê±°ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. REBAGGì˜ ë°©ë²•ë¡ ë„ SMOTE for Regressionë¥¼ ë§Œë“  ì—°êµ¬ì‹¤ì—ì„œ ë‚˜ì˜¨ ë°©ë²•ë¡ ì´ë°, ì—­ì‹œë‚˜ ì„±ëŠ¥ì˜ í–¥ìƒì´ ê±°ì˜ ì—†ë‹¤ê³  ìƒê°ì´ ë“ ë‹¤. ë‹¤ë¥¸ ì—°êµ¬ì›ë¶„ë“¤ë„ ê³¼ì œ í•˜ì‹¤ë•Œ ë§ì´ ì°¸ê³ í•˜ì…¨ìœ¼ë©´ ì¢‹ê² ë‹¤.
- ê·¸ë¦¬ê³  Ensembleì„ ë” ë§ì€ ëª¨ë¸ ìˆ˜ë¡œ ëŠ˜ë ¸ì„ë•Œ ì„±ëŠ¥ì´ ë” í–¥ìƒë  ì¤„ ì˜ˆìƒí•˜ì˜€ìœ¼ë‚˜, ì˜ˆìƒê³¼ ë‹¬ë¦¬ 3ê°œ(x3)ì¼ì„ ë•Œê°€ 6ê°œ(x6)ë¥¼ Ensemble í•˜ì˜€ì„ ë•Œ ë³´ë‹¤ ëª¨ë‘ ì„±ëŠ¥ì´ ì¢‹ì•˜ë‹¤. ì´ë¥¼ í†µí•´ì„œ Baggingì„ í†µí•œ Ensembleì‹œì— ì ì ˆí•œ Ensemble ê°œìˆ˜ì˜ ì„ íƒì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.
- ì¬ë¯¸ìˆëŠ” ì‚¬ì‹¤ì€ Datasetì— ë”°ë¼ì„œ ì´ Ensembleì˜ ì ì ˆí•œ ìˆ˜ê°€ ë‹¤ë¥¼ ê²ƒ ê°™ë‹¤ê³  ìƒê°ì´ ë“¤ì—ˆì§€ë§Œ, ì´ë²ˆì— Tutorialì—ì„œ í™œìš©í•œ 3ê°œì˜ Dataset ëª¨ë‘ 3ê°œì˜ Ensembleì—ì„œ ëª¨ë‘ 6ê°œì˜ Ensembleë³´ë‹¤ ì¢‹ì€ ê²ƒì„ ë³´ì•„ì„œëŠ” Ensembleì´ Datasetì— Dependecyê°€ ìˆì„ì§€ë„ ëª¨ë¥´ì§€ë§Œ ì¼ë‹¨ ê²°ê³¼ì ìœ¼ë¡œëŠ” Datasetì— ëŒ€í•œ DependencyëŠ” ì˜¤íˆë ¤ ì—†ì–´ ë³´ì´ëŠ”ê²Œ ì‹ ê¸°í•˜ì˜€ë‹¤.
- ë”°ë¼ì„œ Testë¥¼ í†µí•˜ì—¬ Ensemble ê°œìˆ˜ì˜ ìµœì ì ì„ ì°¾ëŠ” Caseë¥¼ Future Workë¡œ ì‹œë„í•´ë³¼ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ë‹¤.



# Conclusion

ê²°ë¡ ì ìœ¼ë¡œ, Ensembleì€ Imbalanced Data(ì—¬ê¸°ì„œëŠ” Imbalanced Regression)ì— íš¨ê³¼ê°€ ìˆë‹¤.



-----

# References

-  ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ ê°•í•„ì„± êµìˆ˜ë‹˜ Business Analytics ê°•ì˜ ìë£Œ
- https://hipolarbear.tistory.com/19
